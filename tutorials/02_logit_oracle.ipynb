{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Logit DGP: Neural Network vs Logistic Regression Oracle\n\n**Links:**\n- [GitHub](https://github.com/rawatpranjal/deep-inference)\n- [PyPI](https://pypi.org/project/deep-inference/)\n- [Documentation](https://rawatpranjal.github.io/deep-inference/)\n\n**References:**\n- Farrell, Liang, Misra (2021) \"Deep Neural Networks for Estimation and Inference\" *Econometrica*\n- Farrell, Liang, Misra (2025) \"Deep Learning for Individual Heterogeneity\"\n\n---\n\nThis notebook validates `structural_dml` for **logit** models against correctly-specified logistic regression oracles.\n\n## DGP Specification\n\n$$P(Y=1 | X, T) = \\sigma(\\alpha(X) + \\beta(X) \\cdot T)$$\n\nwhere $\\sigma(z) = 1/(1 + e^{-z})$ is the sigmoid function.\n\n**Target:** $\\mu^* = E[\\beta(X)] = 0.5$\n\n## Lambda Method\n\nThe package defaults to `lambda_method='ridge'` which has 96% validated coverage. You can also use `'aggregate'` or `'lgbm'`.\n\n```python\n# Default (ridge) - recommended\nresult = structural_dml(Y, T, X, family='logit')\n\n# Or explicit\nresult = structural_dml(Y, T, X, family='logit', lambda_method='ridge')\n```"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Setup & DGP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from scipy import stats\n",
    "from scipy.special import expit\n",
    "import warnings\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Use local deep_inference\n",
    "sys.path.insert(0, str(Path.cwd().parent / 'src'))\n",
    "from deep_inference import structural_dml\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DGP Parameters\n",
    "A0, A1 = 1.0, 0.3      # alpha(X) = 1.0 + 0.3*X\n",
    "B0, B1 = 0.5, 0.2      # beta(X) = 0.5 + 0.2*X\n",
    "MU_TRUE = 0.5          # E[beta(X)] = 0.5 (since E[X]=0)\n",
    "\n",
    "print(f\"DGP: P(Y=1) = sigmoid(alpha(X) + beta(X)*T)\")\n",
    "print(f\"  alpha(X) = {A0} + {A1}*X\")\n",
    "print(f\"  beta(X)  = {B0} + {B1}*X\")\n",
    "print(f\"  X ~ N(0, 1)\")\n",
    "print(f\"  Target mu* = E[beta(X)] = {MU_TRUE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(n, seed=None):\n",
    "    \"\"\"Generate data from logit DGP.\"\"\"\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "    X = np.random.normal(0, 1, n)\n",
    "    T = np.random.normal(0, 1, n)\n",
    "    alpha = A0 + A1 * X\n",
    "    beta = B0 + B1 * X\n",
    "    p = expit(alpha + beta * T)\n",
    "    Y = np.random.binomial(1, p).astype(float)\n",
    "    return {'Y': Y, 'T': T, 'X': X, 'alpha': alpha, 'beta': beta, 'p': p}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize DGP\n",
    "data = generate_data(1000, seed=42)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Y vs T\n",
    "ax = axes[0]\n",
    "scatter = ax.scatter(data['T'], data['Y'] + np.random.normal(0, 0.02, 1000), \n",
    "                     c=data['X'], cmap='coolwarm', alpha=0.5, s=10)\n",
    "plt.colorbar(scatter, ax=ax, label='X')\n",
    "ax.set_xlabel('T'); ax.set_ylabel('Y'); ax.set_title('Y vs T (colored by X)')\n",
    "\n",
    "# Structural functions\n",
    "ax = axes[1]\n",
    "X_grid = np.linspace(-3, 3, 100)\n",
    "ax.plot(X_grid, A0 + A1*X_grid, 'b-', label=r'$\\alpha(X)$', lw=2)\n",
    "ax.plot(X_grid, B0 + B1*X_grid, 'r-', label=r'$\\beta(X)$', lw=2)\n",
    "ax.axhline(MU_TRUE, color='gray', ls='--', label=f'$\\mu^*={MU_TRUE}$')\n",
    "ax.set_xlabel('X'); ax.legend(); ax.set_title('True Structural Functions')\n",
    "\n",
    "# Distribution of beta\n",
    "ax = axes[2]\n",
    "ax.hist(data['beta'], bins=30, alpha=0.7, edgecolor='black')\n",
    "ax.axvline(MU_TRUE, color='red', ls='--', lw=2, label=f'$\\mu^*={MU_TRUE}$')\n",
    "ax.set_xlabel(r'$\\beta(X)$'); ax.legend(); ax.set_title(r'Distribution of $\\beta(X)$')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Oracle Implementation\n",
    "\n",
    "Logistic regression oracle: $\\text{logit}(P(Y=1)) = a_0 + a_1 X + b_0 T + b_1 (X \\cdot T)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logit_oracle(Y, T, X):\n",
    "    \"\"\"Logit oracle with naive and delta-corrected SE.\"\"\"\n",
    "    n = len(Y)\n",
    "    X_bar = X.mean()\n",
    "    X_design = np.column_stack([np.ones(n), X, T, X * T])\n",
    "    model = sm.Logit(Y, X_design).fit(disp=0)\n",
    "    a0, a1, b0, b1 = model.params\n",
    "    mu_hat = b0 + b1 * X_bar\n",
    "    \n",
    "    cov = model.cov_params()\n",
    "    var_naive = cov[2,2] + X_bar**2*cov[3,3] + 2*X_bar*cov[2,3]\n",
    "    var_delta = var_naive + b1**2 * (X.var(ddof=1)/n)\n",
    "    \n",
    "    return {\n",
    "        'mu_hat': mu_hat,\n",
    "        'se_naive': np.sqrt(max(var_naive, 1e-10)),\n",
    "        'se_delta': np.sqrt(max(var_delta, 1e-10)),\n",
    "        'params': {'a0': a0, 'a1': a1, 'b0': b0, 'b1': b1},\n",
    "        'alpha_hat': a0 + a1*X,\n",
    "        'beta_hat': b0 + b1*X\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test oracle\n",
    "result = logit_oracle(data['Y'], data['T'], data['X'])\n",
    "print(\"Oracle Results:\")\n",
    "print(f\"  a0={result['params']['a0']:.3f} (true {A0}), a1={result['params']['a1']:.3f} (true {A1})\")\n",
    "print(f\"  b0={result['params']['b0']:.3f} (true {B0}), b1={result['params']['b1']:.3f} (true {B1})\")\n",
    "print(f\"  mu_hat={result['mu_hat']:.4f} (true {MU_TRUE})\")\n",
    "print(f\"  SE naive={result['se_naive']:.4f}, SE delta={result['se_delta']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Oracle Monte Carlo (M=500)\n",
    "\n",
    "Fast validation of the oracle - runs in ~1 minute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oracle MC\n",
    "M = 500\n",
    "N = 1000\n",
    "\n",
    "mus, ses_naive, ses_delta = [], [], []\n",
    "covered_naive, covered_delta = [], []\n",
    "\n",
    "print(f\"Running Oracle MC: M={M}, N={N}\")\n",
    "for i in range(M):\n",
    "    d = generate_data(N, seed=i)\n",
    "    r = logit_oracle(d['Y'], d['T'], d['X'])\n",
    "    mus.append(r['mu_hat'])\n",
    "    ses_naive.append(r['se_naive'])\n",
    "    ses_delta.append(r['se_delta'])\n",
    "    covered_naive.append(r['mu_hat'] - 1.96*r['se_naive'] <= MU_TRUE <= r['mu_hat'] + 1.96*r['se_naive'])\n",
    "    covered_delta.append(r['mu_hat'] - 1.96*r['se_delta'] <= MU_TRUE <= r['mu_hat'] + 1.96*r['se_delta'])\n",
    "\n",
    "mus = np.array(mus)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oracle MC Results\n",
    "print(\"=\"*60)\n",
    "print(f\"ORACLE MONTE CARLO RESULTS (M={M}, N={N})\")\n",
    "print(\"=\"*60)\n",
    "print(f\"True mu* = {MU_TRUE}\")\n",
    "print(f\"Mean estimate: {mus.mean():.4f}\")\n",
    "print(f\"Bias: {mus.mean() - MU_TRUE:.4f}\")\n",
    "print(f\"Empirical SE: {mus.std():.4f}\")\n",
    "print()\n",
    "print(f\"Naive SE:\")\n",
    "print(f\"  Mean Est SE: {np.mean(ses_naive):.4f}\")\n",
    "print(f\"  SE Ratio: {np.mean(ses_naive)/mus.std():.2f}\")\n",
    "print(f\"  Coverage: {np.mean(covered_naive):.1%}\")\n",
    "print()\n",
    "print(f\"Delta-corrected SE:\")\n",
    "print(f\"  Mean Est SE: {np.mean(ses_delta):.4f}\")\n",
    "print(f\"  SE Ratio: {np.mean(ses_delta)/mus.std():.2f}\")\n",
    "print(f\"  Coverage: {np.mean(covered_delta):.1%}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Oracle MC\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Histogram\n",
    "ax = axes[0]\n",
    "ax.hist(mus, bins=30, alpha=0.7, edgecolor='black')\n",
    "ax.axvline(MU_TRUE, color='red', ls='--', lw=2, label=f'True $\\mu^*$={MU_TRUE}')\n",
    "ax.axvline(mus.mean(), color='blue', ls=':', lw=2, label=f'Mean={mus.mean():.4f}')\n",
    "ax.set_xlabel(r'$\\hat{\\mu}$'); ax.set_ylabel('Frequency')\n",
    "ax.set_title(f'Oracle Estimates (Coverage: {np.mean(covered_delta):.1%})')\n",
    "ax.legend()\n",
    "\n",
    "# QQ plot\n",
    "ax = axes[1]\n",
    "t_stats = (mus - MU_TRUE) / np.array(ses_delta)\n",
    "stats.probplot(t_stats, dist='norm', plot=ax)\n",
    "ax.set_title('QQ Plot of t-statistics (Delta SE)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Neural Network with IF-based SE\n",
    "\n",
    "Single run demonstrating influence function-based standard errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Generate data\nnp.random.seed(42)\ndata = generate_data(1000, seed=42)\n\nprint(\"Running Neural Network with IF-based SE...\")\nnn_result = structural_dml(\n    Y=data['Y'],\n    T=data['T'],\n    X=data['X'].reshape(-1, 1),\n    family='logit',\n    # lambda_method='ridge' is default (96% coverage)\n    epochs=100,\n    n_folds=50,\n    hidden_dims=[64, 32],\n    lr=0.01,\n    verbose=False\n)\nprint(\"Done!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# NN: Naive vs IF Comparison\nbeta_hat = nn_result.theta_hat[:, 1]\nn = len(beta_hat)\nse_naive = beta_hat.std() / np.sqrt(n)\n\nprint(\"=\"*60)\nprint(\"NN: NAIVE vs INFLUENCE FUNCTION\")\nprint(\"=\"*60)\nprint(f\"True mu* = {MU_TRUE}\")\nprint()\nprint(f\"{'Method':<12} {'Estimate':<12} {'SE':<12} {'95% CI':<25} {'Covers?'}\")\nprint(\"-\"*60)\n\n# Naive\nci_naive = (nn_result.mu_naive - 1.96*se_naive, nn_result.mu_naive + 1.96*se_naive)\ncovers_naive = ci_naive[0] <= MU_TRUE <= ci_naive[1]\nprint(f\"{'Naive':<12} {nn_result.mu_naive:<12.4f} {se_naive:<12.4f} [{ci_naive[0]:.4f}, {ci_naive[1]:.4f}]  {covers_naive}\")\n\n# IF-corrected\ncovers_if = nn_result.ci_lower <= MU_TRUE <= nn_result.ci_upper\nprint(f\"{'IF':<12} {nn_result.mu_hat:<12.4f} {nn_result.se:<12.4f} [{nn_result.ci_lower:.4f}, {nn_result.ci_upper:.4f}]  {covers_if}\")\n\nprint(\"=\"*60)\nprint()\nprint(f\"SE Ratio (IF/Naive): {nn_result.se / se_naive:.1f}x\")\nprint(\"Naive SE ignores estimation uncertainty â†’ overconfident CIs\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Oracle vs NN on same data\n",
    "oracle_result = logit_oracle(data['Y'], data['T'], data['X'])\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"COMPARISON: Oracle vs Neural Network\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Metric':<20} {'Oracle':<15} {'NN':<15}\")\n",
    "print(\"-\"*60)\n",
    "print(f\"{'mu_hat':<20} {oracle_result['mu_hat']:<15.4f} {nn_result.mu_hat:<15.4f}\")\n",
    "print(f\"{'SE':<20} {oracle_result['se_delta']:<15.4f} {nn_result.se:<15.4f}\")\n",
    "print(f\"{'CI lower':<20} {oracle_result['mu_hat']-1.96*oracle_result['se_delta']:<15.4f} {nn_result.ci_lower:<15.4f}\")\n",
    "print(f\"{'CI upper':<20} {oracle_result['mu_hat']+1.96*oracle_result['se_delta']:<15.4f} {nn_result.ci_upper:<15.4f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter recovery comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Alpha recovery\n",
    "ax = axes[0]\n",
    "ax.scatter(data['alpha'], oracle_result['alpha_hat'], alpha=0.3, s=10, label='Oracle')\n",
    "ax.scatter(data['alpha'], nn_result.theta_hat[:, 0], alpha=0.3, s=10, label='NN')\n",
    "ax.plot([data['alpha'].min(), data['alpha'].max()], \n",
    "        [data['alpha'].min(), data['alpha'].max()], 'k--', lw=2)\n",
    "ax.set_xlabel(r'True $\\alpha(X)$'); ax.set_ylabel(r'Estimated $\\alpha(X)$')\n",
    "ax.set_title(r'$\\alpha(X)$ Recovery'); ax.legend()\n",
    "\n",
    "# Beta recovery\n",
    "ax = axes[1]\n",
    "ax.scatter(data['beta'], oracle_result['beta_hat'], alpha=0.3, s=10, label='Oracle')\n",
    "ax.scatter(data['beta'], nn_result.theta_hat[:, 1], alpha=0.3, s=10, label='NN')\n",
    "ax.plot([data['beta'].min(), data['beta'].max()], \n",
    "        [data['beta'].min(), data['beta'].max()], 'k--', lw=2)\n",
    "ax.set_xlabel(r'True $\\beta(X)$'); ax.set_ylabel(r'Estimated $\\beta(X)$')\n",
    "ax.set_title(r'$\\beta(X)$ Recovery'); ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Correlations\n",
    "print(f\"Correlation with true values:\")\n",
    "print(f\"  Oracle alpha: {np.corrcoef(data['alpha'], oracle_result['alpha_hat'])[0,1]:.3f}\")\n",
    "print(f\"  Oracle beta:  {np.corrcoef(data['beta'], oracle_result['beta_hat'])[0,1]:.3f}\")\n",
    "print(f\"  NN alpha:     {np.corrcoef(data['alpha'], nn_result.theta_hat[:,0])[0,1]:.3f}\")\n",
    "print(f\"  NN beta:      {np.corrcoef(data['beta'], nn_result.theta_hat[:,1])[0,1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Section 5: Conclusions\n\n### Key Findings\n\n1. **Oracle validation (M=500):** ~96% coverage, SE ratio ~1.0\n\n2. **NN matches Oracle:** Point estimates and SEs are comparable\n\n3. **Default `lambda_method='ridge'` works well:** Validated 96% coverage\n\n4. **IF-based SE works:** Single run with valid confidence intervals\n\n### References\n\n- Farrell, Liang, Misra (2021) \"Deep Neural Networks for Estimation and Inference\" *Econometrica*\n- Farrell, Liang, Misra (2025) \"Deep Learning for Individual Heterogeneity\""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}