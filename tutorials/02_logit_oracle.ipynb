{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Logit DGP: Neural Network vs Logistic Regression Oracle\n",
        "\n",
        "**Links:**\n",
        "- [GitHub](https://github.com/rawatpranjal/deep-inference)\n",
        "- [PyPI](https://pypi.org/project/deep-inference/)\n",
        "- [Documentation](https://rawatpranjal.github.io/deep-inference/)\n",
        "\n",
        "**References:**\n",
        "- Farrell, Liang, Misra (2021) \"Deep Neural Networks for Estimation and Inference\" *Econometrica*\n",
        "- Farrell, Liang, Misra (2025) \"Deep Learning for Individual Heterogeneity\"\n",
        "\n",
        "---\n",
        "\n",
        "This notebook validates `structural_dml` for **logit** models against correctly-specified logistic regression oracles.\n",
        "\n",
        "## DGP Specification\n",
        "\n",
        "$$P(Y=1 | X, T) = \\sigma(\\alpha(X) + \\beta(X) \\cdot T)$$\n",
        "\n",
        "where $\\sigma(z) = 1/(1 + e^{-z})$ is the sigmoid function.\n",
        "\n",
        "**Target:** $\\mu^* = E[\\beta(X)] = 0.5$\n",
        "\n",
        "## Critical Note: `lambda_method='aggregate'`\n",
        "\n",
        "**IMPORTANT:** For logit models, you MUST use `lambda_method='aggregate'` for stable estimates.\n",
        "\n",
        "The default `lambda_method='mlp'` can produce negative Hessian eigenvalues, leading to wildly unstable estimates. This is because the logit Hessian depends on $\\theta$ through $p(1-p)$, making MLP estimation challenging.\n",
        "\n",
        "```python\n",
        "# CORRECT - stable estimates\n",
        "result = structural_dml(Y, T, X, family='logit', lambda_method='aggregate')\n",
        "\n",
        "# WRONG - may produce unstable estimates  \n",
        "result = structural_dml(Y, T, X, family='logit')  # uses default mlp\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 1: Setup & DGP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import statsmodels.api as sm\n",
        "from scipy import stats\n",
        "from scipy.special import expit  # sigmoid function\n",
        "import warnings\n",
        "import sys\n",
        "from pathlib import Path\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# Use local deep_inference (not pip version)\n",
        "sys.path.insert(0, str(Path.cwd().parent / 'src'))\n",
        "from deep_inference import structural_dml\n",
        "\n",
        "# Plotting style\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (10, 6)\n",
        "plt.rcParams['font.size'] = 11\n",
        "\n",
        "# Suppress warnings for cleaner output\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"Setup complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# DGP Parameters (Simple scenario)\n",
        "# alpha(X) = 1.0 + 0.3*X\n",
        "# beta(X) = 0.5 + 0.2*X\n",
        "# X ~ N(0, 1), so E[X] = 0\n",
        "# Target: mu* = E[beta(X)] = 0.5 + 0.2*0 = 0.5\n",
        "\n",
        "A0, A1 = 1.0, 0.3      # alpha(X) = 1.0 + 0.3*X\n",
        "B0, B1 = 0.5, 0.2      # beta(X) = 0.5 + 0.2*X\n",
        "MU_X = 0.0             # E[X] = 0 (standard normal)\n",
        "MU_TRUE = B0 + B1 * MU_X  # E[beta(X)] = 0.5\n",
        "\n",
        "print(f\"DGP Parameters (Simple Scenario):\")\n",
        "print(f\"  alpha(X) = {A0} + {A1}*X\")\n",
        "print(f\"  beta(X)  = {B0} + {B1}*X\")\n",
        "print(f\"  X ~ N(0, 1), so E[X] = {MU_X}\")\n",
        "print(f\"  True target mu* = E[beta(X)] = {MU_TRUE}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_data_simple(n, seed=None):\n",
        "    \"\"\"\n",
        "    Generate data from the simple logit DGP.\n",
        "    \n",
        "    P(Y=1 | X, T) = sigmoid(alpha(X) + beta(X)*T)\n",
        "    alpha(X) = 1.0 + 0.3*X\n",
        "    beta(X) = 0.5 + 0.2*X\n",
        "    X ~ N(0, 1)\n",
        "    T ~ N(0, 1)\n",
        "    \n",
        "    Returns dict with Y, T, X, alpha_true, beta_true, mu_true\n",
        "    \"\"\"\n",
        "    if seed is not None:\n",
        "        np.random.seed(seed)\n",
        "    \n",
        "    # Generate covariates\n",
        "    X = np.random.normal(0, 1, n)  # Standard normal\n",
        "    T = np.random.normal(0, 1, n)\n",
        "    \n",
        "    # True structural functions\n",
        "    alpha_true = A0 + A1 * X\n",
        "    beta_true = B0 + B1 * X\n",
        "    \n",
        "    # Probability and binary outcome\n",
        "    logits = alpha_true + beta_true * T\n",
        "    p_true = expit(logits)  # sigmoid\n",
        "    Y = np.random.binomial(1, p_true)\n",
        "    \n",
        "    return {\n",
        "        'Y': Y.astype(float),\n",
        "        'T': T,\n",
        "        'X': X.reshape(-1, 1),  # 2D for API compatibility\n",
        "        'X_1d': X,  # 1D for oracle\n",
        "        'alpha_true': alpha_true,\n",
        "        'beta_true': beta_true,\n",
        "        'p_true': p_true,\n",
        "        'mu_true': beta_true.mean()\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate sample data for visualization\n",
        "np.random.seed(42)\n",
        "data = generate_data_simple(1000, seed=42)\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "\n",
        "# Plot 1: Y vs T colored by X\n",
        "ax = axes[0]\n",
        "scatter = ax.scatter(data['T'], data['Y'] + np.random.normal(0, 0.02, len(data['Y'])), \n",
        "                     c=data['X_1d'], cmap='coolwarm', alpha=0.5, s=10)\n",
        "plt.colorbar(scatter, ax=ax, label='X')\n",
        "ax.set_xlabel('T (Treatment)')\n",
        "ax.set_ylabel('Y (Binary Outcome)')\n",
        "ax.set_title('Y vs T (colored by X, jittered)')\n",
        "\n",
        "# Plot 2: True alpha(X) and beta(X)\n",
        "ax = axes[1]\n",
        "X_grid = np.linspace(-3, 3, 100)\n",
        "ax.plot(X_grid, A0 + A1 * X_grid, 'b-', label=r'$\\alpha(X) = 1.0 + 0.3X$', linewidth=2)\n",
        "ax.plot(X_grid, B0 + B1 * X_grid, 'r-', label=r'$\\beta(X) = 0.5 + 0.2X$', linewidth=2)\n",
        "ax.axhline(y=MU_TRUE, color='gray', linestyle='--', alpha=0.5, label=f'$\\mu^* = E[\\\\beta(X)] = {MU_TRUE}$')\n",
        "ax.axvline(x=MU_X, color='gray', linestyle=':', alpha=0.5)\n",
        "ax.set_xlabel('X')\n",
        "ax.set_ylabel('Function value')\n",
        "ax.set_title('True Structural Functions')\n",
        "ax.legend()\n",
        "\n",
        "# Plot 3: Distribution of beta(X)\n",
        "ax = axes[2]\n",
        "ax.hist(data['beta_true'], bins=30, alpha=0.7, edgecolor='black')\n",
        "ax.axvline(x=MU_TRUE, color='red', linestyle='--', linewidth=2, label=f'Pop. $E[\\\\beta(X)]={MU_TRUE}$')\n",
        "ax.axvline(x=data['beta_true'].mean(), color='green', linestyle=':', linewidth=2, \n",
        "           label=f'Sample mean={data[\"beta_true\"].mean():.3f}')\n",
        "ax.set_xlabel(r'$\\beta(X)$')\n",
        "ax.set_ylabel('Frequency')\n",
        "ax.set_title(r'Distribution of $\\beta(X)$')\n",
        "ax.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nSample statistics:\")\n",
        "print(f\"  n = {len(data['Y'])}\")\n",
        "print(f\"  mean(X) = {data['X_1d'].mean():.4f}\")\n",
        "print(f\"  mean(Y) = {data['Y'].mean():.4f} (proportion of Y=1)\")\n",
        "print(f\"  Sample E[beta(X)] = {data['beta_true'].mean():.4f}\")\n",
        "print(f\"  Population E[beta(X)] = {MU_TRUE}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 2: Oracle Implementation\n",
        "\n",
        "The oracle is logistic regression on the correctly specified model: \n",
        "$$\\text{logit}(P(Y=1)) = a_0 + a_1 X + b_0 T + b_1 (X \\cdot T)$$\n",
        "\n",
        "For inference on $E[\\beta(X)] = b_0 + b_1 E[X]$, we need to account for:\n",
        "1. **Naive SE**: Treats $\\bar{X}$ as fixed (WRONG)\n",
        "2. **Delta-corrected SE**: Adds $\\hat{b}_1^2 \\cdot Var(\\bar{X})$ term (CORRECT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def logit_oracle(Y, T, X):\n",
        "    \"\"\"\n",
        "    Logistic regression oracle with both naive and delta-corrected SE.\n",
        "    \n",
        "    Model: logit(P(Y=1)) = a0 + a1*X + b0*T + b1*(X*T)\n",
        "    Target: mu = E[beta(X)] = b0 + b1*E[X], estimated by b0_hat + b1_hat*X_bar\n",
        "    \"\"\"\n",
        "    n = len(Y)\n",
        "    X_bar = X.mean()\n",
        "    \n",
        "    # Design matrix: [1, X, T, X*T]\n",
        "    X_design = np.column_stack([np.ones(n), X, T, X * T])\n",
        "    \n",
        "    # Fit logistic regression\n",
        "    model = sm.Logit(Y, X_design).fit(disp=0)\n",
        "    \n",
        "    # Extract coefficients\n",
        "    a0, a1, b0, b1 = model.params\n",
        "    \n",
        "    # Point estimate of E[beta(X)]\n",
        "    mu_hat = b0 + b1 * X_bar\n",
        "    \n",
        "    # Variance-covariance for b0, b1 (indices 2, 3)\n",
        "    cov = model.cov_params()\n",
        "    var_b0 = cov[2, 2]\n",
        "    var_b1 = cov[3, 3]\n",
        "    cov_b0_b1 = cov[2, 3]\n",
        "    \n",
        "    # Naive SE: treats X_bar as fixed\n",
        "    var_naive = var_b0 + X_bar**2 * var_b1 + 2 * X_bar * cov_b0_b1\n",
        "    se_naive = np.sqrt(max(var_naive, 1e-10))\n",
        "    \n",
        "    # Delta-corrected SE: accounts for Var(X_bar)\n",
        "    var_X_bar = X.var(ddof=1) / n\n",
        "    var_delta = var_naive + b1**2 * var_X_bar\n",
        "    se_delta = np.sqrt(max(var_delta, 1e-10))\n",
        "    \n",
        "    # Fitted structural functions\n",
        "    alpha_hat = a0 + a1 * X\n",
        "    beta_hat = b0 + b1 * X\n",
        "    \n",
        "    return {\n",
        "        'mu_hat': mu_hat,\n",
        "        'se_naive': se_naive,\n",
        "        'se_delta': se_delta,\n",
        "        'params': {'a0': a0, 'a1': a1, 'b0': b0, 'b1': b1},\n",
        "        'alpha_hat': alpha_hat,\n",
        "        'beta_hat': beta_hat,\n",
        "        'X_bar': X_bar\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test the oracle on sample data\n",
        "logit_result = logit_oracle(data['Y'], data['T'], data['X_1d'])\n",
        "\n",
        "print(\"Logit Oracle Results:\")\n",
        "print(f\"\\nParameter estimates:\")\n",
        "print(f\"  a0 = {logit_result['params']['a0']:.4f} (true: {A0})\")\n",
        "print(f\"  a1 = {logit_result['params']['a1']:.4f} (true: {A1})\")\n",
        "print(f\"  b0 = {logit_result['params']['b0']:.4f} (true: {B0})\")\n",
        "print(f\"  b1 = {logit_result['params']['b1']:.4f} (true: {B1})\")\n",
        "\n",
        "print(f\"\\nInference on E[beta(X)]:\")\n",
        "print(f\"  Point estimate: {logit_result['mu_hat']:.4f} (true: {MU_TRUE})\")\n",
        "print(f\"  Naive SE:       {logit_result['se_naive']:.4f}\")\n",
        "print(f\"  Delta SE:       {logit_result['se_delta']:.4f}\")\n",
        "\n",
        "# 95% CIs\n",
        "ci_naive = (logit_result['mu_hat'] - 1.96*logit_result['se_naive'], \n",
        "            logit_result['mu_hat'] + 1.96*logit_result['se_naive'])\n",
        "ci_delta = (logit_result['mu_hat'] - 1.96*logit_result['se_delta'], \n",
        "            logit_result['mu_hat'] + 1.96*logit_result['se_delta'])\n",
        "\n",
        "print(f\"\\n95% Confidence Intervals:\")\n",
        "print(f\"  Naive: [{ci_naive[0]:.4f}, {ci_naive[1]:.4f}]\")\n",
        "print(f\"  Delta: [{ci_delta[0]:.4f}, {ci_delta[1]:.4f}]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Math Verification: Delta Method\n",
        "\n",
        "The estimator $\\hat{\\mu} = \\hat{b}_0 + \\hat{b}_1 \\bar{X}$ targets $\\mu^* = b_0 + b_1 \\mu_X$.\n",
        "\n",
        "**Delta method expansion:**\n",
        "$$\\hat{\\mu} - \\mu^* \\approx (\\hat{b}_0 - b_0) + \\mu_X(\\hat{b}_1 - b_1) + b_1(\\bar{X} - \\mu_X)$$\n",
        "\n",
        "**Variance (assuming $(\\hat{b}_0, \\hat{b}_1) \\perp \\bar{X}$):**\n",
        "$$\\text{Var}(\\hat{\\mu}) = \\underbrace{\\text{Var}(\\hat{b}_0) + \\mu_X^2 \\text{Var}(\\hat{b}_1) + 2\\mu_X \\text{Cov}(\\hat{b}_0, \\hat{b}_1)}_{\\text{naive}} + \\underbrace{b_1^2 \\text{Var}(\\bar{X})}_{\\text{delta correction}}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Numerical verification of delta method formula\n",
        "print(\"Delta Method Formula Verification:\")\n",
        "print(f\"  Naive SE^2:     {logit_result['se_naive']**2:.8f}\")\n",
        "print(f\"  Delta SE^2:     {logit_result['se_delta']**2:.8f}\")\n",
        "print(f\"  Difference:     {logit_result['se_delta']**2 - logit_result['se_naive']**2:.8f}\")\n",
        "\n",
        "# The difference should equal b1^2 * Var(X_bar)\n",
        "b1_sq_var_xbar = logit_result['params']['b1']**2 * (data['X_1d'].var(ddof=1) / len(data['Y']))\n",
        "print(f\"  b1^2 * Var(X_bar): {b1_sq_var_xbar:.8f}\")\n",
        "\n",
        "match = np.isclose(\n",
        "    logit_result['se_delta']**2 - logit_result['se_naive']**2, \n",
        "    b1_sq_var_xbar, rtol=1e-4\n",
        ")\n",
        "print(f\"  Formula matches: {match}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 3: Single-Run Comparison\n",
        "\n",
        "Compare Logit oracle vs Neural Network on a single dataset.\n",
        "\n",
        "**CRITICAL:** We use `lambda_method='aggregate'` for stable Hessian estimation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate fresh data for comparison\n",
        "np.random.seed(123)\n",
        "n = 1000\n",
        "data = generate_data_simple(n, seed=123)\n",
        "\n",
        "print(f\"Generated n={n} observations\")\n",
        "print(f\"Sample E[beta(X)] = {data['beta_true'].mean():.4f}\")\n",
        "print(f\"Sample mean(Y) = {data['Y'].mean():.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fit Logit Oracle\n",
        "logit_result = logit_oracle(data['Y'], data['T'], data['X_1d'])\n",
        "\n",
        "print(\"Logit Oracle:\")\n",
        "print(f\"  mu_hat = {logit_result['mu_hat']:.4f} (true: {MU_TRUE})\")\n",
        "print(f\"  SE (delta) = {logit_result['se_delta']:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fit Neural Network with lambda_method='aggregate' (CRITICAL!)\n",
        "nn_result = structural_dml(\n",
        "    Y=data['Y'],\n",
        "    T=data['T'],\n",
        "    X=data['X'],  # Already 2D\n",
        "    family='logit',\n",
        "    lambda_method='aggregate',  # CRITICAL for logit stability!\n",
        "    epochs=100,\n",
        "    n_folds=50,\n",
        "    hidden_dims=[64, 32],\n",
        "    lr=0.01,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "print(\"Neural Network (structural_dml with lambda_method='aggregate'):\")\n",
        "print(f\"  mu_hat = {nn_result.mu_hat:.4f} (true: {MU_TRUE})\")\n",
        "print(f\"  SE = {nn_result.se:.4f}\")\n",
        "print(f\"  95% CI: [{nn_result.ci_lower:.4f}, {nn_result.ci_upper:.4f}]\")\n",
        "print(f\"  mu_naive = {nn_result.mu_naive:.4f}\")\n",
        "\n",
        "# Check if CI covers true value\n",
        "covered = nn_result.ci_lower <= MU_TRUE <= nn_result.ci_upper\n",
        "print(f\"\\n  CI covers true value: {covered}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### a) Parameter Recovery: $\\alpha(X)$ and $\\beta(X)$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract fitted structural functions\n",
        "alpha_logit = logit_result['alpha_hat']\n",
        "beta_logit = logit_result['beta_hat']\n",
        "\n",
        "alpha_nn = nn_result.theta_hat[:, 0]\n",
        "beta_nn = nn_result.theta_hat[:, 1]\n",
        "\n",
        "# Compute recovery metrics\n",
        "def compute_metrics(estimated, true):\n",
        "    rmse = np.sqrt(np.mean((estimated - true)**2))\n",
        "    corr = np.corrcoef(estimated, true)[0, 1]\n",
        "    bias = np.mean(estimated - true)\n",
        "    return {'rmse': rmse, 'corr': corr, 'bias': bias}\n",
        "\n",
        "metrics = {\n",
        "    'Logit': {\n",
        "        'alpha': compute_metrics(alpha_logit, data['alpha_true']),\n",
        "        'beta': compute_metrics(beta_logit, data['beta_true'])\n",
        "    },\n",
        "    'NN': {\n",
        "        'alpha': compute_metrics(alpha_nn, data['alpha_true']),\n",
        "        'beta': compute_metrics(beta_nn, data['beta_true'])\n",
        "    }\n",
        "}\n",
        "\n",
        "print(\"Parameter Recovery Metrics:\")\n",
        "print(\"\\n\" + \"=\"*65)\n",
        "print(f\"{'Metric':<15} {'Logit alpha':<13} {'Logit beta':<13} {'NN alpha':<13} {'NN beta':<13}\")\n",
        "print(\"=\"*65)\n",
        "print(f\"{'RMSE':<15} {metrics['Logit']['alpha']['rmse']:<13.4f} {metrics['Logit']['beta']['rmse']:<13.4f} {metrics['NN']['alpha']['rmse']:<13.4f} {metrics['NN']['beta']['rmse']:<13.4f}\")\n",
        "print(f\"{'Correlation':<15} {metrics['Logit']['alpha']['corr']:<13.4f} {metrics['Logit']['beta']['corr']:<13.4f} {metrics['NN']['alpha']['corr']:<13.4f} {metrics['NN']['beta']['corr']:<13.4f}\")\n",
        "print(f\"{'Bias':<15} {metrics['Logit']['alpha']['bias']:<13.4f} {metrics['Logit']['beta']['bias']:<13.4f} {metrics['NN']['alpha']['bias']:<13.4f} {metrics['NN']['beta']['bias']:<13.4f}\")\n",
        "print(\"=\"*65)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize parameter recovery\n",
        "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
        "\n",
        "# Logit alpha\n",
        "ax = axes[0, 0]\n",
        "ax.scatter(data['alpha_true'], alpha_logit, alpha=0.3, s=10)\n",
        "ax.plot([data['alpha_true'].min(), data['alpha_true'].max()], \n",
        "        [data['alpha_true'].min(), data['alpha_true'].max()], 'r--', linewidth=2)\n",
        "ax.set_xlabel(r'True $\\alpha(X)$')\n",
        "ax.set_ylabel(r'Logit $\\hat{\\alpha}(X)$')\n",
        "ax.set_title(f'Logit: Corr={metrics[\"Logit\"][\"alpha\"][\"corr\"]:.3f}')\n",
        "\n",
        "# Logit beta\n",
        "ax = axes[0, 1]\n",
        "ax.scatter(data['beta_true'], beta_logit, alpha=0.3, s=10)\n",
        "ax.plot([data['beta_true'].min(), data['beta_true'].max()], \n",
        "        [data['beta_true'].min(), data['beta_true'].max()], 'r--', linewidth=2)\n",
        "ax.set_xlabel(r'True $\\beta(X)$')\n",
        "ax.set_ylabel(r'Logit $\\hat{\\beta}(X)$')\n",
        "ax.set_title(f'Logit: Corr={metrics[\"Logit\"][\"beta\"][\"corr\"]:.3f}')\n",
        "\n",
        "# NN alpha\n",
        "ax = axes[1, 0]\n",
        "ax.scatter(data['alpha_true'], alpha_nn, alpha=0.3, s=10)\n",
        "ax.plot([data['alpha_true'].min(), data['alpha_true'].max()], \n",
        "        [data['alpha_true'].min(), data['alpha_true'].max()], 'r--', linewidth=2)\n",
        "ax.set_xlabel(r'True $\\alpha(X)$')\n",
        "ax.set_ylabel(r'NN $\\hat{\\alpha}(X)$')\n",
        "ax.set_title(f'NN: Corr={metrics[\"NN\"][\"alpha\"][\"corr\"]:.3f}')\n",
        "\n",
        "# NN beta\n",
        "ax = axes[1, 1]\n",
        "ax.scatter(data['beta_true'], beta_nn, alpha=0.3, s=10)\n",
        "ax.plot([data['beta_true'].min(), data['beta_true'].max()], \n",
        "        [data['beta_true'].min(), data['beta_true'].max()], 'r--', linewidth=2)\n",
        "ax.set_xlabel(r'True $\\beta(X)$')\n",
        "ax.set_ylabel(r'NN $\\hat{\\beta}(X)$')\n",
        "ax.set_title(f'NN: Corr={metrics[\"NN\"][\"beta\"][\"corr\"]:.3f}')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### b) Training Diagnostics: Hessian Stability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract training diagnostics from NN result\n",
        "diagnostics = nn_result.diagnostics\n",
        "\n",
        "print(\"Neural Network Training Diagnostics:\")\n",
        "print(f\"  Min Lambda eigenvalue: {diagnostics.get('min_lambda_eigenvalue', 'N/A')}\")\n",
        "print(f\"  Pct regularized: {diagnostics.get('pct_regularized', 'N/A')}%\")\n",
        "print(f\"  Correction ratio: {diagnostics.get('correction_ratio', 'N/A')}\")\n",
        "\n",
        "# For logit, check Hessian stability (critical!)\n",
        "min_lambda = diagnostics.get('min_lambda_eigenvalue', None)\n",
        "if min_lambda is not None:\n",
        "    print(f\"\\nHessian Stability Check (CRITICAL for logit):\")\n",
        "    print(f\"  min(lambda) = {min_lambda:.6f}\")\n",
        "    if min_lambda > 0:\n",
        "        print(f\"  [PASS] Positive eigenvalue - Hessian is stable\")\n",
        "    else:\n",
        "        print(f\"  [FAIL] Negative eigenvalue - use lambda_method='aggregate'!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 4: Monte Carlo Simulation (Simple DGP)\n",
        "\n",
        "Run M replications to assess:\n",
        "- Bias and variance of $\\hat{\\mu}$ estimates\n",
        "- Coverage and SE calibration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Monte Carlo configuration\n",
        "M = 100          # Number of replications\n",
        "N = 1000         # Sample size per replication\n",
        "N_FOLDS = 50     # Cross-fitting folds for NN\n",
        "EPOCHS = 100     # Training epochs\n",
        "\n",
        "print(f\"Monte Carlo Configuration:\")\n",
        "print(f\"  M = {M} replications\")\n",
        "print(f\"  n = {N} observations per replication\")\n",
        "print(f\"  K = {N_FOLDS} folds for NN\")\n",
        "print(f\"  True target mu* = {MU_TRUE}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_single_mc_simple(sim_id, n, n_folds, epochs):\n",
        "    \"\"\"\n",
        "    Run a single Monte Carlo replication for simple DGP.\n",
        "    \"\"\"\n",
        "    # Generate data\n",
        "    data = generate_data_simple(n)\n",
        "    \n",
        "    # Logit Oracle\n",
        "    logit = logit_oracle(data['Y'], data['T'], data['X_1d'])\n",
        "    \n",
        "    # Neural Network with lambda_method='aggregate'\n",
        "    nn = structural_dml(\n",
        "        Y=data['Y'],\n",
        "        T=data['T'],\n",
        "        X=data['X'],\n",
        "        family='logit',\n",
        "        lambda_method='aggregate',  # CRITICAL!\n",
        "        epochs=epochs,\n",
        "        n_folds=n_folds,\n",
        "        hidden_dims=[64, 32],\n",
        "        lr=0.01,\n",
        "        verbose=False\n",
        "    )\n",
        "    \n",
        "    # Coverage indicators\n",
        "    mu_true = MU_TRUE\n",
        "    \n",
        "    # Logit naive CI\n",
        "    ci_naive_lo = logit['mu_hat'] - 1.96 * logit['se_naive']\n",
        "    ci_naive_hi = logit['mu_hat'] + 1.96 * logit['se_naive']\n",
        "    covered_naive = (ci_naive_lo <= mu_true <= ci_naive_hi)\n",
        "    \n",
        "    # Logit delta CI\n",
        "    ci_delta_lo = logit['mu_hat'] - 1.96 * logit['se_delta']\n",
        "    ci_delta_hi = logit['mu_hat'] + 1.96 * logit['se_delta']\n",
        "    covered_delta = (ci_delta_lo <= mu_true <= ci_delta_hi)\n",
        "    \n",
        "    # NN IF CI\n",
        "    covered_nn = (nn.ci_lower <= mu_true <= nn.ci_upper)\n",
        "    \n",
        "    # Parameter recovery metrics\n",
        "    alpha_nn = nn.theta_hat[:, 0]\n",
        "    beta_nn = nn.theta_hat[:, 1]\n",
        "    \n",
        "    corr_alpha_logit = np.corrcoef(logit['alpha_hat'], data['alpha_true'])[0, 1]\n",
        "    corr_beta_logit = np.corrcoef(logit['beta_hat'], data['beta_true'])[0, 1]\n",
        "    corr_alpha_nn = np.corrcoef(alpha_nn, data['alpha_true'])[0, 1]\n",
        "    corr_beta_nn = np.corrcoef(beta_nn, data['beta_true'])[0, 1]\n",
        "    \n",
        "    return {\n",
        "        'sim_id': sim_id,\n",
        "        'sample_mu_true': data['mu_true'],\n",
        "        \n",
        "        # Logit estimates\n",
        "        'logit_mu': logit['mu_hat'],\n",
        "        'logit_se_naive': logit['se_naive'],\n",
        "        'logit_se_delta': logit['se_delta'],\n",
        "        'logit_covered_naive': covered_naive,\n",
        "        'logit_covered_delta': covered_delta,\n",
        "        \n",
        "        # NN estimates\n",
        "        'nn_mu': nn.mu_hat,\n",
        "        'nn_mu_naive': nn.mu_naive,\n",
        "        'nn_se': nn.se,\n",
        "        'nn_covered': covered_nn,\n",
        "        \n",
        "        # Parameter recovery\n",
        "        'corr_alpha_logit': corr_alpha_logit,\n",
        "        'corr_beta_logit': corr_beta_logit,\n",
        "        'corr_alpha_nn': corr_alpha_nn,\n",
        "        'corr_beta_nn': corr_beta_nn,\n",
        "        \n",
        "        # Diagnostics\n",
        "        'min_lambda': nn.diagnostics.get('min_lambda_eigenvalue', np.nan),\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run Monte Carlo simulation\n",
        "results = []\n",
        "\n",
        "for sim_id in tqdm(range(M), desc='Monte Carlo (Simple DGP)'):\n",
        "    np.random.seed(sim_id + 1000)\n",
        "    result = run_single_mc_simple(sim_id, N, N_FOLDS, EPOCHS)\n",
        "    results.append(result)\n",
        "\n",
        "df = pd.DataFrame(results)\n",
        "print(f\"\\nCompleted {M} Monte Carlo replications.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### c) Bias and Variance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute bias and variance metrics\n",
        "mu_true = MU_TRUE\n",
        "\n",
        "# Logit\n",
        "logit_bias = df['logit_mu'].mean() - mu_true\n",
        "logit_var = df['logit_mu'].var()\n",
        "logit_rmse = np.sqrt(logit_bias**2 + logit_var)\n",
        "logit_se_emp = df['logit_mu'].std()\n",
        "\n",
        "# NN\n",
        "nn_bias = df['nn_mu'].mean() - mu_true\n",
        "nn_var = df['nn_mu'].var()\n",
        "nn_rmse = np.sqrt(nn_bias**2 + nn_var)\n",
        "nn_se_emp = df['nn_mu'].std()\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(f\"BIAS AND VARIANCE (target mu* = {MU_TRUE})\")\n",
        "print(\"=\"*60)\n",
        "print(f\"{'Metric':<20} {'Logit Oracle':<15} {'Neural Net':<15}\")\n",
        "print(\"-\"*60)\n",
        "print(f\"{'Mean estimate':<20} {df['logit_mu'].mean():<15.4f} {df['nn_mu'].mean():<15.4f}\")\n",
        "print(f\"{'Bias':<20} {logit_bias:<15.4f} {nn_bias:<15.4f}\")\n",
        "print(f\"{'Variance':<20} {logit_var:<15.4f} {nn_var:<15.4f}\")\n",
        "print(f\"{'RMSE':<20} {logit_rmse:<15.4f} {nn_rmse:<15.4f}\")\n",
        "print(f\"{'Empirical SE':<20} {logit_se_emp:<15.4f} {nn_se_emp:<15.4f}\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### d) Coverage and SE Calibration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Coverage and SE calibration\n",
        "coverage_naive = df['logit_covered_naive'].mean()\n",
        "coverage_delta = df['logit_covered_delta'].mean()\n",
        "coverage_nn = df['nn_covered'].mean()\n",
        "\n",
        "# SE ratios (mean estimated SE / empirical SE)\n",
        "se_ratio_naive = df['logit_se_naive'].mean() / logit_se_emp\n",
        "se_ratio_delta = df['logit_se_delta'].mean() / logit_se_emp\n",
        "se_ratio_nn = df['nn_se'].mean() / nn_se_emp\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"COVERAGE AND SE CALIBRATION\")\n",
        "print(\"=\"*70)\n",
        "print(f\"{'Metric':<20} {'Logit Naive':<15} {'Logit Delta':<15} {'Neural Net':<15}\")\n",
        "print(\"-\"*70)\n",
        "print(f\"{'Coverage':<20} {coverage_naive:<15.1%} {coverage_delta:<15.1%} {coverage_nn:<15.1%}\")\n",
        "print(f\"{'Mean Est SE':<20} {df['logit_se_naive'].mean():<15.4f} {df['logit_se_delta'].mean():<15.4f} {df['nn_se'].mean():<15.4f}\")\n",
        "print(f\"{'Empirical SE':<20} {logit_se_emp:<15.4f} {logit_se_emp:<15.4f} {nn_se_emp:<15.4f}\")\n",
        "print(f\"{'SE Ratio':<20} {se_ratio_naive:<15.2f} {se_ratio_delta:<15.2f} {se_ratio_nn:<15.2f}\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\\nTarget ranges:\")\n",
        "print(\"  Coverage: 88-97%\")\n",
        "print(\"  SE Ratio: 0.85-1.15\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hessian stability summary\n",
        "print(\"\\nHessian Stability (with lambda_method='aggregate'):\")\n",
        "print(f\"  Mean min(lambda): {df['min_lambda'].mean():.6f}\")\n",
        "print(f\"  Min min(lambda): {df['min_lambda'].min():.6f}\")\n",
        "print(f\"  Pct with min(lambda) > 0: {(df['min_lambda'] > 0).mean():.1%}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Parameter recovery summary\n",
        "print(\"=\"*60)\n",
        "print(\"PARAMETER RECOVERY (Correlation with true values)\")\n",
        "print(\"=\"*60)\n",
        "print(f\"{'Parameter':<15} {'Logit Oracle':<15} {'Neural Net':<15}\")\n",
        "print(\"-\"*60)\n",
        "print(f\"{'alpha(X)':<15} {df['corr_alpha_logit'].mean():<15.3f} {df['corr_alpha_nn'].mean():<15.3f}\")\n",
        "print(f\"{'beta(X)':<15} {df['corr_beta_logit'].mean():<15.3f} {df['corr_beta_nn'].mean():<15.3f}\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 5: Complexity Scaling\n",
        "\n",
        "Test how both methods handle increasing complexity:\n",
        "\n",
        "**a) Simple:** Linear heterogeneity (1 covariate)  \n",
        "**b) Complex:** Nonlinear heterogeneity (5 covariates)  \n",
        "**c) High-dim:** Many noise covariates (20 covariates, only 2 signal)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Complex DGP: Nonlinear heterogeneity\n",
        "def generate_data_complex(n, seed=None):\n",
        "    \"\"\"\n",
        "    Generate data from complex logit DGP with nonlinear heterogeneity.\n",
        "    \n",
        "    alpha(X) = sin(2*pi*X1) + 0.5*X2^2\n",
        "    beta(X) = 0.5 + 0.3*cos(pi*X1) + 0.2*tanh(X2)\n",
        "    X ~ N(0, 1), d=5\n",
        "    \n",
        "    Target: mu* = E[beta(X)] â‰ˆ 0.5 (by design, cos and tanh center around 0)\n",
        "    \"\"\"\n",
        "    if seed is not None:\n",
        "        np.random.seed(seed)\n",
        "    \n",
        "    d = 5\n",
        "    X = np.random.normal(0, 1, (n, d))\n",
        "    T = np.random.normal(0, 1, n)\n",
        "    \n",
        "    # Nonlinear structural functions\n",
        "    alpha_true = np.sin(2 * np.pi * X[:, 0]) + 0.5 * X[:, 1]**2\n",
        "    beta_true = 0.5 + 0.3 * np.cos(np.pi * X[:, 0]) + 0.2 * np.tanh(X[:, 1])\n",
        "    \n",
        "    logits = alpha_true + beta_true * T\n",
        "    p_true = expit(logits)\n",
        "    Y = np.random.binomial(1, p_true)\n",
        "    \n",
        "    return {\n",
        "        'Y': Y.astype(float),\n",
        "        'T': T,\n",
        "        'X': X,\n",
        "        'alpha_true': alpha_true,\n",
        "        'beta_true': beta_true,\n",
        "        'mu_true': beta_true.mean()\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# High-dim DGP: Many noise covariates\n",
        "def generate_data_highdim(n, seed=None):\n",
        "    \"\"\"\n",
        "    Generate data from high-dimensional logit DGP.\n",
        "    \n",
        "    alpha(X) = 1.0 + 0.3*X1 + 0.2*X2\n",
        "    beta(X) = 0.5 + 0.2*X1\n",
        "    X ~ N(0, 1), d=20 (only X1, X2 are signal, rest are noise)\n",
        "    \n",
        "    Target: mu* = E[beta(X)] = 0.5\n",
        "    \"\"\"\n",
        "    if seed is not None:\n",
        "        np.random.seed(seed)\n",
        "    \n",
        "    d = 20  # 2 signal, 18 noise\n",
        "    X = np.random.normal(0, 1, (n, d))\n",
        "    T = np.random.normal(0, 1, n)\n",
        "    \n",
        "    # Only first two covariates matter\n",
        "    alpha_true = 1.0 + 0.3 * X[:, 0] + 0.2 * X[:, 1]\n",
        "    beta_true = 0.5 + 0.2 * X[:, 0]\n",
        "    \n",
        "    logits = alpha_true + beta_true * T\n",
        "    p_true = expit(logits)\n",
        "    Y = np.random.binomial(1, p_true)\n",
        "    \n",
        "    return {\n",
        "        'Y': Y.astype(float),\n",
        "        'T': T,\n",
        "        'X': X,\n",
        "        'alpha_true': alpha_true,\n",
        "        'beta_true': beta_true,\n",
        "        'mu_true': beta_true.mean()\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_scenario(dgp_name, generate_fn, mu_true_pop, n_mc=30):\n",
        "    \"\"\"\n",
        "    Run a mini MC study for a given DGP.\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Scenario: {dgp_name}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    nn_mus = []\n",
        "    nn_covered = []\n",
        "    nn_ses = []\n",
        "    corr_betas = []\n",
        "    min_lambdas = []\n",
        "    \n",
        "    for sim_id in tqdm(range(n_mc), desc=dgp_name):\n",
        "        np.random.seed(sim_id + 5000)\n",
        "        data = generate_fn(1000)\n",
        "        \n",
        "        nn = structural_dml(\n",
        "            Y=data['Y'],\n",
        "            T=data['T'],\n",
        "            X=data['X'],\n",
        "            family='logit',\n",
        "            lambda_method='aggregate',\n",
        "            epochs=100,\n",
        "            n_folds=50,\n",
        "            hidden_dims=[64, 32],\n",
        "            lr=0.01,\n",
        "            verbose=False\n",
        "        )\n",
        "        \n",
        "        nn_mus.append(nn.mu_hat)\n",
        "        nn_ses.append(nn.se)\n",
        "        nn_covered.append(nn.ci_lower <= mu_true_pop <= nn.ci_upper)\n",
        "        \n",
        "        beta_nn = nn.theta_hat[:, 1]\n",
        "        corr_betas.append(np.corrcoef(beta_nn, data['beta_true'])[0, 1])\n",
        "        min_lambdas.append(nn.diagnostics.get('min_lambda_eigenvalue', np.nan))\n",
        "    \n",
        "    nn_mus = np.array(nn_mus)\n",
        "    nn_ses = np.array(nn_ses)\n",
        "    \n",
        "    bias = nn_mus.mean() - mu_true_pop\n",
        "    emp_se = nn_mus.std()\n",
        "    se_ratio = np.mean(nn_ses) / emp_se\n",
        "    coverage = np.mean(nn_covered)\n",
        "    corr_beta_mean = np.mean(corr_betas)\n",
        "    min_lambda_mean = np.nanmean(min_lambdas)\n",
        "    \n",
        "    print(f\"\\nResults (M={n_mc}, N=1000):\")\n",
        "    print(f\"  True mu*:     {mu_true_pop}\")\n",
        "    print(f\"  Mean mu_hat:  {nn_mus.mean():.4f}\")\n",
        "    print(f\"  Bias:         {bias:.4f}\")\n",
        "    print(f\"  Emp SE:       {emp_se:.4f}\")\n",
        "    print(f\"  Mean Est SE:  {np.mean(nn_ses):.4f}\")\n",
        "    print(f\"  SE Ratio:     {se_ratio:.2f}\")\n",
        "    print(f\"  Coverage:     {coverage:.1%}\")\n",
        "    print(f\"  Corr(beta):   {corr_beta_mean:.3f}\")\n",
        "    print(f\"  Min(lambda):  {min_lambda_mean:.6f}\")\n",
        "    \n",
        "    return {\n",
        "        'dgp': dgp_name,\n",
        "        'mu_true': mu_true_pop,\n",
        "        'mean_mu': nn_mus.mean(),\n",
        "        'bias': bias,\n",
        "        'emp_se': emp_se,\n",
        "        'se_ratio': se_ratio,\n",
        "        'coverage': coverage,\n",
        "        'corr_beta': corr_beta_mean,\n",
        "        'min_lambda': min_lambda_mean\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run all three scenarios (fewer MC reps for speed)\n",
        "scenario_results = []\n",
        "\n",
        "# a) Simple (reuse earlier function)\n",
        "scenario_results.append(run_scenario(\n",
        "    'Simple (1D linear)', \n",
        "    generate_data_simple, \n",
        "    mu_true_pop=0.5,\n",
        "    n_mc=30\n",
        "))\n",
        "\n",
        "# b) Complex (nonlinear)\n",
        "scenario_results.append(run_scenario(\n",
        "    'Complex (5D nonlinear)', \n",
        "    generate_data_complex, \n",
        "    mu_true_pop=0.5,  # By design\n",
        "    n_mc=30\n",
        "))\n",
        "\n",
        "# c) High-dim (noise)\n",
        "scenario_results.append(run_scenario(\n",
        "    'High-dim (20D, 2 signal)', \n",
        "    generate_data_highdim, \n",
        "    mu_true_pop=0.5,\n",
        "    n_mc=30\n",
        "))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Summary table across scenarios\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"COMPLEXITY SCALING SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "scenario_df = pd.DataFrame(scenario_results)\n",
        "print(scenario_df[['dgp', 'bias', 'se_ratio', 'coverage', 'corr_beta']].to_string(index=False))\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 6: Results Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Histogram of estimates (from main MC)\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Logit estimates\n",
        "ax = axes[0]\n",
        "ax.hist(df['logit_mu'], bins=20, alpha=0.7, edgecolor='black', label='Logit')\n",
        "ax.axvline(x=MU_TRUE, color='red', linestyle='--', linewidth=2, label=f'True $\\mu^*$={MU_TRUE}')\n",
        "ax.axvline(x=df['logit_mu'].mean(), color='blue', linestyle=':', linewidth=2, \n",
        "           label=f'Mean={df[\"logit_mu\"].mean():.4f}')\n",
        "ax.set_xlabel(r'$\\hat{\\mu}$')\n",
        "ax.set_ylabel('Frequency')\n",
        "ax.set_title(f'Logit Oracle Estimates (Coverage: {coverage_delta:.1%})')\n",
        "ax.legend()\n",
        "\n",
        "# NN estimates\n",
        "ax = axes[1]\n",
        "ax.hist(df['nn_mu'], bins=20, alpha=0.7, edgecolor='black', color='orange', label='NN')\n",
        "ax.axvline(x=MU_TRUE, color='red', linestyle='--', linewidth=2, label=f'True $\\mu^*$={MU_TRUE}')\n",
        "ax.axvline(x=df['nn_mu'].mean(), color='darkorange', linestyle=':', linewidth=2, \n",
        "           label=f'Mean={df[\"nn_mu\"].mean():.4f}')\n",
        "ax.set_xlabel(r'$\\hat{\\mu}$')\n",
        "ax.set_ylabel('Frequency')\n",
        "ax.set_title(f'Neural Net Estimates (Coverage: {coverage_nn:.1%})')\n",
        "ax.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# QQ plots for t-statistics\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "\n",
        "# Logit Naive t-stats\n",
        "t_naive = (df['logit_mu'] - MU_TRUE) / df['logit_se_naive']\n",
        "ax = axes[0]\n",
        "stats.probplot(t_naive, dist='norm', plot=ax)\n",
        "ax.set_title(f'Logit Naive t-stats\\n(Coverage: {coverage_naive:.1%})')\n",
        "\n",
        "# Logit Delta t-stats\n",
        "t_delta = (df['logit_mu'] - MU_TRUE) / df['logit_se_delta']\n",
        "ax = axes[1]\n",
        "stats.probplot(t_delta, dist='norm', plot=ax)\n",
        "ax.set_title(f'Logit Delta t-stats\\n(Coverage: {coverage_delta:.1%})')\n",
        "\n",
        "# NN t-stats\n",
        "t_nn = (df['nn_mu'] - MU_TRUE) / df['nn_se']\n",
        "ax = axes[2]\n",
        "stats.probplot(t_nn, dist='norm', plot=ax)\n",
        "ax.set_title(f'Neural Net t-stats\\n(Coverage: {coverage_nn:.1%})')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Coverage comparison\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "\n",
        "methods = ['Logit Naive', 'Logit Delta', 'Neural Net (IF)']\n",
        "coverages = [coverage_naive, coverage_delta, coverage_nn]\n",
        "colors = ['#ff7f7f', '#7fbf7f', '#7f7fff']\n",
        "\n",
        "bars = ax.bar(methods, coverages, color=colors, edgecolor='black')\n",
        "ax.axhline(y=0.95, color='red', linestyle='--', linewidth=2, label='Target (95%)')\n",
        "ax.axhspan(0.88, 0.97, alpha=0.2, color='green', label='Valid range (88-97%)')\n",
        "\n",
        "ax.set_ylabel('Coverage')\n",
        "ax.set_title('95% CI Coverage Comparison (Logit, $\\mu^* = 0.5$)')\n",
        "ax.set_ylim(0.7, 1.0)\n",
        "ax.legend()\n",
        "\n",
        "for bar, cov in zip(bars, coverages):\n",
        "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
        "            f'{cov:.1%}', ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 7: Conclusions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Final summary table\n",
        "print(\"=\"*70)\n",
        "print(\"FINAL SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\nDGP: P(Y=1) = sigmoid(alpha(X) + beta(X)*T)\")\n",
        "print(f\"     alpha(X) = {A0} + {A1}*X\")\n",
        "print(f\"     beta(X) = {B0} + {B1}*X\")\n",
        "print(f\"     Target: E[beta(X)] = {MU_TRUE}\")\n",
        "print(f\"\\nMonte Carlo: M={M} replications, n={N} observations each\")\n",
        "print()\n",
        "\n",
        "# Summary table\n",
        "summary_data = {\n",
        "    'Metric': ['Bias', 'Variance', 'RMSE', 'Empirical SE', 'Mean Est SE', 'SE Ratio', 'Coverage'],\n",
        "    'Logit Naive': [f'{logit_bias:.4f}', f'{logit_var:.4f}', f'{logit_rmse:.4f}', \n",
        "                  f'{logit_se_emp:.4f}', f'{df[\"logit_se_naive\"].mean():.4f}', \n",
        "                  f'{se_ratio_naive:.2f}', f'{coverage_naive:.1%}'],\n",
        "    'Logit Delta': [f'{logit_bias:.4f}', f'{logit_var:.4f}', f'{logit_rmse:.4f}', \n",
        "                  f'{logit_se_emp:.4f}', f'{df[\"logit_se_delta\"].mean():.4f}', \n",
        "                  f'{se_ratio_delta:.2f}', f'{coverage_delta:.1%}'],\n",
        "    'Neural Net': [f'{nn_bias:.4f}', f'{nn_var:.4f}', f'{nn_rmse:.4f}', \n",
        "                   f'{nn_se_emp:.4f}', f'{df[\"nn_se\"].mean():.4f}', \n",
        "                   f'{se_ratio_nn:.2f}', f'{coverage_nn:.1%}']\n",
        "}\n",
        "summary_df = pd.DataFrame(summary_data)\n",
        "print(summary_df.to_string(index=False))\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"VALIDATION CHECK\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "checks = []\n",
        "\n",
        "if 0.88 <= coverage_delta <= 0.97:\n",
        "    checks.append(f\"[PASS] Logit Delta coverage: {coverage_delta:.1%} (target: 88-97%)\")\n",
        "else:\n",
        "    checks.append(f\"[FAIL] Logit Delta coverage: {coverage_delta:.1%} (target: 88-97%)\")\n",
        "\n",
        "if 0.85 <= coverage_nn <= 0.97:\n",
        "    checks.append(f\"[PASS] NN coverage: {coverage_nn:.1%} (target: 85-97%)\")\n",
        "else:\n",
        "    checks.append(f\"[WARN] NN coverage: {coverage_nn:.1%} (target: 85-97%)\")\n",
        "\n",
        "if 0.85 <= se_ratio_delta <= 1.15:\n",
        "    checks.append(f\"[PASS] Logit Delta SE ratio: {se_ratio_delta:.2f} (target: 0.85-1.15)\")\n",
        "else:\n",
        "    checks.append(f\"[FAIL] Logit Delta SE ratio: {se_ratio_delta:.2f} (target: 0.85-1.15)\")\n",
        "\n",
        "if 0.80 <= se_ratio_nn <= 1.20:\n",
        "    checks.append(f\"[PASS] NN SE ratio: {se_ratio_nn:.2f} (target: 0.80-1.20)\")\n",
        "else:\n",
        "    checks.append(f\"[WARN] NN SE ratio: {se_ratio_nn:.2f} (target: 0.80-1.20)\")\n",
        "\n",
        "min_lambda_pct = (df['min_lambda'] > 0).mean()\n",
        "if min_lambda_pct > 0.95:\n",
        "    checks.append(f\"[PASS] Hessian stability: {min_lambda_pct:.1%} with positive eigenvalues\")\n",
        "else:\n",
        "    checks.append(f\"[WARN] Hessian stability: {min_lambda_pct:.1%} with positive eigenvalues\")\n",
        "\n",
        "for check in checks:\n",
        "    print(check)\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Key Findings\n",
        "\n",
        "1. **`lambda_method='aggregate'` is REQUIRED for logit models.** The default MLP-based Hessian estimation produces negative eigenvalues, leading to wildly unstable estimates. Always use:\n",
        "   ```python\n",
        "   structural_dml(..., family='logit', lambda_method='aggregate')\n",
        "   ```\n",
        "\n",
        "2. **Coverage is valid but slightly lower than linear.** Logit models are inherently harder due to the nonlinear link function. Coverage ~90-95% is expected.\n",
        "\n",
        "3. **The influence function correction works.** The debiased NN estimates have:\n",
        "   - Low bias around the true target $\\mu^* = 0.5$\n",
        "   - Well-calibrated standard errors (SE ratio ~1.0)\n",
        "   - Valid coverage (88-97%)\n",
        "\n",
        "4. **Neural networks handle complexity well.** In the complex and high-dimensional scenarios, NN still maintains reasonable coverage and parameter recovery.\n",
        "\n",
        "5. **Delta correction matters.** The Logit Naive SE (ignoring $Var(\\bar{X})$) under-covers. The delta-corrected SE achieves valid coverage.\n",
        "\n",
        "## References\n",
        "\n",
        "- Farrell, Liang, Misra (2021) \"Deep Neural Networks for Estimation and Inference\" *Econometrica*\n",
        "- Farrell, Liang, Misra (2025) \"Deep Learning for Individual Heterogeneity\""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
