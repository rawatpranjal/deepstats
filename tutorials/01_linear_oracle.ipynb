{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear DGP: Neural Network vs OLS Oracle\n",
    "\n",
    "This notebook validates the `structural_dml` package against an oracle (correctly specified OLS) for a known linear DGP.\n",
    "\n",
    "## DGP Specification\n",
    "\n",
    "$$Y = \\alpha(X) + \\beta(X) \\cdot T + \\varepsilon$$\n",
    "\n",
    "where:\n",
    "- $\\alpha(X) = a_0 + a_1 X$\n",
    "- $\\beta(X) = b_0 + b_1 X$\n",
    "- $\\varepsilon | X \\sim N(0, (1 + 0.5|X|)^2)$ (heteroskedastic)\n",
    "- $X \\sim N(\\mu_X=1, \\sigma^2=1)$\n",
    "- $T \\sim N(0, 1)$, independent of $X$\n",
    "\n",
    "**Parameters:** $a_0=1, a_1=0.5, b_0=-1, b_1=1$\n",
    "\n",
    "**Target:** $\\mu^* = E[\\beta(X)] = b_0 + b_1 \\cdot E[X] = -1 + 1 \\cdot 1 = 0$\n",
    "\n",
    "## Key Insight\n",
    "\n",
    "Testing $H_0: E[\\beta(X)] = 0$ requires accounting for the randomness of $\\bar{X}$:\n",
    "- **Naive OLS** (treat $\\bar{X}$ as fixed): ~11% false positives\n",
    "- **Delta-corrected OLS**: ~5% false positives  \n",
    "- **Neural Net with IF**: ~5% false positives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Setup & DGP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from scipy import stats\n",
    "import warnings\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Add parent directory to path for deep_inference\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "from src.deep_inference import structural_dml\n",
    "\n",
    "# Plotting style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DGP Parameters\n",
    "A0, A1 = 1.0, 0.5      # alpha(X) = a0 + a1*X\n",
    "B0, B1 = -1.0, 1.0     # beta(X) = b0 + b1*X\n",
    "MU_X = 1.0             # E[X] = 1\n",
    "MU_TRUE = B0 + B1 * MU_X  # E[beta(X)] = -1 + 1*1 = 0\n",
    "\n",
    "print(f\"DGP Parameters:\")\n",
    "print(f\"  alpha(X) = {A0} + {A1}*X\")\n",
    "print(f\"  beta(X)  = {B0} + {B1}*X\")\n",
    "print(f\"  E[X] = {MU_X}\")\n",
    "print(f\"  True target mu* = E[beta(X)] = {MU_TRUE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(n, seed=None):\n",
    "    \"\"\"\n",
    "    Generate data from the linear DGP.\n",
    "    \n",
    "    Y = alpha(X) + beta(X)*T + eps\n",
    "    alpha(X) = a0 + a1*X\n",
    "    beta(X) = b0 + b1*X\n",
    "    eps | X ~ N(0, (1 + 0.5*|X|)^2)  [heteroskedastic]\n",
    "    X ~ N(1, 1)\n",
    "    T ~ N(0, 1)\n",
    "    \n",
    "    Returns dict with Y, T, X, alpha_true, beta_true, mu_true\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "    \n",
    "    # Generate covariates\n",
    "    X = np.random.normal(MU_X, 1.0, n)\n",
    "    T = np.random.normal(0.0, 1.0, n)\n",
    "    \n",
    "    # True structural functions\n",
    "    alpha_true = A0 + A1 * X\n",
    "    beta_true = B0 + B1 * X\n",
    "    \n",
    "    # Heteroskedastic errors\n",
    "    sigma = 1.0 + 0.5 * np.abs(X)\n",
    "    eps = np.random.normal(0, sigma)\n",
    "    \n",
    "    # Outcome\n",
    "    Y = alpha_true + beta_true * T + eps\n",
    "    \n",
    "    return {\n",
    "        'Y': Y,\n",
    "        'T': T,\n",
    "        'X': X,\n",
    "        'alpha_true': alpha_true,\n",
    "        'beta_true': beta_true,\n",
    "        'mu_true': beta_true.mean()  # Sample E[beta(X)]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample data for visualization\n",
    "np.random.seed(42)\n",
    "data = generate_data(1000, seed=42)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Plot 1: Y vs T colored by X\n",
    "ax = axes[0]\n",
    "scatter = ax.scatter(data['T'], data['Y'], c=data['X'], cmap='coolwarm', alpha=0.5, s=10)\n",
    "plt.colorbar(scatter, ax=ax, label='X')\n",
    "ax.set_xlabel('T (Treatment)')\n",
    "ax.set_ylabel('Y (Outcome)')\n",
    "ax.set_title('Y vs T (colored by X)')\n",
    "\n",
    "# Plot 2: True alpha(X) and beta(X)\n",
    "ax = axes[1]\n",
    "X_grid = np.linspace(-2, 4, 100)\n",
    "ax.plot(X_grid, A0 + A1 * X_grid, 'b-', label=r'$\\alpha(X) = 1 + 0.5X$', linewidth=2)\n",
    "ax.plot(X_grid, B0 + B1 * X_grid, 'r-', label=r'$\\beta(X) = -1 + X$', linewidth=2)\n",
    "ax.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "ax.axvline(x=MU_X, color='gray', linestyle=':', alpha=0.5, label=f'E[X]={MU_X}')\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Function value')\n",
    "ax.set_title('True Structural Functions')\n",
    "ax.legend()\n",
    "\n",
    "# Plot 3: Distribution of X and beta(X)\n",
    "ax = axes[2]\n",
    "ax.hist(data['beta_true'], bins=30, alpha=0.7, edgecolor='black')\n",
    "ax.axvline(x=MU_TRUE, color='red', linestyle='--', linewidth=2, label=f'Pop. E[beta(X)]={MU_TRUE}')\n",
    "ax.axvline(x=data['beta_true'].mean(), color='green', linestyle=':', linewidth=2, \n",
    "           label=f'Sample mean={data[\"beta_true\"].mean():.3f}')\n",
    "ax.set_xlabel(r'$\\beta(X)$')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title(r'Distribution of $\\beta(X)$')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nSample statistics:\")\n",
    "print(f\"  n = {len(data['Y'])}\")\n",
    "print(f\"  mean(X) = {data['X'].mean():.4f}\")\n",
    "print(f\"  Sample E[beta(X)] = {data['beta_true'].mean():.4f}\")\n",
    "print(f\"  Population E[beta(X)] = {MU_TRUE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Oracle Implementation\n",
    "\n",
    "The oracle is OLS on the correctly specified model: $Y = a_0 + a_1 X + b_0 T + b_1 (X \\cdot T) + \\varepsilon$\n",
    "\n",
    "For inference on $E[\\beta(X)] = b_0 + b_1 E[X]$, we need to account for:\n",
    "1. **Naive SE**: Treats $\\bar{X}$ as fixed (WRONG - inflated Type I error)\n",
    "2. **Delta-corrected SE**: Adds $\\hat{b}_1^2 \\cdot Var(\\bar{X})$ term (CORRECT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ols_oracle(Y, T, X):\n",
    "    \"\"\"\n",
    "    OLS oracle with both naive and delta-corrected SE for E[beta(X)].\n",
    "    \n",
    "    Model: Y = a0 + a1*X + b0*T + b1*(X*T) + eps\n",
    "    Target: mu = E[beta(X)] = b0 + b1*E[X], estimated by b0_hat + b1_hat*X_bar\n",
    "    \n",
    "    Returns:\n",
    "        mu_hat: Point estimate of E[beta(X)]\n",
    "        se_naive: SE treating X_bar as fixed\n",
    "        se_delta: Delta-corrected SE accounting for Var(X_bar)\n",
    "        params: Dict with a0, a1, b0, b1 estimates\n",
    "        alpha_hat: Fitted alpha(X) = a0 + a1*X\n",
    "        beta_hat: Fitted beta(X) = b0 + b1*X\n",
    "    \"\"\"\n",
    "    n = len(Y)\n",
    "    X_bar = X.mean()\n",
    "    \n",
    "    # Design matrix: [1, X, T, X*T]\n",
    "    X_design = np.column_stack([np.ones(n), X, T, X * T])\n",
    "    \n",
    "    # Fit OLS with HC3 robust SEs (for heteroskedasticity)\n",
    "    model = sm.OLS(Y, X_design).fit(cov_type='HC3')\n",
    "    \n",
    "    # Extract coefficients\n",
    "    a0, a1, b0, b1 = model.params\n",
    "    \n",
    "    # Point estimate of E[beta(X)]\n",
    "    mu_hat = b0 + b1 * X_bar\n",
    "    \n",
    "    # Variance-covariance for b0, b1 (indices 2, 3)\n",
    "    cov = model.cov_params()\n",
    "    var_b0 = cov[2, 2]\n",
    "    var_b1 = cov[3, 3]\n",
    "    cov_b0_b1 = cov[2, 3]\n",
    "    \n",
    "    # Naive SE: treats X_bar as fixed\n",
    "    # Var(b0 + b1*X_bar) = Var(b0) + X_bar^2*Var(b1) + 2*X_bar*Cov(b0,b1)\n",
    "    var_naive = var_b0 + X_bar**2 * var_b1 + 2 * X_bar * cov_b0_b1\n",
    "    se_naive = np.sqrt(var_naive)\n",
    "    \n",
    "    # Delta-corrected SE: accounts for Var(X_bar)\n",
    "    # Additional term: b1^2 * Var(X_bar) = b1^2 * Var(X)/n\n",
    "    var_X_bar = X.var(ddof=1) / n\n",
    "    var_delta = var_naive + b1**2 * var_X_bar\n",
    "    se_delta = np.sqrt(var_delta)\n",
    "    \n",
    "    # Fitted structural functions\n",
    "    alpha_hat = a0 + a1 * X\n",
    "    beta_hat = b0 + b1 * X\n",
    "    \n",
    "    return {\n",
    "        'mu_hat': mu_hat,\n",
    "        'se_naive': se_naive,\n",
    "        'se_delta': se_delta,\n",
    "        'params': {'a0': a0, 'a1': a1, 'b0': b0, 'b1': b1},\n",
    "        'alpha_hat': alpha_hat,\n",
    "        'beta_hat': beta_hat,\n",
    "        'X_bar': X_bar\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the oracle on sample data\n",
    "ols_result = ols_oracle(data['Y'], data['T'], data['X'])\n",
    "\n",
    "print(\"OLS Oracle Results:\")\n",
    "print(f\"\\nParameter estimates:\")\n",
    "print(f\"  a0 = {ols_result['params']['a0']:.4f} (true: {A0})\")\n",
    "print(f\"  a1 = {ols_result['params']['a1']:.4f} (true: {A1})\")\n",
    "print(f\"  b0 = {ols_result['params']['b0']:.4f} (true: {B0})\")\n",
    "print(f\"  b1 = {ols_result['params']['b1']:.4f} (true: {B1})\")\n",
    "\n",
    "print(f\"\\nInference on E[beta(X)]:\")\n",
    "print(f\"  Point estimate: {ols_result['mu_hat']:.4f} (true: {MU_TRUE})\")\n",
    "print(f\"  Naive SE:       {ols_result['se_naive']:.4f}\")\n",
    "print(f\"  Delta SE:       {ols_result['se_delta']:.4f}\")\n",
    "print(f\"  Ratio delta/naive: {ols_result['se_delta']/ols_result['se_naive']:.3f}\")\n",
    "\n",
    "# 95% CIs\n",
    "ci_naive = (ols_result['mu_hat'] - 1.96*ols_result['se_naive'], \n",
    "            ols_result['mu_hat'] + 1.96*ols_result['se_naive'])\n",
    "ci_delta = (ols_result['mu_hat'] - 1.96*ols_result['se_delta'], \n",
    "            ols_result['mu_hat'] + 1.96*ols_result['se_delta'])\n",
    "\n",
    "print(f\"\\n95% Confidence Intervals:\")\n",
    "print(f\"  Naive: [{ci_naive[0]:.4f}, {ci_naive[1]:.4f}]\")\n",
    "print(f\"  Delta: [{ci_delta[0]:.4f}, {ci_delta[1]:.4f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Single-Run Comparison\n",
    "\n",
    "Compare OLS oracle vs Neural Network on a single dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate fresh data for comparison\n",
    "np.random.seed(123)\n",
    "n = 1000\n",
    "data = generate_data(n, seed=123)\n",
    "\n",
    "print(f\"Generated n={n} observations\")\n",
    "print(f\"Sample E[beta(X)] = {data['beta_true'].mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit OLS Oracle\n",
    "ols_result = ols_oracle(data['Y'], data['T'], data['X'])\n",
    "\n",
    "print(\"OLS Oracle:\")\n",
    "print(f\"  mu_hat = {ols_result['mu_hat']:.4f}\")\n",
    "print(f\"  SE (delta) = {ols_result['se_delta']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit Neural Network\n",
    "nn_result = structural_dml(\n",
    "    Y=data['Y'],\n",
    "    T=data['T'],\n",
    "    X=data['X'].reshape(-1, 1),  # Must be 2D\n",
    "    family='linear',\n",
    "    epochs=100,\n",
    "    n_folds=50,\n",
    "    hidden_dims=[64, 32],\n",
    "    lr=0.01,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "print(\"Neural Network (structural_dml):\")\n",
    "print(f\"  mu_hat = {nn_result.mu_hat:.4f}\")\n",
    "print(f\"  SE = {nn_result.se:.4f}\")\n",
    "print(f\"  95% CI: [{nn_result.ci_lower:.4f}, {nn_result.ci_upper:.4f}]\")\n",
    "print(f\"  mu_naive = {nn_result.mu_naive:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Parameter Recovery: $\\alpha(X)$ and $\\beta(X)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract fitted structural functions\n",
    "alpha_ols = ols_result['alpha_hat']\n",
    "beta_ols = ols_result['beta_hat']\n",
    "\n",
    "alpha_nn = nn_result.theta_hat[:, 0]\n",
    "beta_nn = nn_result.theta_hat[:, 1]\n",
    "\n",
    "# Compute recovery metrics\n",
    "def compute_metrics(estimated, true):\n",
    "    rmse = np.sqrt(np.mean((estimated - true)**2))\n",
    "    corr = np.corrcoef(estimated, true)[0, 1]\n",
    "    bias = np.mean(estimated - true)\n",
    "    return {'rmse': rmse, 'corr': corr, 'bias': bias}\n",
    "\n",
    "metrics = {\n",
    "    'OLS': {\n",
    "        'alpha': compute_metrics(alpha_ols, data['alpha_true']),\n",
    "        'beta': compute_metrics(beta_ols, data['beta_true'])\n",
    "    },\n",
    "    'NN': {\n",
    "        'alpha': compute_metrics(alpha_nn, data['alpha_true']),\n",
    "        'beta': compute_metrics(beta_nn, data['beta_true'])\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Parameter Recovery Metrics:\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"{'Metric':<15} {'OLS alpha':<12} {'OLS beta':<12} {'NN alpha':<12} {'NN beta':<12}\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'RMSE':<15} {metrics['OLS']['alpha']['rmse']:<12.4f} {metrics['OLS']['beta']['rmse']:<12.4f} {metrics['NN']['alpha']['rmse']:<12.4f} {metrics['NN']['beta']['rmse']:<12.4f}\")\n",
    "print(f\"{'Correlation':<15} {metrics['OLS']['alpha']['corr']:<12.4f} {metrics['OLS']['beta']['corr']:<12.4f} {metrics['NN']['alpha']['corr']:<12.4f} {metrics['NN']['beta']['corr']:<12.4f}\")\n",
    "print(f\"{'Bias':<15} {metrics['OLS']['alpha']['bias']:<12.4f} {metrics['OLS']['beta']['bias']:<12.4f} {metrics['NN']['alpha']['bias']:<12.4f} {metrics['NN']['beta']['bias']:<12.4f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize parameter recovery\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Sort by X for clean plots\n",
    "sort_idx = np.argsort(data['X'])\n",
    "X_sorted = data['X'][sort_idx]\n",
    "\n",
    "# OLS alpha\n",
    "ax = axes[0, 0]\n",
    "ax.scatter(data['alpha_true'], alpha_ols, alpha=0.3, s=10)\n",
    "ax.plot([data['alpha_true'].min(), data['alpha_true'].max()], \n",
    "        [data['alpha_true'].min(), data['alpha_true'].max()], 'r--', linewidth=2)\n",
    "ax.set_xlabel(r'True $\\alpha(X)$')\n",
    "ax.set_ylabel(r'OLS $\\hat{\\alpha}(X)$')\n",
    "ax.set_title(f'OLS: Corr={metrics[\"OLS\"][\"alpha\"][\"corr\"]:.3f}')\n",
    "\n",
    "# OLS beta\n",
    "ax = axes[0, 1]\n",
    "ax.scatter(data['beta_true'], beta_ols, alpha=0.3, s=10)\n",
    "ax.plot([data['beta_true'].min(), data['beta_true'].max()], \n",
    "        [data['beta_true'].min(), data['beta_true'].max()], 'r--', linewidth=2)\n",
    "ax.set_xlabel(r'True $\\beta(X)$')\n",
    "ax.set_ylabel(r'OLS $\\hat{\\beta}(X)$')\n",
    "ax.set_title(f'OLS: Corr={metrics[\"OLS\"][\"beta\"][\"corr\"]:.3f}')\n",
    "\n",
    "# NN alpha\n",
    "ax = axes[1, 0]\n",
    "ax.scatter(data['alpha_true'], alpha_nn, alpha=0.3, s=10)\n",
    "ax.plot([data['alpha_true'].min(), data['alpha_true'].max()], \n",
    "        [data['alpha_true'].min(), data['alpha_true'].max()], 'r--', linewidth=2)\n",
    "ax.set_xlabel(r'True $\\alpha(X)$')\n",
    "ax.set_ylabel(r'NN $\\hat{\\alpha}(X)$')\n",
    "ax.set_title(f'NN: Corr={metrics[\"NN\"][\"alpha\"][\"corr\"]:.3f}')\n",
    "\n",
    "# NN beta\n",
    "ax = axes[1, 1]\n",
    "ax.scatter(data['beta_true'], beta_nn, alpha=0.3, s=10)\n",
    "ax.plot([data['beta_true'].min(), data['beta_true'].max()], \n",
    "        [data['beta_true'].min(), data['beta_true'].max()], 'r--', linewidth=2)\n",
    "ax.set_xlabel(r'True $\\beta(X)$')\n",
    "ax.set_ylabel(r'NN $\\hat{\\beta}(X)$')\n",
    "ax.set_title(f'NN: Corr={metrics[\"NN\"][\"beta\"][\"corr\"]:.3f}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot fitted functions vs X\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# alpha(X) vs X\n",
    "ax = axes[0]\n",
    "ax.scatter(data['X'], data['alpha_true'], alpha=0.2, s=10, label='True', color='black')\n",
    "ax.scatter(data['X'], alpha_ols, alpha=0.2, s=10, label='OLS', color='blue')\n",
    "ax.scatter(data['X'], alpha_nn, alpha=0.2, s=10, label='NN', color='red')\n",
    "\n",
    "# True line\n",
    "X_grid = np.linspace(data['X'].min(), data['X'].max(), 100)\n",
    "ax.plot(X_grid, A0 + A1 * X_grid, 'k-', linewidth=2, label='True function')\n",
    "\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel(r'$\\alpha(X)$')\n",
    "ax.set_title(r'Recovery of $\\alpha(X) = 1 + 0.5X$')\n",
    "ax.legend()\n",
    "\n",
    "# beta(X) vs X\n",
    "ax = axes[1]\n",
    "ax.scatter(data['X'], data['beta_true'], alpha=0.2, s=10, label='True', color='black')\n",
    "ax.scatter(data['X'], beta_ols, alpha=0.2, s=10, label='OLS', color='blue')\n",
    "ax.scatter(data['X'], beta_nn, alpha=0.2, s=10, label='NN', color='red')\n",
    "\n",
    "# True line\n",
    "ax.plot(X_grid, B0 + B1 * X_grid, 'k-', linewidth=2, label='True function')\n",
    "ax.axhline(y=MU_TRUE, color='gray', linestyle='--', alpha=0.5, label=f'E[beta(X)]={MU_TRUE}')\n",
    "\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel(r'$\\beta(X)$')\n",
    "ax.set_title(r'Recovery of $\\beta(X) = -1 + X$')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Training Diagnostics: Overfitting/Underfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract training diagnostics from NN result\n",
    "diagnostics = nn_result.diagnostics\n",
    "\n",
    "print(\"Neural Network Training Diagnostics:\")\n",
    "print(f\"  Min Lambda eigenvalue: {diagnostics.get('min_lambda_eigenvalue', 'N/A')}\")\n",
    "print(f\"  Pct regularized: {diagnostics.get('pct_regularized', 'N/A')}%\")\n",
    "print(f\"  Correction ratio: {diagnostics.get('correction_ratio', 'N/A')}\")\n",
    "\n",
    "# Training histories (if available)\n",
    "histories = diagnostics.get('histories', [])\n",
    "if histories:\n",
    "    # Plot training curves from first few folds\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    n_plot = min(5, len(histories))\n",
    "    \n",
    "    for i, hist in enumerate(histories[:n_plot]):\n",
    "        if hasattr(hist, 'train_losses') and hist.train_losses:\n",
    "            axes[0].plot(hist.train_losses, alpha=0.5, label=f'Fold {i+1}')\n",
    "        if hasattr(hist, 'val_losses') and hist.val_losses:\n",
    "            axes[1].plot(hist.val_losses, alpha=0.5, label=f'Fold {i+1}')\n",
    "    \n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('Training Loss')\n",
    "    axes[0].set_title('Training Loss by Fold')\n",
    "    axes[0].legend()\n",
    "    \n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].set_ylabel('Validation Loss')\n",
    "    axes[1].set_title('Validation Loss by Fold')\n",
    "    axes[1].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Compute average final losses\n",
    "    final_train = [h.train_losses[-1] for h in histories if hasattr(h, 'train_losses') and h.train_losses]\n",
    "    final_val = [h.val_losses[-1] for h in histories if hasattr(h, 'val_losses') and h.val_losses]\n",
    "    \n",
    "    if final_train and final_val:\n",
    "        print(f\"\\nAverage final losses across folds:\")\n",
    "        print(f\"  Train: {np.mean(final_train):.4f}\")\n",
    "        print(f\"  Val:   {np.mean(final_val):.4f}\")\n",
    "        print(f\"  Gap:   {np.mean(final_val) - np.mean(final_train):.4f}\")\n",
    "else:\n",
    "    print(\"\\nNo training histories available in diagnostics.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Monte Carlo Simulation\n",
    "\n",
    "Run M replications to assess:\n",
    "- c) Bias and variance of $\\hat{\\mu}$ estimates\n",
    "- d) Coverage and SE calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monte Carlo configuration\n",
    "M = 100          # Number of replications\n",
    "N = 1000         # Sample size per replication\n",
    "N_FOLDS = 50     # Cross-fitting folds for NN\n",
    "EPOCHS = 100     # Training epochs\n",
    "\n",
    "print(f\"Monte Carlo Configuration:\")\n",
    "print(f\"  M = {M} replications\")\n",
    "print(f\"  n = {N} observations per replication\")\n",
    "print(f\"  K = {N_FOLDS} folds for NN\")\n",
    "print(f\"  True target mu* = {MU_TRUE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_single_mc(sim_id, n, n_folds, epochs):\n",
    "    \"\"\"\n",
    "    Run a single Monte Carlo replication.\n",
    "    \n",
    "    Returns dict with OLS and NN results.\n",
    "    \"\"\"\n",
    "    # Generate data\n",
    "    data = generate_data(n)\n",
    "    \n",
    "    # OLS Oracle\n",
    "    ols = ols_oracle(data['Y'], data['T'], data['X'])\n",
    "    \n",
    "    # Neural Network\n",
    "    nn = structural_dml(\n",
    "        Y=data['Y'],\n",
    "        T=data['T'],\n",
    "        X=data['X'].reshape(-1, 1),\n",
    "        family='linear',\n",
    "        epochs=epochs,\n",
    "        n_folds=n_folds,\n",
    "        hidden_dims=[64, 32],\n",
    "        lr=0.01,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    # Coverage indicators\n",
    "    mu_true = MU_TRUE  # Population target\n",
    "    \n",
    "    # OLS naive CI\n",
    "    ci_naive_lo = ols['mu_hat'] - 1.96 * ols['se_naive']\n",
    "    ci_naive_hi = ols['mu_hat'] + 1.96 * ols['se_naive']\n",
    "    covered_naive = (ci_naive_lo <= mu_true <= ci_naive_hi)\n",
    "    \n",
    "    # OLS delta CI\n",
    "    ci_delta_lo = ols['mu_hat'] - 1.96 * ols['se_delta']\n",
    "    ci_delta_hi = ols['mu_hat'] + 1.96 * ols['se_delta']\n",
    "    covered_delta = (ci_delta_lo <= mu_true <= ci_delta_hi)\n",
    "    \n",
    "    # NN IF CI\n",
    "    covered_nn = (nn.ci_lower <= mu_true <= nn.ci_upper)\n",
    "    \n",
    "    # Parameter recovery metrics\n",
    "    alpha_nn = nn.theta_hat[:, 0]\n",
    "    beta_nn = nn.theta_hat[:, 1]\n",
    "    \n",
    "    corr_alpha_ols = np.corrcoef(ols['alpha_hat'], data['alpha_true'])[0, 1]\n",
    "    corr_beta_ols = np.corrcoef(ols['beta_hat'], data['beta_true'])[0, 1]\n",
    "    corr_alpha_nn = np.corrcoef(alpha_nn, data['alpha_true'])[0, 1]\n",
    "    corr_beta_nn = np.corrcoef(beta_nn, data['beta_true'])[0, 1]\n",
    "    \n",
    "    return {\n",
    "        'sim_id': sim_id,\n",
    "        'sample_mu_true': data['mu_true'],  # Sample E[beta(X)]\n",
    "        \n",
    "        # OLS estimates\n",
    "        'ols_mu': ols['mu_hat'],\n",
    "        'ols_se_naive': ols['se_naive'],\n",
    "        'ols_se_delta': ols['se_delta'],\n",
    "        'ols_covered_naive': covered_naive,\n",
    "        'ols_covered_delta': covered_delta,\n",
    "        \n",
    "        # NN estimates\n",
    "        'nn_mu': nn.mu_hat,\n",
    "        'nn_mu_naive': nn.mu_naive,\n",
    "        'nn_se': nn.se,\n",
    "        'nn_covered': covered_nn,\n",
    "        \n",
    "        # Parameter recovery\n",
    "        'corr_alpha_ols': corr_alpha_ols,\n",
    "        'corr_beta_ols': corr_beta_ols,\n",
    "        'corr_alpha_nn': corr_alpha_nn,\n",
    "        'corr_beta_nn': corr_beta_nn,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Monte Carlo simulation\n",
    "results = []\n",
    "\n",
    "for sim_id in tqdm(range(M), desc='Monte Carlo'):\n",
    "    np.random.seed(sim_id + 1000)  # Reproducible seeds\n",
    "    result = run_single_mc(sim_id, N, N_FOLDS, EPOCHS)\n",
    "    results.append(result)\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "print(f\"\\nCompleted {M} Monte Carlo replications.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Bias and Variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute bias and variance metrics\n",
    "mu_true = MU_TRUE\n",
    "\n",
    "# OLS\n",
    "ols_bias = df['ols_mu'].mean() - mu_true\n",
    "ols_var = df['ols_mu'].var()\n",
    "ols_rmse = np.sqrt(ols_bias**2 + ols_var)\n",
    "ols_se_emp = df['ols_mu'].std()\n",
    "\n",
    "# NN\n",
    "nn_bias = df['nn_mu'].mean() - mu_true\n",
    "nn_var = df['nn_mu'].var()\n",
    "nn_rmse = np.sqrt(nn_bias**2 + nn_var)\n",
    "nn_se_emp = df['nn_mu'].std()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"BIAS AND VARIANCE (target mu* = 0)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Metric':<20} {'OLS Oracle':<15} {'Neural Net':<15}\")\n",
    "print(\"-\"*60)\n",
    "print(f\"{'Mean estimate':<20} {df['ols_mu'].mean():<15.4f} {df['nn_mu'].mean():<15.4f}\")\n",
    "print(f\"{'Bias':<20} {ols_bias:<15.4f} {nn_bias:<15.4f}\")\n",
    "print(f\"{'Variance':<20} {ols_var:<15.4f} {nn_var:<15.4f}\")\n",
    "print(f\"{'RMSE':<20} {ols_rmse:<15.4f} {nn_rmse:<15.4f}\")\n",
    "print(f\"{'Empirical SE':<20} {ols_se_emp:<15.4f} {nn_se_emp:<15.4f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d) Coverage and SE Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coverage and SE calibration\n",
    "coverage_naive = df['ols_covered_naive'].mean()\n",
    "coverage_delta = df['ols_covered_delta'].mean()\n",
    "coverage_nn = df['nn_covered'].mean()\n",
    "\n",
    "# SE ratios (mean estimated SE / empirical SE)\n",
    "se_ratio_naive = df['ols_se_naive'].mean() / ols_se_emp\n",
    "se_ratio_delta = df['ols_se_delta'].mean() / ols_se_emp\n",
    "se_ratio_nn = df['nn_se'].mean() / nn_se_emp\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"COVERAGE AND SE CALIBRATION\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Metric':<20} {'OLS Naive':<15} {'OLS Delta':<15} {'Neural Net':<15}\")\n",
    "print(\"-\"*70)\n",
    "print(f\"{'Coverage':<20} {coverage_naive:<15.1%} {coverage_delta:<15.1%} {coverage_nn:<15.1%}\")\n",
    "print(f\"{'Mean Est SE':<20} {df['ols_se_naive'].mean():<15.4f} {df['ols_se_delta'].mean():<15.4f} {df['nn_se'].mean():<15.4f}\")\n",
    "print(f\"{'Empirical SE':<20} {ols_se_emp:<15.4f} {ols_se_emp:<15.4f} {nn_se_emp:<15.4f}\")\n",
    "print(f\"{'SE Ratio':<20} {se_ratio_naive:<15.2f} {se_ratio_delta:<15.2f} {se_ratio_nn:<15.2f}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nTarget ranges:\")\n",
    "print(\"  Coverage: 93-97%\")\n",
    "print(\"  SE Ratio: 0.9-1.2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter recovery summary\n",
    "print(\"=\"*60)\n",
    "print(\"PARAMETER RECOVERY (Correlation with true values)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Parameter':<15} {'OLS Oracle':<15} {'Neural Net':<15}\")\n",
    "print(\"-\"*60)\n",
    "print(f\"{'alpha(X)':<15} {df['corr_alpha_ols'].mean():<15.3f} {df['corr_alpha_nn'].mean():<15.3f}\")\n",
    "print(f\"{'beta(X)':<15} {df['corr_beta_ols'].mean():<15.3f} {df['corr_beta_nn'].mean():<15.3f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Results Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of estimates\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# OLS estimates\n",
    "ax = axes[0]\n",
    "ax.hist(df['ols_mu'], bins=20, alpha=0.7, edgecolor='black', label='OLS')\n",
    "ax.axvline(x=MU_TRUE, color='red', linestyle='--', linewidth=2, label=f'True mu*={MU_TRUE}')\n",
    "ax.axvline(x=df['ols_mu'].mean(), color='blue', linestyle=':', linewidth=2, \n",
    "           label=f'Mean={df[\"ols_mu\"].mean():.4f}')\n",
    "ax.set_xlabel(r'$\\hat{\\mu}$')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title(f'OLS Oracle Estimates (Coverage: {coverage_delta:.1%})')\n",
    "ax.legend()\n",
    "\n",
    "# NN estimates\n",
    "ax = axes[1]\n",
    "ax.hist(df['nn_mu'], bins=20, alpha=0.7, edgecolor='black', color='orange', label='NN')\n",
    "ax.axvline(x=MU_TRUE, color='red', linestyle='--', linewidth=2, label=f'True mu*={MU_TRUE}')\n",
    "ax.axvline(x=df['nn_mu'].mean(), color='darkorange', linestyle=':', linewidth=2, \n",
    "           label=f'Mean={df[\"nn_mu\"].mean():.4f}')\n",
    "ax.set_xlabel(r'$\\hat{\\mu}$')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title(f'Neural Net Estimates (Coverage: {coverage_nn:.1%})')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QQ plots for t-statistics\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# OLS Naive t-stats\n",
    "t_naive = (df['ols_mu'] - MU_TRUE) / df['ols_se_naive']\n",
    "ax = axes[0]\n",
    "stats.probplot(t_naive, dist='norm', plot=ax)\n",
    "ax.set_title(f'OLS Naive t-stats\\n(Coverage: {coverage_naive:.1%})')\n",
    "\n",
    "# OLS Delta t-stats\n",
    "t_delta = (df['ols_mu'] - MU_TRUE) / df['ols_se_delta']\n",
    "ax = axes[1]\n",
    "stats.probplot(t_delta, dist='norm', plot=ax)\n",
    "ax.set_title(f'OLS Delta t-stats\\n(Coverage: {coverage_delta:.1%})')\n",
    "\n",
    "# NN t-stats\n",
    "t_nn = (df['nn_mu'] - MU_TRUE) / df['nn_se']\n",
    "ax = axes[2]\n",
    "stats.probplot(t_nn, dist='norm', plot=ax)\n",
    "ax.set_title(f'Neural Net t-stats\\n(Coverage: {coverage_nn:.1%})')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"t-statistic summary:\")\n",
    "print(f\"  OLS Naive: mean={t_naive.mean():.3f}, std={t_naive.std():.3f}\")\n",
    "print(f\"  OLS Delta: mean={t_delta.mean():.3f}, std={t_delta.std():.3f}\")\n",
    "print(f\"  Neural Net: mean={t_nn.mean():.3f}, std={t_nn.std():.3f}\")\n",
    "print(f\"  Standard normal: mean=0, std=1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison of OLS naive vs delta coverage\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "methods = ['OLS Naive', 'OLS Delta', 'Neural Net (IF)']\n",
    "coverages = [coverage_naive, coverage_delta, coverage_nn]\n",
    "colors = ['#ff7f7f', '#7fbf7f', '#7f7fff']\n",
    "\n",
    "bars = ax.bar(methods, coverages, color=colors, edgecolor='black')\n",
    "ax.axhline(y=0.95, color='red', linestyle='--', linewidth=2, label='Target (95%)')\n",
    "ax.axhspan(0.93, 0.97, alpha=0.2, color='green', label='Valid range (93-97%)')\n",
    "\n",
    "ax.set_ylabel('Coverage')\n",
    "ax.set_title('95% CI Coverage Comparison')\n",
    "ax.set_ylim(0.8, 1.0)\n",
    "ax.legend()\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, cov in zip(bars, coverages):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "            f'{cov:.1%}', ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary table\n",
    "print(\"=\"*70)\n",
    "print(\"FINAL SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nDGP: Y = alpha(X) + beta(X)*T + eps\")\n",
    "print(f\"     alpha(X) = {A0} + {A1}*X\")\n",
    "print(f\"     beta(X) = {B0} + {B1}*X\")\n",
    "print(f\"     Target: E[beta(X)] = {MU_TRUE}\")\n",
    "print(f\"\\nMonte Carlo: M={M} replications, n={N} observations each\")\n",
    "print()\n",
    "\n",
    "# Summary table\n",
    "summary_data = {\n",
    "    'Metric': ['Bias', 'Variance', 'RMSE', 'Empirical SE', 'Mean Est SE', 'SE Ratio', 'Coverage'],\n",
    "    'OLS Naive': [f'{ols_bias:.4f}', f'{ols_var:.4f}', f'{ols_rmse:.4f}', \n",
    "                  f'{ols_se_emp:.4f}', f'{df[\"ols_se_naive\"].mean():.4f}', \n",
    "                  f'{se_ratio_naive:.2f}', f'{coverage_naive:.1%}'],\n",
    "    'OLS Delta': [f'{ols_bias:.4f}', f'{ols_var:.4f}', f'{ols_rmse:.4f}', \n",
    "                  f'{ols_se_emp:.4f}', f'{df[\"ols_se_delta\"].mean():.4f}', \n",
    "                  f'{se_ratio_delta:.2f}', f'{coverage_delta:.1%}'],\n",
    "    'Neural Net': [f'{nn_bias:.4f}', f'{nn_var:.4f}', f'{nn_rmse:.4f}', \n",
    "                   f'{nn_se_emp:.4f}', f'{df[\"nn_se\"].mean():.4f}', \n",
    "                   f'{se_ratio_nn:.2f}', f'{coverage_nn:.1%}']\n",
    "}\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"VALIDATION CHECK\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Check targets\n",
    "checks = []\n",
    "\n",
    "if coverage_naive < 0.93:\n",
    "    checks.append(f\"[EXPECTED] OLS Naive coverage ({coverage_naive:.1%}) < 93% - ignoring Var(X_bar)\")\n",
    "    \n",
    "if 0.93 <= coverage_delta <= 0.97:\n",
    "    checks.append(f\"[PASS] OLS Delta coverage: {coverage_delta:.1%} (target: 93-97%)\")\n",
    "else:\n",
    "    checks.append(f\"[FAIL] OLS Delta coverage: {coverage_delta:.1%} (target: 93-97%)\")\n",
    "\n",
    "if 0.93 <= coverage_nn <= 0.97:\n",
    "    checks.append(f\"[PASS] NN coverage: {coverage_nn:.1%} (target: 93-97%)\")\n",
    "else:\n",
    "    checks.append(f\"[FAIL] NN coverage: {coverage_nn:.1%} (target: 93-97%)\")\n",
    "\n",
    "if 0.9 <= se_ratio_delta <= 1.2:\n",
    "    checks.append(f\"[PASS] OLS Delta SE ratio: {se_ratio_delta:.2f} (target: 0.9-1.2)\")\n",
    "else:\n",
    "    checks.append(f\"[FAIL] OLS Delta SE ratio: {se_ratio_delta:.2f} (target: 0.9-1.2)\")\n",
    "\n",
    "if 0.9 <= se_ratio_nn <= 1.2:\n",
    "    checks.append(f\"[PASS] NN SE ratio: {se_ratio_nn:.2f} (target: 0.9-1.2)\")\n",
    "else:\n",
    "    checks.append(f\"[FAIL] NN SE ratio: {se_ratio_nn:.2f} (target: 0.9-1.2)\")\n",
    "\n",
    "corr_beta_nn_mean = df['corr_beta_nn'].mean()\n",
    "if corr_beta_nn_mean > 0.9:\n",
    "    checks.append(f\"[PASS] NN beta(X) recovery: Corr={corr_beta_nn_mean:.3f} (target: >0.9)\")\n",
    "else:\n",
    "    checks.append(f\"[WARN] NN beta(X) recovery: Corr={corr_beta_nn_mean:.3f} (target: >0.9)\")\n",
    "\n",
    "for check in checks:\n",
    "    print(check)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Findings\n",
    "\n",
    "1. **OLS Naive (ignoring $Var(\\bar{X})$)**: Under-covers because it ignores the sampling variability of $\\bar{X}$ in $\\hat{\\mu} = \\hat{b}_0 + \\hat{b}_1 \\bar{X}$.\n",
    "\n",
    "2. **OLS Delta (accounting for $Var(\\bar{X})$)**: Achieves valid coverage by adding $\\hat{b}_1^2 Var(\\bar{X})$ to the variance.\n",
    "\n",
    "3. **Neural Network with Influence Functions**: Achieves valid coverage comparable to the correctly-specified oracle, demonstrating that:\n",
    "   - The influence function correction properly accounts for regularization bias\n",
    "   - The SE estimate is well-calibrated\n",
    "   - The method works even for simple DGPs where OLS is optimal\n",
    "\n",
    "4. **Parameter Recovery**: For this simple linear DGP, both OLS and NN achieve near-perfect recovery of $\\alpha(X)$ and $\\beta(X)$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
