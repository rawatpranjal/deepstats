{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multimodal Gallery: Text & Image Embeddings as Covariates\n",
    "\n",
    "This gallery demonstrates deep-inference on high-dimensional embeddings from text and images. Three examples covering **Linear**, **Logit**, and **Poisson** models.\n",
    "\n",
    "Each example has:\n",
    "- **Y**: Meaningful outcome\n",
    "- **T**: Treatment of interest  \n",
    "- **X**: High-dimensional embeddings (simulating text/image features)\n",
    "\n",
    "## Examples\n",
    "\n",
    "1. **LINEAR**: Wages ~ Experience | Job Description Embeddings\n",
    "2. **LOGIT**: Purchase ~ Discount | Product Image Embeddings  \n",
    "3. **POISSON**: Citations ~ Open Access | Paper Abstract Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '../src')\n",
    "sys.path.insert(0, 'src')\n",
    "\n",
    "from deep_inference import structural_dml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_class_embeddings(n, dim, n_classes, class_names=None, seed=42):\n",
    "    \"\"\"\n",
    "    Simulate embeddings that cluster by class (like real BERT/ResNet).\n",
    "    \n",
    "    The NN must learn to \"classify\" from X to predict β(X).\n",
    "    This mimics real image/text recognition tasks.\n",
    "    \n",
    "    Args:\n",
    "        n: Number of observations\n",
    "        dim: Embedding dimension\n",
    "        n_classes: Number of semantic classes\n",
    "        class_names: Optional list of class names for display\n",
    "        seed: Random seed\n",
    "    \n",
    "    Returns:\n",
    "        X: (n, dim) embeddings clustered by class\n",
    "        class_labels: (n,) integer class labels\n",
    "        class_names: List of class name strings\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Generate class centroids in embedding space (well-separated)\n",
    "    centroids = np.random.randn(n_classes, dim) * 3.0\n",
    "    \n",
    "    # Assign random class labels\n",
    "    class_labels = np.random.choice(n_classes, n)\n",
    "    \n",
    "    # Each observation = centroid + Gaussian noise\n",
    "    X = centroids[class_labels] + np.random.randn(n, dim) * 0.8\n",
    "    \n",
    "    if class_names is None:\n",
    "        class_names = [f\"Class_{i}\" for i in range(n_classes)]\n",
    "    \n",
    "    return X, class_labels, class_names\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"MULTIMODAL GALLERY: Deep Inference with High-Dimensional Embeddings\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Example 1: LINEAR - Wages by Job Category\n",
    "\n",
    "**Scenario**: A labor economist studies how experience affects wages. The experience premium varies by **job category** - the NN must infer the category from job description embeddings.\n",
    "\n",
    "| Variable | Description |\n",
    "|----------|-------------|\n",
    "| Y | Log hourly wage (continuous) |\n",
    "| T | Years of experience (standardized) |\n",
    "| X | 64-dim embeddings of job descriptions |\n",
    "\n",
    "**Job Categories** (5 classes with different experience premiums):\n",
    "- Entry-level: β = 0.4 (small experience premium)\n",
    "- Mid-level: β = 0.7\n",
    "- Senior: β = 1.0 (baseline)\n",
    "- Executive: β = 1.3\n",
    "- Specialist: β = 1.6 (highest premium - experience matters most)\n",
    "\n",
    "The NN must **classify** jobs from embeddings to predict β(X)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2024)\n",
    "N = 10000\n",
    "EMBED_DIM = 64\n",
    "\n",
    "# Job categories with distinct experience premiums\n",
    "JOB_CATEGORIES = [\"Entry-level\", \"Mid-level\", \"Senior\", \"Executive\", \"Specialist\"]\n",
    "BETA_BY_JOB = {0: 0.4, 1: 0.7, 2: 1.0, 3: 1.3, 4: 1.6}\n",
    "\n",
    "# Generate class-clustered embeddings (NN must learn to classify!)\n",
    "X_wages, job_class, _ = simulate_class_embeddings(\n",
    "    N, dim=EMBED_DIM, n_classes=5, class_names=JOB_CATEGORIES, seed=2024\n",
    ")\n",
    "\n",
    "# True β depends on job category (discrete heterogeneity)\n",
    "beta_wages = np.array([BETA_BY_JOB[c] for c in job_class])\n",
    "alpha_wages = 2.5 + 0.3 * np.sin(job_class)  # Base wage varies by category\n",
    "\n",
    "# Treatment: experience (confounded - harder test)\n",
    "T_wages = beta_wages + np.random.normal(0, 0.5, N)\n",
    "\n",
    "# Outcome: log wage\n",
    "Y_wages = alpha_wages + beta_wages * T_wages + np.random.normal(0, 0.3, N)\n",
    "\n",
    "mu_true_wages = beta_wages.mean()\n",
    "\n",
    "print(f\"Data: N={N}, X dim={EMBED_DIM}\")\n",
    "print(f\"True E[β(X)] = {mu_true_wages:.4f} (avg {mu_true_wages*100:.1f}% wage increase per year exp)\")\n",
    "print(f\"Heterogeneity: β ranges from {beta_wages.min():.3f} to {beta_wages.max():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Running deep-inference (Linear family)...\")\n",
    "result_wages = structural_dml(\n",
    "    Y=Y_wages, T=T_wages, X=X_wages,\n",
    "    family='linear',\n",
    "    hidden_dims=[64, 32],\n",
    "    epochs=200,\n",
    "    n_folds=20,\n",
    "    lambda_method='lgbm',\n",
    "    lr=0.01,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "print(f\"\\nRESULTS:\")\n",
    "print(f\"  True E[β]:     {mu_true_wages:.4f}\")\n",
    "print(f\"  Estimated:     {result_wages.mu_hat:.4f}\")\n",
    "print(f\"  SE:            {result_wages.se:.4f}\")\n",
    "print(f\"  95% CI:        [{result_wages.ci_lower:.4f}, {result_wages.ci_upper:.4f}]\")\n",
    "print(f\"  Covers truth:  {result_wages.ci_lower <= mu_true_wages <= result_wages.ci_upper}\")"
   ]
  },
  {
   "cell_type": "code",
   "source": "# Statsmodels-style summary\nprint(\"\\nPublication-ready summary:\")\nprint(result_wages.summary())",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heterogeneity analysis\n",
    "beta_hat_wages = result_wages.theta_hat[:, 1]\n",
    "corr_wages = np.corrcoef(beta_wages, beta_hat_wages)[0, 1]\n",
    "print(f\"Heterogeneity Recovery:\")\n",
    "print(f\"  Corr(β_true, β_hat): {corr_wages:.3f}\")\n",
    "\n",
    "# Classification accuracy: does NN identify high-β categories?\n",
    "top_idx = np.argsort(beta_hat_wages)[-int(N*0.1):]\n",
    "bottom_idx = np.argsort(beta_hat_wages)[:int(N*0.1)]\n",
    "print(f\"\\n  Top 10% (highest estimated β):\")\n",
    "print(f\"    True category distribution: {np.bincount(job_class[top_idx], minlength=5)}\")\n",
    "print(f\"    Avg β_true: {beta_wages[top_idx].mean():.4f}\")\n",
    "print(f\"    Avg β_hat: {beta_hat_wages[top_idx].mean():.4f}\")\n",
    "print(f\"  Bottom 10% (lowest estimated β):\")\n",
    "print(f\"    True category distribution: {np.bincount(job_class[bottom_idx], minlength=5)}\")\n",
    "print(f\"    Avg β_true: {beta_wages[bottom_idx].mean():.4f}\")\n",
    "print(f\"    Avg β_hat: {beta_hat_wages[bottom_idx].mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Example 2: LOGIT - Purchase by Product Category\n",
    "\n",
    "**Scenario**: An e-commerce company studies discount effectiveness. The discount sensitivity varies by **product category** - the NN must infer the category from product image embeddings.\n",
    "\n",
    "| Variable | Description |\n",
    "|----------|-------------|\n",
    "| Y | Purchase (0/1 binary) |\n",
    "| T | Discount level (standardized) |\n",
    "| X | 64-dim embeddings from product images |\n",
    "\n",
    "**Product Categories** (5 classes with different discount sensitivities):\n",
    "- Electronics: β = 0.4 (low sensitivity - people research anyway)\n",
    "- Fashion: β = 0.8\n",
    "- Home & Garden: β = 1.2 (baseline)\n",
    "- Beauty: β = 1.6\n",
    "- Sports: β = 2.0 (highest - impulse buyers love discounts)\n",
    "\n",
    "The NN must **classify** products from image embeddings to predict β(X)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2025)\n",
    "N = 16000  # Binary outcomes need ~2x data\n",
    "EMBED_DIM = 64\n",
    "\n",
    "# Product categories with distinct discount sensitivities\n",
    "PRODUCT_CATEGORIES = [\"Electronics\", \"Fashion\", \"Home\", \"Beauty\", \"Sports\"]\n",
    "BETA_BY_PRODUCT = {0: 0.4, 1: 0.8, 2: 1.2, 3: 1.6, 4: 2.0}\n",
    "\n",
    "# Generate class-clustered embeddings (NN must learn to classify!)\n",
    "X_purchase, product_class, _ = simulate_class_embeddings(\n",
    "    N, dim=EMBED_DIM, n_classes=5, class_names=PRODUCT_CATEGORIES, seed=2025\n",
    ")\n",
    "\n",
    "# True β depends on product category (discrete heterogeneity)\n",
    "beta_purchase = np.array([BETA_BY_PRODUCT[c] for c in product_class])\n",
    "alpha_purchase = 0.5 + 0.3 * np.sin(product_class)  # Base varies by category\n",
    "\n",
    "# Treatment: discount level (confounded - harder test)\n",
    "T_purchase = beta_purchase + np.random.normal(0, 0.5, N)\n",
    "\n",
    "from scipy.special import expit\n",
    "prob_purchase = expit(alpha_purchase + beta_purchase * T_purchase)\n",
    "Y_purchase = np.random.binomial(1, prob_purchase).astype(float)\n",
    "\n",
    "mu_true_purchase = beta_purchase.mean()\n",
    "\n",
    "print(f\"Data: N={N}, X dim={EMBED_DIM}\")\n",
    "print(f\"True E[β(X)] = {mu_true_purchase:.4f} (avg log-odds increase per discount unit)\")\n",
    "print(f\"Heterogeneity: β ranges from {beta_purchase.min():.3f} to {beta_purchase.max():.3f}\")\n",
    "print(f\"Purchase rate: {Y_purchase.mean():.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "source": "# Statsmodels-style summary\nprint(\"\\nPublication-ready summary:\")\nprint(result_purchase.summary())",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Running deep-inference (Logit family)...\")\n",
    "result_purchase = structural_dml(\n",
    "    Y=Y_purchase, T=T_purchase, X=X_purchase,\n",
    "    family='logit',\n",
    "    hidden_dims=[64, 32],\n",
    "    epochs=200,\n",
    "    n_folds=20,\n",
    "    lambda_method='lgbm',\n",
    "    lr=0.01,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "print(f\"\\nRESULTS:\")\n",
    "print(f\"  True E[β]:     {mu_true_purchase:.4f}\")\n",
    "print(f\"  Estimated:     {result_purchase.mu_hat:.4f}\")\n",
    "print(f\"  SE:            {result_purchase.se:.4f}\")\n",
    "print(f\"  95% CI:        [{result_purchase.ci_lower:.4f}, {result_purchase.ci_upper:.4f}]\")\n",
    "print(f\"  Covers truth:  {result_purchase.ci_lower <= mu_true_purchase <= result_purchase.ci_upper}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heterogeneity analysis\n",
    "beta_hat_purchase = result_purchase.theta_hat[:, 1]\n",
    "corr_purchase = np.corrcoef(beta_purchase, beta_hat_purchase)[0, 1]\n",
    "print(f\"Heterogeneity Recovery:\")\n",
    "print(f\"  Corr(β_true, β_hat): {corr_purchase:.3f}\")\n",
    "\n",
    "# Classification accuracy: does NN identify high-sensitivity categories?\n",
    "top_idx = np.argsort(beta_hat_purchase)[-int(N*0.1):]\n",
    "bottom_idx = np.argsort(beta_hat_purchase)[:int(N*0.1)]\n",
    "print(f\"\\n  Products where discounts WORK (top 10% β_hat):\")\n",
    "print(f\"    Category distribution: {np.bincount(product_class[top_idx], minlength=5)}\")\n",
    "print(f\"    Avg β_true: {beta_purchase[top_idx].mean():.4f}\")\n",
    "print(f\"    Avg β_hat: {beta_hat_purchase[top_idx].mean():.4f}\")\n",
    "print(f\"  Products where discounts DON'T WORK (bottom 10% β_hat):\")\n",
    "print(f\"    Category distribution: {np.bincount(product_class[bottom_idx], minlength=5)}\")\n",
    "print(f\"    Avg β_true: {beta_purchase[bottom_idx].mean():.4f}\")\n",
    "print(f\"    Avg β_hat: {beta_hat_purchase[bottom_idx].mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Example 3: POISSON - Citations by Research Field\n",
    "\n",
    "**Scenario**: A bibliometrics researcher studies the Open Access citation advantage. The OA advantage varies by **research field** - the NN must infer the field from paper abstract embeddings.\n",
    "\n",
    "| Variable | Description |\n",
    "|----------|-------------|\n",
    "| Y | Citation count (non-negative integer) |\n",
    "| T | Open Access intensity (standardized) |\n",
    "| X | 64-dim embeddings of paper abstracts |\n",
    "\n",
    "**Research Fields** (5 classes with different OA advantages):\n",
    "- Humanities: β = 0.3 (low - already accessible, small audience)\n",
    "- Economics: β = 0.6\n",
    "- Biology: β = 1.0 (baseline)\n",
    "- Physics: β = 1.4\n",
    "- CS/ML: β = 1.8 (highest - paywalls really hurt, preprint culture)\n",
    "\n",
    "The NN must **classify** papers from abstract embeddings to predict β(X)."
   ]
  },
  {
   "cell_type": "code",
   "source": "# Statsmodels-style summary\nprint(\"\\nPublication-ready summary:\")\nprint(result_cite.summary())",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2026)\n",
    "N = 12000\n",
    "EMBED_DIM = 64\n",
    "\n",
    "# Research fields with distinct OA advantages\n",
    "RESEARCH_FIELDS = [\"Humanities\", \"Economics\", \"Biology\", \"Physics\", \"CS/ML\"]\n",
    "BETA_BY_FIELD = {0: 0.3, 1: 0.6, 2: 1.0, 3: 1.4, 4: 1.8}\n",
    "\n",
    "# Generate class-clustered embeddings (NN must learn to classify!)\n",
    "X_cite, field_class, _ = simulate_class_embeddings(\n",
    "    N, dim=EMBED_DIM, n_classes=5, class_names=RESEARCH_FIELDS, seed=2026\n",
    ")\n",
    "\n",
    "# True β depends on research field (discrete heterogeneity)\n",
    "beta_cite = np.array([BETA_BY_FIELD[c] for c in field_class])\n",
    "alpha_cite = 1.0 + 0.3 * np.sin(field_class)  # Base citation rate varies by field\n",
    "\n",
    "# Treatment: OA intensity (confounded - harder test)\n",
    "T_cite = beta_cite + np.random.normal(0, 0.5, N)\n",
    "\n",
    "# Poisson outcome (citation counts)\n",
    "log_lambda = alpha_cite + beta_cite * T_cite\n",
    "lambda_cite = np.exp(np.clip(log_lambda, -5, 5))  # Clip for numerical stability\n",
    "Y_cite = np.random.poisson(lambda_cite).astype(float)\n",
    "\n",
    "mu_true_cite = beta_cite.mean()\n",
    "\n",
    "print(f\"Data: N={N}, X dim={EMBED_DIM}\")\n",
    "print(f\"True E[β(X)] = {mu_true_cite:.4f} (avg log-rate increase from OA)\")\n",
    "print(f\"  → Exp(β) = {np.exp(mu_true_cite):.2f}x citation multiplier\")\n",
    "print(f\"Heterogeneity: β ranges from {beta_cite.min():.3f} to {beta_cite.max():.3f}\")\n",
    "print(f\"Mean citations: {Y_cite.mean():.1f}, Max: {Y_cite.max():.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Running deep-inference (Poisson family)...\")\n",
    "result_cite = structural_dml(\n",
    "    Y=Y_cite, T=T_cite, X=X_cite,\n",
    "    family='poisson',\n",
    "    hidden_dims=[64, 32],\n",
    "    epochs=200,\n",
    "    n_folds=20,\n",
    "    lambda_method='lgbm',\n",
    "    lr=0.01,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "print(f\"\\nRESULTS:\")\n",
    "print(f\"  True E[β]:     {mu_true_cite:.4f}\")\n",
    "print(f\"  Estimated:     {result_cite.mu_hat:.4f}\")\n",
    "print(f\"  SE:            {result_cite.se:.4f}\")\n",
    "print(f\"  95% CI:        [{result_cite.ci_lower:.4f}, {result_cite.ci_upper:.4f}]\")\n",
    "print(f\"  Covers truth:  {result_cite.ci_lower <= mu_true_cite <= result_cite.ci_upper}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heterogeneity analysis\n",
    "beta_hat_cite = result_cite.theta_hat[:, 1]\n",
    "corr_cite = np.corrcoef(beta_cite, beta_hat_cite)[0, 1]\n",
    "print(f\"Heterogeneity Recovery:\")\n",
    "print(f\"  Corr(β_true, β_hat): {corr_cite:.3f}\")\n",
    "\n",
    "# Classification accuracy: does NN identify high-OA-advantage fields?\n",
    "top_idx = np.argsort(beta_hat_cite)[-int(N*0.1):]\n",
    "bottom_idx = np.argsort(beta_hat_cite)[:int(N*0.1)]\n",
    "print(f\"\\n  Papers with LARGEST OA advantage (top 10% β_hat):\")\n",
    "print(f\"    Field distribution: {np.bincount(field_class[top_idx], minlength=5)}\")\n",
    "print(f\"    Avg β_true: {beta_cite[top_idx].mean():.4f} ({np.exp(beta_cite[top_idx].mean()):.1f}x)\")\n",
    "print(f\"    Avg β_hat: {beta_hat_cite[top_idx].mean():.4f}\")\n",
    "print(f\"  Papers with SMALLEST OA advantage (bottom 10% β_hat):\")\n",
    "print(f\"    Field distribution: {np.bincount(field_class[bottom_idx], minlength=5)}\")\n",
    "print(f\"    Avg β_true: {beta_cite[bottom_idx].mean():.4f} ({np.exp(beta_cite[bottom_idx].mean()):.1f}x)\")\n",
    "print(f\"    Avg β_hat: {beta_hat_cite[bottom_idx].mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Gallery Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"GALLERY SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\"\"\n",
    "{'Model':<12} {'Outcome':<20} {'Treatment':<15} {'X Dim':<8} {'Covers?':<10} {'Corr(β)':<10}\n",
    "{'-'*80}\n",
    "{'Linear':<12} {'Log wages':<20} {'Experience':<15} {64:<8} {result_wages.ci_lower <= mu_true_wages <= result_wages.ci_upper!s:<10} {corr_wages:.3f}\n",
    "{'Logit':<12} {'Purchase (0/1)':<20} {'Discount %':<15} {64:<8} {result_purchase.ci_lower <= mu_true_purchase <= result_purchase.ci_upper!s:<10} {corr_purchase:.3f}\n",
    "{'Poisson':<12} {'Citations':<20} {'Open Access':<15} {64:<8} {result_cite.ci_lower <= mu_true_cite <= result_cite.ci_upper!s:<10} {corr_cite:.3f}\n",
    "{'-'*80}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Insights\n",
    "\n",
    "1. **EMBEDDINGS AS COVARIATES**: Feature embeddings (64 dim) work seamlessly as covariates X. The neural network learns which dimensions drive heterogeneity.\n",
    "\n",
    "2. **VALID INFERENCE**: Despite high-dimensional X, influence function correction provides valid 95% confidence intervals. Naive SEs would be ~5x too small.\n",
    "\n",
    "3. **HETEROGENEITY RECOVERY**: The package captures treatment effect heterogeneity driven by latent factors in the embeddings. This enables:\n",
    "   - Targeting (which products to discount?)\n",
    "   - Personalization (which workers benefit from training?)\n",
    "   - Policy design (which papers to make open access?)\n",
    "\n",
    "4. **REAL-WORLD USAGE**:\n",
    "   - Replace simulated embeddings with real features (PCA of BERT/ResNet)\n",
    "   - For very high-dim embeddings (384-768), use larger N or apply PCA first\n",
    "   - Rule of thumb: n/dim ratio > 50 for stable estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## HTE Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_hte_distribution(name, beta_true, beta_hat, unit=\"\"):\n",
    "    \"\"\"Print HTE distribution summary.\"\"\"\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  {'':20} {'True β(X)':<20} {'Estimated β̂(X)':<20}\")\n",
    "    print(f\"  {'-'*60}\")\n",
    "    print(f\"  {'Mean':<20} {beta_true.mean():<20.4f} {beta_hat.mean():<20.4f}\")\n",
    "    print(f\"  {'Std Dev':<20} {beta_true.std():<20.4f} {beta_hat.std():<20.4f}\")\n",
    "    print(f\"  {'Min':<20} {beta_true.min():<20.4f} {beta_hat.min():<20.4f}\")\n",
    "    print(f\"  {'25th %ile':<20} {np.percentile(beta_true, 25):<20.4f} {np.percentile(beta_hat, 25):<20.4f}\")\n",
    "    print(f\"  {'Median':<20} {np.median(beta_true):<20.4f} {np.median(beta_hat):<20.4f}\")\n",
    "    print(f\"  {'75th %ile':<20} {np.percentile(beta_true, 75):<20.4f} {np.percentile(beta_hat, 75):<20.4f}\")\n",
    "    print(f\"  {'Max':<20} {beta_true.max():<20.4f} {beta_hat.max():<20.4f}\")\n",
    "    if unit:\n",
    "        print(f\"\\n  Interpretation: β represents {unit}\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"HETEROGENEOUS TREATMENT EFFECT (HTE) DISTRIBUTIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print_hte_distribution(\n",
    "    \"LINEAR: Experience → Wages\",\n",
    "    beta_wages, beta_hat_wages,\n",
    "    \"% wage increase per year of experience\"\n",
    ")\n",
    "\n",
    "print_hte_distribution(\n",
    "    \"LOGIT: Discount → Purchase\",\n",
    "    beta_purchase, beta_hat_purchase,\n",
    "    \"log-odds increase per 1% discount\"\n",
    ")\n",
    "\n",
    "print_hte_distribution(\n",
    "    \"POISSON: Open Access → Citations\",\n",
    "    beta_cite, beta_hat_cite,\n",
    "    \"log citation rate increase from OA\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ascii_histogram(data, bins=20, width=50, title=\"\"):\n",
    "    \"\"\"Print ASCII histogram.\"\"\"\n",
    "    counts, edges = np.histogram(data, bins=bins)\n",
    "    max_count = max(counts)\n",
    "    print(f\"\\n  {title}\")\n",
    "    print(f\"  {'─'*width}\")\n",
    "    for i, count in enumerate(counts):\n",
    "        bar_len = int(count / max_count * (width - 15)) if max_count > 0 else 0\n",
    "        lo, hi = edges[i], edges[i+1]\n",
    "        print(f\"  {lo:>6.3f} │{'█' * bar_len}\")\n",
    "    print(f\"  {'─'*width}\")\n",
    "\n",
    "print(\"-\"*80)\n",
    "print(\"HTE DISTRIBUTIONS (Estimated β̂(X))\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "ascii_histogram(beta_hat_wages, title=\"LINEAR: β̂(X) for Experience Effect on Wages\")\n",
    "ascii_histogram(beta_hat_purchase, title=\"LOGIT: β̂(X) for Discount Effect on Purchase\")\n",
    "ascii_histogram(beta_hat_cite, title=\"POISSON: β̂(X) for Open Access Effect on Citations\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"GALLERY COMPLETE\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}