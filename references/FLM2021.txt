Deep Neural Networks for Estimation and Inference‚àó
Max H. Farrell

Tengyuan Liang

Sanjog Misra

University of Chicago, Booth School of Business

arXiv:1809.09953v3 [econ.EM] 18 Sep 2019

September 19, 2019

Abstract
We study deep neural networks and their use in semiparametric inference. We establish
novel rates of convergence for deep feedforward neural nets. Our new rates are sufficiently fast
(in some cases minimax optimal) to allow us to establish valid second-step inference after firststep estimation with deep learning, a result also new to the literature. Our estimation rates
and semiparametric inference results handle the current standard architecture: fully connected
feedforward neural networks (multi-layer perceptrons), with the now-common rectified linear
unit activation function and a depth explicitly diverging with the sample size. We discuss other
architectures as well, including fixed-width, very deep networks. We establish nonasymptotic
bounds for these deep nets for a general class of nonparametric regression-type loss functions,
which includes as special cases least squares, logistic regression, and other generalized linear
models. We then apply our theory to develop semiparametric inference, focusing on causal parameters for concreteness, such as treatment effects, expected welfare, and decomposition effects.
Inference in many other semiparametric contexts can be readily obtained. We demonstrate the
effectiveness of deep learning with a Monte Carlo analysis and an empirical application to direct
mail marketing.

Keywords: Deep Learning, Neural Networks, Rectified Linear Unit, Nonasymptotic Bounds,
Convergence Rates, Semiparametric Inference, Treatment Effects, Program Evaluation, Treatment
Targeting.

1

Introduction

Statistical machine learning methods are being rapidly integrated into the social and medical sciences. Economics is no exception, and there has been a recent surge of research that applies and
explores machine learning methods in the context of econometric modeling, particularly in ‚Äúbig
data‚Äù settings. Furthermore, theoretical properties of these methods are the subject of intense recent study. This has netted several breakthroughs both theoretically, such as robust, valid inference
following machine learning, and in novel applications and conclusions. Our goal in the present work
‚àó
We thank Milica Popovic for outstanding research assistance. Liang gratefully acknowledges support from the
George C. Tiao Fellowship. Misra gratefully acknowledges support from the Neubauer Family Foundation. We are
thank Guido Imbens, the handling co-editor, and two anonymous reviewers, as well as Alex Belloni, Xiaohong Chen,
Denis Chetverikov, Chris Hansen, Whitney Newey, and Andres Santos, for thoughtful comments, suggestions, and
discussions that substantially improved the paper.

is to study a particular statistical machine learning technique which is widely popular in industrial
applications, but less frequently used in academic work and largely ignored in recent theoretical
developments on inference: deep neural networks. To our knowledge we provide the first inference
results using deep learning methods.
Neural networks are estimation methods that model the relationship between inputs and outputs using layers of connected computational units (neurons), patterned after the biological neural
networks of brains. These computational units sit between the inputs and output and allow datadriven learning of the appropriate model, in addition to learning the parameters of that model.
Put into terms more familiar in nonparametric econometrics: neural networks can be thought of as
a (complex) type of sieve estimation where the basis functions are flexibly learned from the data.
Neural networks are perhaps not as familiar to economists as other methods, and indeed, were out
of favor in the machine learning community for several years, returning to prominence only very
recently in the form of deep learning. Deep neural nets contain many hidden layers of neurons
between the input and output layers, and have been found to exhibit superior performance across
a variety of contexts. Our work aims to bring wider attention to these methods and to take the
first step toward filling the gaps in the theoretical understanding of inference using deep neural
networks. Our results can be used in many economic contexts, including selection models, games,
consumer surplus, and dynamic discrete choice.
Before the recent surge in attention, neural networks had taken a back seat to other methods
(such as kernel methods or forests) largely because of their modest empirical performance and challenging optimization. However, the availability of scalable computing and stochastic optimization
techniques (LeCun et al., 1998; Kingma and Ba, 2014) and the change from smooth sigmoid-type
activation functions to rectified linear units (ReLU), x 7‚Üí max(x, 0) (Nair and Hinton, 2010), have
seemingly overcome optimization hurdles and now this form of deep learning matches or sets the
state of the art in many prediction contexts (Krizhevsky et al., 2012; He et al., 2016). Our theoretical results speak directly to this modern implementation of deep learning: we explicitly model the
depth of the network as diverging with the sample size and focus on the ReLU activation function.
Further back in history, before falling out of favor, neural networks were widely studied and
applied, particularly in the 1990s. In that time, shallow neural networks with smooth activation
functions were shown to have many good theoretical properties. Intuitively, neural networks are
1

a form of sieve estimation, wherein basis functions of the original variables are used to approximate unknown nonparametric objects. What sets neural nets apart is that the basis functions are
themselves learned from the data by optimizing over many flexible combinations of simple functions. It has been known for some time that such networks yield universal approximations (Hornik
et al., 1989). Comprehensive theoretical treatments are given by White (1992) and Anthony and
Bartlett (1999). Of particular relevance in this strand of theoretical work is Chen and White (1999),
where it was shown that single-layer, sigmoid-based networks could attain sufficiently fast rates for
semiparametric inference (see Chen (2007) for more references).
We explicitly depart from the extant literature by focusing on the modern setting of deep neural
networks with the rectified linear (ReLU) activation function. We provide nonasymptotic bounds
for nonparametric estimation using deep neural networks, immediately implying convergence rates.
The bounds and convergence rates appear to be new to the literature and are one of the main
theoretical contributions of the paper. We provide results for a general class of smooth loss functions
for nonparametric regression style problems, covering as special cases generalized linear models and
other empirically useful contexts. In our application to causal inference we specialize our results
to linear and logistic regression as concrete illustrations. Our proof strategy employs a localization
analysis that uses scale-insensitive measures of complexity, allowing us to consider richer classes
of neural networks. This is in contrast to analyses which restrict the networks to have bounded
parameters for each unit (discussed more below) and to the application of scale sensitive measures
such as metric entropy (used by Chen and White, 1999, for example). These approaches would
not deliver our sharp bounds and fast rates. Recent developments in approximation theory and
complexity for deep ReLU networks are important building blocks for our results.
Our second main result establishes valid inference on finite-dimensional parameters following
first-step estimation using deep learning. We focus on causal inference for concreteness and wide
applicability, as well as to allow direct comparison to the literature. Program evaluation with
observational data is one of the most common and important inference problems, and has often
been used as a test case for theoretical study of inference following machine learning (e.g., Belloni
et al., 2014; Farrell, 2015; Belloni et al., 2017; Athey et al., 2018). Causal inference as a whole is a
vast literature; see Imbens and Rubin (2015) for a broad review and Abadie and Cattaneo (2018)
for a recent review of program evaluation methods, and further references in both. Deep neural
2

networks have been argued (experimentally) to outperform the previous state-of-the-art in causal
inference (Westreich et al., 2010; Johansson et al., 2016; Shalit et al., 2017; Hartford et al., 2017).
To the best of our knowledge, ours are among the first theoretical results that explicitly deliver
inference using deep neural networks.
We give specific results for average treatment effects, counterfactual expected utility/profits
from treatment targeting strategies, and decomposition effects. Our results allow planners (e.g.,
firms or medical providers) to compare different strategies, either predetermined or estimated using
auxiliary data, and recognizing that targeting can be costly, decide which strategy to implement. An
interesting, and potentially useful, point we make in this context is that the selection on observables
framework yields identification of counterfactual average outcomes without additional structural
assumptions, so that, e.g., expected profit from a counterfactual treatment rule can be evaluated.
The usefulness of our deep learning results is of course not limited to causal inference. In
particular, our results yield inference on essentially any estimand that admits a locally robust
estimator (Chernozhukov et al., 2018c) that depends only on target functions within our class
of loss function (under appropriate regularity conditions). Our aim is not to innovate at the
semiparametric step, for example by seeking weaker conditions on the first stage, but rather, we
aim to utilize such results. Prior work has verified the high-level conditions for other first-stage
estimators, such as traditional kernels or series/sieves, lasso methods, sigmoid-based shallow neural
networks, and others (under suitable assumptions for each method). Our work contributes directly
to this area of research by showing that deep nets are a valid and useful first-step estimator, in
particular, attaining a rate of o(n‚àí1/4 ) under appropriate smoothness conditions. Finally, we do
not rely on sample splitting or cross fitting. In particular, we use localization explicitly to directly
verify conditions required for valid inference, which may be a novel application of this proof method
that is useful in future semiparametric inference problems.
We numerically illustrate our results, and more generally the utility of deep learning, with a
detailed simulation study and an empirical study of a direct mail marketing campaign. Our data
come from a large US consumer products retailer and consists around to three hundred thousand
consumers with one hundred fifty covariates. Hitsch and Misra (2018) recently used this data to
study various estimators, both traditional and modern, of heterogeneous treatment effects. We refer
the reader to that paper for a more complete description of the data as well as results using other
3

estimators (see also Hansen et al. (2017)). We study the effect of catalog mailings on consumer
purchases, and moreover, compare different targeting strategies (i.e. to which consumers catalogs
should be mailed). The cost of sending out a single catalog can be close to one dollar, and with
millions being set out, carefully assessing the targeting strategy is crucial. Our results suggest that
deep nets are at least as good as (and sometimes better than) the best methods in Hitsch and Misra
(2018).
The remainder of the paper proceeds as follows. Next, we briefly review the related theoretical literature. Section 2 introduces deep ReLU networks and states our main theoretical results:
nonasymptotic bounds and convergence rates for general nonparametric regression-type loss functions. The semiparametric inference problem is set up in Section 3 and asymptotic results are
presented in Section 4. The empirical application is presented in Section 5. Results of a simulation
study are reported in Section 6. Section 7 concludes. All proofs are given in the appendix.

1.1

Related Theoretical Literature

Our paper contributes to several rapidly growing literatures, and we can not hope to do justice
to each here. We give only those citations of particular relevance; more references can be found
within these works. First, there has been much recent study of the statistical properties of the
machine learning tools as an end in itself. Many studies have focused on the lasso and its variants
(Bickel et al., 2009; Belloni et al., 2011, 2012; Farrell, 2015) and tree/forest based methods (Wager
and Athey, 2018), though earlier work studied shallow (typically with a single hidden layer) neural
networks with smooth activation functions (White, 1989, 1992; Chen and White, 1999). We fill the
gap in this literature by studying deep neural networks with the non-smooth ReLU activation.
A second, intertwined strand of literature focuses on inference following the use of machine
learning methods, often with a focus on average causal effects. Initial theoretical results were concerned with obtaining valid inference on a coefficient in a high-dimensional regression, following
model selection or regularization, with particular focus on the lasso (Belloni et al., 2012; Javanmard and Montanari, 2014; van de Geer et al., 2014). Intuitively, this is a semiparametric problem,
where the coefficient of interest is estimable at the parametric rate and the remaining coefficients
are collectively a nonparametric nuisance parameter estimated using machine learning methods.
Building on this intuition, many have studied the semiparametric stage directly, such as obtaining
4

novel, weaker conditions easing the application of machine learning methods (Belloni et al., 2014;
Farrell, 2015; Chernozhukov et al., 2018c; Belloni et al., 2018, and references therein). Conceptually related to this strand are targeted maximum likelihood (van der Laan and Rose, 2001) and
the higher-order influence functions (Robins et al., 2008, 2017). Our work builds on this work,
employing conditions therein, and in particular, verifying them for deep ReLU nets.
Finally, our convergence rates build on, and contribute to, the recent theoretical machine learning literature on deep neural networks. Because of the renaissance in deep learning, a considerable
amount of study has been done in recent years. Of particular relevance to us are Yarotsky (2017,
2018) and Bartlett et al. (2017); a recent textbook treatment, containing numerous other references,
is given by Goodfellow et al. (2016).

2

Deep Neural Networks

In this section we will give our main theoretical results: nonasymptotic bounds and associated
convergence rates for deep neural network estimation. The utility of these results for secondstep semiparametric causal inference (the downstream task), for which our rates are sufficiently
rapid, is demonstrated in Section 4. We view our results as an initial step in establishing both
the estimation and inference theory for modern deep learning, i.e. neural networks built using
the multi-layer perceptron architecture (described below) and the nonsmooth ReLU activation
function. This combination is crucial: it has demonstrated state of the art performance empirically
and can be feasibly optimized. This is in contrast with sigmoid-based networks, either shallow (for
which theory exists, but may not match empirical performance) or deep (which are not feasible to
optimize), and with shallow ReLU networks, which are not known to approximate broad classes
functions.
As neural networks are perhaps less familiar to economists and other social scientists, we first
briefly review the construction of deep ReLU nets. Our main focus will be on the fully connected
feedfoward neural network, frequently referred to as a multi-layer perceptron, as this is the most
commonly implemented network architecture and we want our results to inform empirical practice.
However, our results are more general, accommodating other architectures provided they are able
to yield a universal approximation (in the appropriate function class), and so we review neural nets

5

more generally and give concrete examples.
Our goal is to estimate an unknown, assumed-smooth function f‚àó (x), that relates the covariates
X ‚àà Rd to an outcome Y as the minimizer of the expectation of the per-observation loss function.
Collecting these random variables into the vector Z = (Y, X 0 )0 ‚àà Rd+1 , with z = (y, x0 )0 denoting
a realization, we write
f‚àó = arg min E [` (f, Z)] .
We allow for any loss function that is Lipschitz in f and obeys a curvature condition around f‚àó .
Specifically, for constants c1 , c2 , and C` that are bounded and bounded away from zero, we assume
that `(f, z) obeys
|`(f, z) ‚àí `(g, z)| ‚â§ C` |f (x) ‚àí g(x)|,




c1 E (f ‚àí f‚àó )2 ‚â§ E[`(f, Z)] ‚àí E[`(f‚àó , Z)] ‚â§ c2 E (f ‚àí f‚àó )2 .

(2.1)

Our results will be stated for a general loss obeying these two conditions.1 We give a unified
localization analysis of all such problems. This family of loss function covers many interesting
problems. Two leading examples, used in our application to causal inference, are least squares and
logistic regression, corresponding to the outcome and propensity score models respectively. For
least squares, the target function and loss are

f‚àó (x) := E[Y |X = x]

and

1
` (f, z) = (y ‚àí f (x))2 ,
2

(2.2)



` (f, z) = ‚àíyf (x) + log 1 + ef (x) .

(2.3)

respectively, while for logistic regression these are

f‚àó (x) := log

E[Y |X = x]
1 ‚àí E[Y |X = x]

and

Lemma 8 verifies, with explicit constants, that (2.1) holds for these two. Losses obeying (2.1)
extend beyond these cases to other generalized linear models, such as count models, and can even
cover multinomial logistic regression (multiclass classification), as shown in Lemma 9.
1

We thank an anonymous referee for suggesting this approach.

6

Figure 1: Illustration of a feedforward neural network with W = 18, L = 2, U = 5, and input
dimension d = 2. The input units are shown in blue at left, the output in red at right, and the
hidden units in grey between them.

2.1

Neural Network Constructions

For any loss, we estimate the target function using a deep ReLU network. We will give a brief outline
of their construction here, paying closer attention to the details germane to our theory; complete
introductions, and further references, are given by Anthony and Bartlett (1999) and Goodfellow
et al. (2016).
The crucial choice is the specific network architecture, or class. In general we will call this
FDNN . From a theoretical point of view, different classes have different complexity and different
approximating power. We give results for several concrete examples below. We will focus on
feedforward neural networks. An example of a feedforward network is shown in Figure 1. The
network consists of d input units, corresponding to the covariates X ‚àà Rd , one output unit for
the outcome Y . Between these are U hidden units, or computational nodes or neurons. These
are connected by a directed acyclic graph specifying the architecture. The key graphical feature
of a feedforward network is that hidden units are grouped in a sequence of L layers, the depth
of the network, where a node is in layer l = 1, 2, . . . , L, if it has a predecessor in layer l ‚àí 1 and
no predecessor in any layer l0 ‚â• l. The width of the network at a given layer, denoted Hl , is the
number of units in that layer. The network is completed with the choice of an activation function
œÉ : R 7‚Üí R applied to the output of each node as described below. In this paper, we focus on
the popular ReLU activation function œÉ(x) = max(x, 0), though our results can be extended (at
notational cost) to cover piecewise linear activation functions (see also Remark 3).
An important and widely used subclass is the one that is fully connected between consecutive
layers but has no other connections and each layer has number of hidden units that are of the same
order of magnitude. This architecture is often referred to as a Multi-Layer Perceptron (MLP) and

7

Figure 2: Illustration of multi-layer perceptron FMLP with H = 3, L = 2 (U = 6, W = 25), and
input dimension d = 2.
we denote this class as FMLP . See Figure 2, cf. Figure 1. We will assume that all the width of all
layers share a common asymptotic order H, implying that for this class U  LH.
We will allow for generic feedforward networks in our results, but we present special results for
the MLP case, as it is widely used in empirical practice. As we will see below, the architecture,
through its complexity, and more importantly, approximation power, plays a crucial role in the
final convergence rate. In particular, we find only a suboptimal rate for the MLP case, but our
upper bound is still sufficient for semiparametric inference. As a note on exposition, while our
main results are in fact nonasymptotic bounds that hold with high probability, for simplicity we
will refer to them as ‚Äúrates‚Äù in most discussion.
To build intuition on the computation, and compare to other nonparametric methods, let us
focus on least squares for the moment, i.e. Equation (2.2), with a continuous outcome using a
multilayer perceptron with constant width H. Each hidden unit u receives an input in the form of
a linear combination xÃÉ0 w +b, and then returns œÉ(xÃÉ0 w +b), where the vector xÃÉ collects the output of
all the units with a directed edge into u (i.e., from prior layers), w is a vector of weights, and b is a
constant term. (The constant term is often referred to as the ‚Äúbias‚Äù in the deep learning literature,
but given the loaded meaning of this term in inference, we will largely avoid referring to b as a bias.)
The final layer‚Äôs output is simply xÃÉ0 w + b in the least squares case. The collection, over all nodes, of
w and b, constitutes the parameters Œ∏ which are optimized in the final estimation. We denote W as
the total number of parameters of the network. For the MLP, W = (d+1)H+(L‚àí1)(H 2 +H)+H+1.
In general, W , U , L, and H, may change with n, but we suppress this in the notation.
Optimization proceeds layer-by-layer using (variants of) stochastic gradient descent, with gradients of the parameters calculated by back-propagation (implementing the chain rule) induced
by the network structure. To see this, let xÃÉh,l denote the scalar output of a node u = (h, l), for

8

h = 1, . . . H, l = 1, . . . L, and let xÃÉl = (xÃÉ1,l , . . . , xÃÉH,l )0 for layer l ‚â§ L. Each node thus computes
xÃÉh,l = œÉ(xÃÉ0l‚àí1 wh,l‚àí1 + bh,l‚àí1 ) and the final output is yÃÇ = xÃÉ0L wL + bL . Once we recall that the
network begins with the original observation x, we can view xÃÉL = xÃÉL (x), and thus the final output may be seen as a basis function approximation (albeit a complex and random one) written
as fÀÜMLP (x) = xÃÉL (x)0 wL + bL , which is reminiscent of a traditional series (linear sieve) estimator. If all layers save the last were fixed, we could simply optimize using least squares directly:
(wL , bL ) = arg minw,b kyi ‚àí xÃÉ0L w ‚àí bk2n .
The crucial distinction is that the basis functions xÃÉL (¬∑) are learned from the data. The ‚Äúbasis‚Äù
is xÃÉL = (xÃÉ1,L , . . . , xÃÉH,L )0 , where each xÃÉh,L = œÉ(xÃÉ0L‚àí1 wh,L‚àí1 + bh,L‚àí1 ). Therefore, ‚Äúbefore‚Äù we can
0
solve the least squares problem above, we would have to estimate (wh,L‚àí1
, bh,L‚àí1 ), h = 1, . . . , H,

anticipating the final estimation. These in turn depend on the prior layer, and so forth back to
the original inputs X. Measuring the gradient of the loss with respect to each layer of parameters
uses the chain rule recursively, and is implemented by back-propagation. This is simply a sketch
of course; for further introduction, see Hastie et al. (2009) and Goodfellow et al. (2016).
To further clarify the use of deep nets, it is useful to make explicit analogies to more classical
nonparametric techniques, leveraging the form fÀÜMLP (x) = xÃÉL (x)0 wL + bL . For a traditional series
estimator, say smoothing splines, the two choices for the practitioner are the spline basis (the
shape and the degree) and the number of terms (knots), commonly referred to as the smoothing
and tuning parameters, respectively. In kernel regression, these would respectively be the shape of
the kernel (and degree of local polynomial) and the bandwidth(s). For neural networks, the same
phenomena are present: the architecture as a whole (the graph structure and activation function)
are the smoothing parameters while the width and depth play the role of tuning parameters for a
set architecture.
The architecture plays a crucial role in that it determines the approximation power of the
network, and it is worth noting that because of the relative complexity of neural networks, such
approximations, and comparisons across architectures, are not simple. It is comparatively obvious
that quartic splines are more flexible than cubic splines (for the same number of knots) as is a
higher degree local polynomial (for the same bandwidth). At a glance, it may not be clear what
function class a given network architecture (width, depth, graph structure, and activation function)
can approximate. As we will show below, the MLP architecture is not yet known to yield an optimal
9

approximation (for a given width and depth) and therefore we are only able to prove a bound with
slower than optimal rate. As a final note, computational considerations are important for deep nets
in a way that is not true conventionally; see Remarks 1, 2, and 3.
Just as for classical nonparametrics, for a fixed architecture, it is the tuning parameter choices
that determine the rate of convergence (for a fixed smoothness of the underlying function). The
recent wave of theoretical study of deep learning is still in its infancy. As such, there is no understanding yet of optimal architecture(s) or tuning parameters. Choices of both are quite difficult,
and only preliminary research has been done (see e.g., Daniely, 2017; Telgarsky, 2016; Safran and
Shamir, 2016; Mhaskar and Poggio, 2016a; Raghu et al., 2017, and references therein). Further
exploration of these ideas is beyond the current scope. It is interesting to note that in some cases,
a good approximation can be obtained even with a fixed width H, provided the network is deep
enough, a very particular way of enriching the ‚Äúsieve space‚Äù FDNN ; see Corollary 2.
In sum, for a user-chosen architecture FDNN , encompassing the choices œÉ(¬∑), U , L, W , and the
graph structure, the final estimate is computed using observed samples zi = (yi , x0i )0 , i = 1, 2, . . . , n,
of Z, by solving

fbDNN := arg min

n
X

` (f, zi ) .

(2.4)

fŒ∏ ‚ààFDNN i=1
kfŒ∏ k‚àû ‚â§2M

Recall that Œ∏ collects, over all nodes, the weights and constants w and b. When (2.4) is restricted
to the MLP class we denote the resulting estimator fbMLP . The choice of M may be arbitrarily
large, and is part of the definition of the class FDNN . This is neither a tuning parameter nor
regularization in the usual sense: it is not assumed to vary with n, and beyond being finite and
bounding kf‚àó k‚àû (see Assumption 1), no properties of M are required. This is simply a formalization
of the requirement that the optimizer is not allowed to diverge on the function level in the l‚àû sense‚Äì
the weakest form of constraint. It is important to note that while typically regularization will alter
the approximation power of the class, that is not the case with the choice of M as we will assume
that the true function f‚àó (x) is bounded, as is standard in nonparametric analysis. With some extra
notational burden, one can make the dependence of the bound on M explicit, though we omit this
for clarity as it is not related to statistical issues.

10

Remark 1. In applications it is common to apply some form of regularization to the optimization
of (2.4). However, in theory, the role of explicit regularization is unclear and may be unnecessary,
as stochastic gradient descent presents good, if not better, solutions empirically (see Section 6 and
Zhang et al., 2016). Regularization may improve empirical performance in low signal-to-noise ratio
problems. A detailed investigation is beyond the scope of the current work, though we do investigate
this numerically in Sections 5 and 6. There are many alternative regularization methods, including
L1 and L2 (weight decay) penalties, drop out, and others.

2.2

y

Bounds and Convergence Rates for Multi-Layer Perceptrons

We can now state our main theoretical results: bounds and convergence rates for deep ReLU
networks. All proofs appear in the Appendix. We study neural networks from a nonparametric
point of view (e.g., White, 1989, 1992; Schmidt-Hieber, 2017; Liang, 2018; Bauer and Kohler,
2017, in specific scenarios). Chen and Shen (1998) and Chen and White (1999) share our goal, fast
convergence rates for use in semiparametric inference, but focus on shallow, sigmoid-based networks
compared to our deep, ReLU-based networks, though they consider dependent data which we do
not. Our theoretical approach is quite different. In particular, Chen and White (1999) obtain
sufficiently fast rates by following the approach of Barron (1993) in using Maurey‚Äôs method (Pisier,
1981) for approximation, but applying the refinement of Makovoz (1996). Our analysis of deep
nets instead employs localization methods (Koltchinskii and Panchenko, 2000; Bartlett et al., 2005;
Koltchinskii, 2006, 2011; Liang et al., 2015), along with the recent approximation work of Yarotsky
(2017, 2018) and complexity results of Bartlett et al. (2017).
The regularity conditions we require are collected in the following.
Assumption 1. Assume that zi = (yi , x0i )0 , 1 ‚â§ i ‚â§ n are i.i.d. copies of Z = (Y, X) ‚àà Y √ó[‚àí1, 1]d ,
where X is continuously distributed. For an absolute constant M > 0, assume kf‚àó k‚àû ‚â§ M and
Y ‚äÇ [‚àíM, M ].
This assumption is fairly standard in nonparametrics. The only restriction worth mentioning
is that the outcome is bounded. In many cases this holds by default (such as logistic regression,
where Y = {0, 1}) or count models (where Y = {0, 1, . . . , M }, with M limited by real-world
constraints). For continuous outcomes, such as least squares regression, our restriction is not
11

substantially more limiting than the usual assumption of a model such as Y = f‚àó (X) + Œµ, where
X is compact-supported, f‚àó is bounded, and the stochastic error Œµ possesses many moments.
Indeed, in many applications such a structure is only coherent with bounded outcomes, such as the
common practice of including lagged outcomes as predictors. Next, the assumption of continuously
distributed covariates is quite standard. From a theoretical point of view, covariates taking on
only a few values can be conditioned on and then averaged over, and these will, as usual, not enter
into the dimensionality which curses the rates. Discrete covariates taking on many values may be
more realistically thought of as continuous, and it may be more accurate to allow these to slow
the convergence rates. Our focus on L2 (X) convergence allows for these essentially automatically.
Finally, from a practical point of view, deep networks handle discrete covariates seamlessly and
have demonstrated excellent empirical performance, which is in contrast to other more classical
nonparametric techniques that may require manual adaptation.
Proceeding now to our results, we begin with the most important network architecture, the
multi-layer perceptron. This is the most widely used network architecture in practice and an
important contribution of our work is to cover this directly, along with ReLU activation. MLPs
are now known to approximate smooth functions well, leading to our next assumption: that the
target function f‚àó lies in a Sobolev ball with certain smoothness. Discussion of Sobolev spaces, and
comparisons to HoÃàlder and Besov spaces, can be found in Gine and Nickl (2016).
Assumption 2. Assume f‚àó lies in the Sobolev ball W Œ≤,‚àû ([‚àí1, 1]d ), with smoothness Œ≤ ‚àà N+ ,
(
f‚àó (x) ‚àà W

Œ≤,‚àû

d

([‚àí1, 1] ) :=

)
Œ±

f : max ess sup |D f (x)| ‚â§ 1 ,
Œ±,|Œ±|‚â§Œ≤ x‚àà[‚àí1,1]d

where Œ± = (Œ±1 , . . . , Œ±d ), |Œ±| = Œ±1 + . . . + Œ±d and DŒ± f is the weak derivative.
Under Assumptions 1 and 2 we obtain the following result, which, to the best of our knowledge,
is new to the literature. In some sense, this is our main result for deep learning, as it deals with
the most common architecture. We apply this in Sections 4 and 5 for semiparametric inference.
Theorem 1 (Multi-Layer Perceptron). Suppose Assumptions 1 and 2 hold. Let fbMLP be the deep
MLP-ReLU network estimator defined by (2.4), restricted to FMLP , for a loss function obeying
d

(2.1), with width H  n 2(Œ≤+d) log2 n and depth L  log n. Then with probability at least 1 ‚àí
12

d

exp(‚àín Œ≤+d log8 n), for n large enough,


Œ≤
log log n
‚àí Œ≤+d
2
8
b
(a) kfMLP ‚àí f‚àó kL2 (x) ‚â§ C ¬∑ n
log n +
and
n


h
i
Œ≤
log log n
‚àí Œ≤+d
2
8
b
(b) En (fMLP ‚àí f‚àó ) ‚â§ C ¬∑ n
,
log n +
n
for a constant C > 0 independent of n, which may depend on d, M , and other fixed constants.
Several aspects of this result warrant discussion. We build on the recent results of Bartlett et al.
(2017), who find nearly-tight bounds on the Vapnik-Chervonenkis (VC) and Pseudo-dimension of
deep nets. One contribution of our proof is to use a scale sensitive localization theory with scale
insensitive measures, such as VC- or Pseudo-dimension, for deep neural networks for general smooth
loss functions. For the special case of least squares regression, Koltchinskii (2011) uses a similar
approach, and a similar result to our Theorem 1(a) can be derived for this case using his Theorem
5.2 and Example 3 (p. 85f).
This approach has two tangible benefits. First, we do not restrict the class of network architectures to have bounded weights for each unit (scale insensitive), in accordance to standard practice
(Zhang et al., 2016) and in contrast to the classic sieve analysis with scale sensitive measure such
as metric entropy. Moreover, this allows for a richer set of approximating possibilities, in particular
allowing more flexibility in seeking architectures with specific properties, as we explore in the next
subsection. Second, from a technical point of view, we are able to attain a faster rate on the second
term of the bound, order n‚àí1 in the sample size, instead of the n‚àí1/2 that would result from a
direct application of uniform deviation bounds. This upper bound informs the trade offs between
width and depth, and the approximation power, and may point toward optimal architectures for
statistical inference.
This result gives a nonasymptotic bound that holds with high probability. As mentioned above,
we will generally refer to our results simply as ‚Äúrates‚Äù when this causes no confusion. This result
relies on choosing H appropriately given the smoothness Œ≤ of Assumption 2. Of course, the true
smoothness is unknown and thus in practice the ‚ÄúŒ≤‚Äù appearing in H, and consequently in the
convergence rates, need not match that of Assumption 2. In general, the rate will depend on the
smaller of the two. Most commonly it is assumed that the user-chosen Œ≤ is fixed and that the truth
is smoother; witness the ubiquity of cubic splines and local linear regression. Rather than spell
13

out these consequences directly, we will tacitly assume the true smoothness is not less than the Œ≤
appearing in H (here and below). Adaptive approaches, as in classical nonparametrics, may also
be possible with deep nets, but are beyond the scope of this study.
Even with these choices of H and L, the bound of Theorem 1 is not optimal (for fixed Œ≤, in the
sense of Stone (1982)). We rely on the explicit approximating constructions of Yarotsky (2017), and
it is possible that in the future improved approximation properties of MLPs will be found, allowing
for a sharpening of the results of Theorem 1 immediately, i.e. without change to our theoretical
argument. At present, it is not clear if this rate can be improved, but it is sufficiently fast for valid
inference.

2.3

Other Network Architectures

Theorem 1 covers only one specific architecture, albeit the most important one at present. However,
given that this field is rapidly evolving, it is important to consider other possible architectures which
may be beneficial in some cases. To this end, we will state a more generic result and then two
specific examples: one to obtain a faster rate of convergence and one for fixed-width networks. All
of these results are, at present, more of theoretical interest than practical value, as they are either
agnostic about the network (thus infeasible) or rely on more limiting assumptions.
In order to be agnostic about the specific architecture of the network we need to be flexible in
the approximation power of the class. To this end, we will replace Assumption 2 with the following
generic assumption, rather more of a definition, regarding the approximation power of the network.
Assumption 3. Let f‚àó lie in a class F. For the feedforward network class FDNN , used in (2.4),
let the approximation error DNN be

DNN := sup

inf

f‚àó ‚ààF f ‚ààFDNN
kf k‚àû ‚â§2M

kf ‚àí f‚àó k‚àû .

It may be possible to require only an approximation in the L2 (X) norm, but this assumption
matches the current approximation theory literature and is more comparable with other work in
nonparametrics, and thus we maintain the uniform definition.
Under this condition we obtain the following generic result.

14

Theorem 2 (General Feedforward Architecture). Suppose Assumptions 1 and 3 hold. Let fbDNN
be the deep ReLU network estimator defined by (2.4), for a loss function obeying (2.1). Then with
probability at least 1 ‚àí e‚àíŒ≥ , for n large enough,


W L log W
log log n + Œ≥
2
2
b
(a) kfDNN ‚àí f‚àó kL2 (x) ‚â§ C
log n +
+ DNN and
n
n


h
i
log log n + Œ≥
W L log W
2
2
b
log n +
+ DNN ,
(b) En (fDNN ‚àí f‚àó ) ‚â§ C
n
n
for a constant C > 0 independent of n, which may depend on d, M , and other fixed constants.
This is a more general than Theorem 1, covering the general deep ReLU network problem
defined in (2.4), general feedforward architectures, and the general class of losses defined by (2.1).
The same comments as were made following Theorem 1 apply here as well: the same localization
argument is used with the same benefits. We explicitly use this in the next two corollaries, where
we exploit the allowed flexibility in controlling DNN by stating results for particular architectures.
The bound here is not directly applicable without specifying the network structure, which will
determine both the variance portion (through W , L, and U ) and the approximation error. With
these set, the bound becomes operational upon choosing Œ≥, which can be optimized as desired, and
this will immediately then yield a convergence rate.
Turning to special cases, we first show that the optimal rate of Stone (1982) can be attained, up
to log factors. However, this relies on a rather artificial network structure, designated to approximate functions in a Sobolev space well, but without concern for practical implementation. Thus,
while the following rate improves upon Theorem 1, we view this result as mainly of theoretical
interest: establishing that (certain) deep ReLU networks are able to attain the optimal rate.
Corollary 1 (Optimal Rate). Suppose Assumptions 1 and 2 hold. Let fbOPT solve (2.4) using the
d

(deep and wide) network of Yarotsky (2017, Theorem 1), with W  U  n 2Œ≤+d log n and depth
L  log n, the following hold with probability at least 1 ‚àí e‚àíŒ≥ , for n large enough,


2Œ≤
log
log
n
+
Œ≥
‚àí
2
4
(a) kfbOPT ‚àí f‚àó kL2 (x) ‚â§ C ¬∑ n 2Œ≤+d log n +
and
n


h
i
2Œ≤
log
log
n
+
Œ≥
‚àí
2
4
(b) En (fbOPT ‚àí f‚àó ) ‚â§ C ¬∑ n 2Œ≤+d log n +
,
n
for a constant C > 0 independent of n, which may depend on d, M , and other fixed constants.
15

Next, we turn to very deep networks that are very narrow, which have attracted substantial
recent interest. Theorem 1 and Corollary 1 dealt with networks where the depth and the width
grow with sample size. This matches the most common empirical practice, and is what we use in
Sections 5 and 6. However, it is possible to allow for networks of fixed width, provided the depth is
sufficiently large. The next result is perhaps the largest departure from the classical study of neural
networks: earlier work considered networks with diverging width but fixed depth (often a single
layer), while the reverse is true here. The activation function is of course qualitatively different as
well, being piecewise linear instead of smooth. Using recent results (Mhaskar and Poggio, 2016b;
Hanin, 2017; Yarotsky, 2018) we can establish the following rate for very deep, fixed-width MLPs.
Corollary 2 (Fixed Width Networks). Let the conditions of Theorem 1 hold, with Œ≤ ‚â• 1 in
d

Assumption 2. Let fbFW solve (2.4) for an MLP with fixed width H = 2d+10 and depth L  n 2(2+d) .
Then with probability at least 1 ‚àí e‚àíŒ≥ , for n large enough,


2
log log n + Œ≥
‚àí 2+d
2
2
b
(a) kfFW ‚àí f‚àó kL2 (x) ‚â§ C ¬∑ n
log n +
and
n


i
h
2
log log n + Œ≥
‚àí 2+d
2
2
b
(b) En (fFW ‚àí f‚àó ) ‚â§ C ¬∑ n
log n +
,
n
for a constant C > 0 independent of n, which may depend on d, M , and other fixed constants.
This result is again mainly of theoretical interest. The class is only able to approximate well
functions with Œ≤ = 1 (cf. the choice of L) which limits the potential applications of the result
because, in practice, d will be large enough to render this rate, unlike those above, too slow for use
in later inference procedures. In particular, if d ‚â• 3, the sufficient conditions of Theorem 3 fail.
Finally, as mentioned following Theorem 1, our theory here will immediately yield a faster
rate upon discovery of improved approximation power of this class of networks. In other words,
for example, if a proof became available that fixed-width, very deep networks can approximate
Œ≤-smooth functions (as in Assumption 2), then Corollary 2 will trivially be improvable to match
the rate of Theorem 1. Similarly, if the MLP architecture can be shown to share the approximation
power with that of Corollary 1, then Theorem 1 will itself deliver the optimal rate. Our proofs will
not require adjustment.
Remark 2. Although there has been a great deal of work in easing implementation (optimization
and tuning) of deep nets, it still may be a challenge in some settings, particularly when using
16

non-standard architectures. See also Remark 1. Given the renewed interest in deep networks,
this is an area of study already (Hartford et al., 2017; Polson and Rockova, 2018) and we expect
this to continue and that implementations will rapidly evolve. This is perhaps another reason that
Theorem 1 is, at the present time, the most practically useful, but that (as just discussed) Theorem
2 will be increasingly useful in the future.

y

Remark 3. Our results can be extended easily to include piecewise linear activation functions
beyond ReLU. Intuitively, being itself piecewise linear, appropriate combinations of a fixed number
of ReLU functions can equal a piecewise linear function (with a fixed number of knots) and therefore
the complexity and approximation power can be easily adjusted to this case. See Bartlett et al.
(2017).
In principle, similar rates of convergence could be attained for other activation functions, given
results on their approximation error. However, it is not clear what practical value would be offered
due to computational issues (in which the activation choice plays a crucial role). Indeed, the
recent switch to ReLU stems not from their greater approximation power, but from the fact that
optimizing a deep net with sigmoid-type activation is unstable or impossible in practice. Thus, while
it is certainly possible that we could complement the single-layer results with rates for sigmoid-based
deep networks, these results would have no consequences for real-world practice.
From a purely practical point of view, several variations of the ReLU activation function have
been proposed recently (including the so-called Leaky ReLU, Randomized ReLU, (Scaled) Exponential Linear Units, and so forth) and have been found in some experiments to improve optimization
properties. It is not clear what theoretical properties these activation functions have or if the computational benefits persist more generically, though this area is rapidly evolving. We conjecture
that our results could be extended to include these activation functions.

3

y

Parameters of Interest

We will use the results above, in particular Theorem 1, coupled with results in the semiparametric
literature, to deliver valid asymptotic inference for causal effects. The novelty of our results is not
in this semiparametric stage per se, but rather in delivering valid inference after relying on deep

17

learning for the first step estimation. In this section we define the parameters of interest, while
asymptotic inference is discussed next.
We will focus, for concreteness, on causal parameters that are of interest across different disciplines: average treatment effects, expected utility (or profits) under different targeting policies,
average effects on (non-)treated subpopulations, and decomposition effects. Our focus on causal
inference with observational data is due to the popularity of these estimands both in applications
and in theoretical work, thus allowing our results to be put to immediate use and easily compared to
prior literature. The average treatment effect in particular is often used as a benchmark parameter
for studying inference following machine learning (see references in the Introduction). However,
armed with our results for deep neural networks we can cover a great deal more (some discussion
is in Section 3.4).
The estimation of average causal effects is a well-studied problem, and we will give only a brief
overview here. Recent reviews and further references are given by Belloni et al. (2017); Athey et al.
(2017); Abadie and Cattaneo (2018). We consider the standard setup for program evaluation with
observational data: we observe a sample of n units, each exposed to a binary treatment, and for
each unit we observe a vector of pre-treatment covariates, X ‚àà Rd , treatment status T ‚àà {0, 1}, and
a scalar post-treatment outcome Y . The observed outcome obeys Y = T Y (1) + (1 ‚àí T )Y (0), where
Y (t) is the (potential) outcome under treatment status t ‚àà {0, 1}. The ‚Äúfundamental problem‚Äù is
that only Y (0) or Y (1) is observed for each unit, never both.
The crucial identification assumptions, which pertain to all the parameters we consider, are
selection on observables, also known as ignorability, unconfoundedness, missingness at random, or
conditional independence, and overlap, or common support. Let p(x) = P[T = 1|X = x] denote
the propensity score and ¬µt (x) = E[Y (t)|X = x], t ‚àà {0, 1} denote the two outcome regression
functions. We then assume the following throughout, beyond which, we will mostly need only
regularity conditions for inference.
Assumption 4. For t ‚àà {0, 1} and almost surely X, E[Y (t)|T, X = x] = E[Y (t)|X = x] and
pÃÑ ‚â§ p(x) ‚â§ 1 ‚àí pÃÑ for some pÃÑ > 0.
It will be useful to divide our discussion between parameters that are fully marginal averages,
such as the average treatment effect, and those which are for specific subpopulations. Here, ‚Äúsub18

populations‚Äù refer to the treated or nontreated groups, with corresponding parameters such as
the treatment effect for the treated. Any parameter, in either case, can be studied for a suitable
subpopulation defined by the covariates X, such as a specific demographic group. Though causal
effects as a whole share some structure, there are slight conceptual and notational differences. In
particular, the form of the efficient influence function and doubly robust estimator is different for
the two sets, but common within.

3.1

Full-Population Average Effect Parameters

Here we are interested in averages over the entire population. The prototypical parameter of interest
is the average treatment effect:
œÑ = E[Y (1) ‚àí Y (0)].

(3.1)

In the context of our empirical example, the treatment is being mailed a catalog and the outcome
is dollars spent (results for the binary purchase decision are available on request). The average
treatment effect, also referred to as ‚Äúlift‚Äù in digital contexts, corresponds to the expected gain in
revenue from an average individual receiving the catalog compared to the same person not receiving
the catalog.
A closely related parameter of interest is the average realized outcome, which in general may
be interpreted as the expected utility or welfare from a counterfactual treatment policy. In the
context of our empirical application this is expected profits; in a medical context it would be the
total health outcome. The question of interest here is whether a change in the treatment policy
would be beneficial in terms of increasing outcomes, and this is judged using observational data.
Intuitively, the average treatment effect is the expected gain from treating the ‚Äúnext‚Äù person,
relative to if they had not been exposed. That is, it is the expected change in the outcome.
Expected utility/profit, on the other hand, is concerned with the total outcome, not the difference
in outcomes. In the context of our empirical application, we are interested in total sales rather
than the change in sales. Our discussion is grounded in this language for easy comparison.
The parameter depends on a counterfactual/hypothetical treatment targeting strategy, which
is often itself the object of evaluation. This is simply a rule that assigns a given set of characteristics (e.g. a consumer profile), determined by the covariates X, to treatment status: that

19

is, a known function (which may include randomization but is not estimated from the sample)
s(x) : supp{X} 7‚Üí {0, 1}. Note well that this is not necessarily the observed treatment: s(xi ) 6= ti .
The policy maker may wish to evaluate the gain from targeting only a certain subset of customers,
a price discrimination strategy, or comparisons of different such policies. Our assumptions, while
standard, deliver identification of such counterfactuals at no cost.
The parameter of interest is expected utility, or profit, from a fixed policy, given by



œÄ(s) = E s(X)Y (1) + (1 ‚àí s(X)) Y (0) ,

(3.2)

where we make explicit the dependence on the policy s(¬∑). Compare to Equation (3.1) and recall
that the observed outcome obeys Y = T Y (1) + (1 ‚àí T )Y (0). Whereas œÑ is the gain in assigning the
next person to treatment and is given by the difference in potential outcomes, œÄ(s) is the expected
outcome that would be observed for the next person if the treatment rule were s(x).
A natural question is whether a candidate targeting strategy, say s0 (x), is superior to baseline
or status quo policy, s0 (x). This amounts to testing the hypothesis H0 : œÄ(s0 ) ‚â• œÄ(s0 ). To evaluate
this, we can study the difference in expected profits, which amounts to



œÄ(s0 , s0 ) = œÄ(s0 ) ‚àí œÄ(s0 ) = E (s0 (X) ‚àí s0 (X))Y (1) + s0 (X) ‚àí s0 (X) Y (0) .

(3.3)

Assumption 4 provides identification for œÄ(s) and œÄ(s0 , s0 ), arguing analogously as for œÑ . Moreover,
notice that œÄ(s0 , s0 ) = E[(s0 (X) ‚àí s0 (X))(Y (1) ‚àí Y (0))] = E[(s0 (X) ‚àí s0 (X))œÑ (X)], where œÑ (x) =
E[Y (1) ‚àí Y (0) | X = x] is the conditional average treatment effect. The latter form makes clear
that only those differently treated, of course, impact the evaluation of s0 compared to s0 . The
strategy s0 will be superior if, on average, it targets those with a higher individual treatment effect.
Estimating the optimal treatment policy from the data is discussed briefly in Section 3.3.
The common structure of these parameters is that they all involve full-population averages of
the potential outcomes, possibly scaled by a known function. For these parameters, the influence
function is known from Hahn (1998), and estimators based on the influence function are doubly
robust, as they remain consistent if either the regression functions or the propensity score are
correctly specified (Robins et al., 1994, 1995). With a slight abuse of terminology (since we are

20

omitting the centering), the influence function for a single average potential outcome, t ‚àà {0, 1}, is
given by, for z = (y, t, x0 )0 ,

œàt (z) =

1{T = t}(y ‚àí ¬µt (x))
P[T = t | X = x]

+ ¬µt (x).

(3.4)

Our estimation of œÑ , œÄ(s), and œÄ(s0 , s0 ) will utilize sample averages of this function, with unknown
objects replaced by estimators. Our use of influence functions here follows the recent literature in
econometrics showing that the double robustness implies valid inference under weaker conditions
on the first step nonparametric estimates (Farrell, 2015; Chernozhukov et al., 2018a).

3.2

Subpopulation Effect Parameters

The second type of causal effects of interest are based on potential outcomes averaged over only a
specific treatment group. A single such average, for t, t0 ‚àà {0, 1}, is denoted by
œÅt,t0 = E[Y (t) | T = t0 ].

(3.5)

Many interesting parameters are linear combinations of these for different t and t0 . We focus on
two for concreteness. (We could also consider averages restricted by targeting-type functions, as in
expected utility/profit, but for brevity we omit this.) The most well-studied of these parameters is
the treatment effect on the treated, given by

œÑ1,0 = E[Y (1) ‚àí Y (0) | T = 1] = œÅ1,1 ‚àí œÅ0,1 .

(3.6)

To appreciate the breadth of this framework, and the applicability of our causal inference results, we also consider a decomposition parameter, a semiparametric analogue of Oaxaca-Blinder
(Kitagawa, 1955; Oaxaca, 1973; Blinder, 1973). In this context, the ‚Äútreatment‚Äù variable T is typically not a treatment assignment per se, but rather an exogenous covariate such as a demographic
indicator, perhaps most commonly a male/female indicator. See Fortin et al. (2011) for a complete
discussion and further references. The parameter of interest in this case is the decomposition of
‚àÜ = E[Y (1) | T = 1] ‚àí E[Y (0) | T = 0], into the difference in the covariate distributions and the
difference in expected outcomes. These can be written as functions of different œÅt,t0 . For example,
21

‚àÜX = E[Y (1)|T = 1] ‚àí E[Y (1)|T = 0] = E[¬µ1 (X)|T = 1] ‚àí E[¬µ1 (X)|T = 0] = œÅ1,1 ‚àí œÅ1,0 . We are in
general interested in

‚àÜ = ‚àÜX + ‚àÜ¬µ ,

‚àÜX = œÅ1,1 ‚àí œÅ1,0 ,

and

‚àÜ¬µ = œÅ1,0 ‚àí œÅ0,0 .

(3.7)

Just as in the case of full-population averages, the influence function is known and leads to a
doubly robust estimator. For a single œÅt,t0 , the (uncentered) influence function is (cf. (3.4)):

œàt,t0 (z) =

P[T = t0 | X = x] 1{T = t}(y ‚àí ¬µt (x)) 1{T = t0 }¬µt (x)
+
.
P[T = t0 ]
P[T = t | X = x]
P[T = t0 ]

(3.8)

Estimation and inference requires, as above, estimation of the propensity scores and regression functions, depending on the exact choices of t and t0 , and here we also require the marginal probability
of treatments.

3.3

Optimal Policies

Moving beyond a fixed parameter, our results on deep neural networks can be used to address
optimal targeting. In the notation of Section 3.1, this amounts to finding a policy, say s? (x),
that maximizes a given measure of utility stemming from treatment, generally the expected gain
relative to a baseline policy. In Section 3.1 we considered the utility (or profit) difference between
two given strategies, a candidate s0 (x) and a baseline s0 (x). Instead of inference on œÄ(s0 , s0 ), we
can use the data to find the s? (x) which maximizes the gain relative to the baseline. This problem
has been widely studied in econometrics and statistics; for detailed discussion and numerous references see Manski (2004), Hirano and Porter (2009), Kitagawa and Tetenov (2018), and Athey and
Wager (2018). In particular, the latter noticed using the locally robust framework allows policy
optimization under nearly the same conditions as inference and proved fast convergence rates of
the estimated policy in terms of regret.
More formally, we want to find the optimal choice s? (x) in some policy/action space S. The
policy space, and thus its complexity, is user determined. Simple examples include simple decision trees or univariate-based strategies; more can be found in the references above. Recall that
œÄ(s0 , s0 ) = E[Y (s0 )] ‚àí E[Y (s0 )] = E[(s0 (X) ‚àí s0 (X))œÑ (X)], where œÑ (x) = E[Y (1) ‚àí Y (0) | X = x]

22

is the conditional average treatment effect. Given a space S, we wish to find the policy s? (x) ‚àà S
which solves maxs0 ‚ààS œÄ(s0 , s0 ). The main result of Athey and Wager (2018) is that replacing œÄ
with the doubly-robust œÄÃÇ of Equation (4.3), and minimizing the empirical analogue of regret, one
obtains an estimator sÃÇ(x) of the optimal policy that obeys the regret bound œÄ(s? , s0 ) ‚àí œÄ(sÃÇ, s0 ) =
p
OP ( VC(S)/n) (a formal statement would be notationally burdensome). The complexity of the
user-chosen policy space enters the bound through its VC dimension. Simple, interpretable policy
classes often have bounded or slowly-growing dimension, implying rapid convergence.

3.4

Other Estimands

There are of course many other contexts where first-step deep learning is useful. Only trivial
extensions to the above would be required for other causal effects, such as multi-valued treatments (reviewed by Cattaneo, 2010) and others with doubly-robust estimators (Sloczynski and
Wooldridge, 2018). Further, under selection on observables, treatment effects, missing data, measurement error, and data combination are equivalent, and thus all our results apply immediately
to those contexts. For reviews of these and Assumption 4 more broadly, see Chen et al. (2004);
Tsiatis (2006); Heckman and Vytlacil (2007); Imbens and Wooldridge (2009).
Moving beyond causal effects, any estimand with a locally/doubly robust estimator depending
only on target functions falling into our class of losses can be covered using the results of Section 2.
For example, estimands requiring distribution estimation require further study; see Liang (2018) for
recent results via Generative Adversarial Networks (GANs). More precisely, along with regularity
conditions, our theory can be used to verify the conditions of Chernozhukov et al. (2018c), who
treat more general semiparametric estimands using local robustness, sometimes relying on sample
splitting or cross fitting. Further in this vein, our results on deep neural networks can be used
to address optimal targeting, i.e., finding the policy, say s? (x), that maximizes a given measure
of utility, by applying the results of Athey and Wager (2018), who noticed that using the locally
robust framework allows policy optimization.
More broadly, the learning of features using deep neural networks is becoming increasingly
popular and our results speak to this context directly. To illustrate, consider the simple example
of a linear model where some predictors are features learned from independent data. Here, the
object of interest is the fixed-dimension coefficient vector Œª, which we assume can be partitioned
23

as Œª = (Œª01 , Œª02 )0 according to the model Y = f (X)0 Œª1 + W 0 Œª2 + Œµ. The features f (X), often a
‚Äúscore‚Äù of some type, are generally learned from auxiliary (and independent) data. For a recent
example, see Liu et al. (2017). In such cases, inference on Œª can proceed directly, as long as care
is taken to interpret the results. See Section 4.2.

4

Asymptotic Inference

We now turn to asymptotic inference for the causal parameters discussed above. We first define the
estimators, which are based on sample averages of the (uncentered) influence functions (3.4) and
(3.8). We then give a generic result for single averages which can then be combined for inference on
a given parameter of interest. Below we discuss inference under randomized treatment and using
sample splitting.
Throughout, we assume we have a sample {zi = (yi , ti , x0i )0 }ni=1 from Z = (Y, T, X 0 )0 . We then
form
œàÃÇt (zi ) =

1{ti = t}(yi ‚àí ¬µÃÇt (xi ))
PÃÇ[T = t | X = xi ]

+ ¬µÃÇt (xi ),

(4.1)

where PÃÇ[T = t | X = xi ] = pÃÇ(xi ) for t = 1 and 1 ‚àí pÃÇ(xi ) for t = 0, and similarly

œàÃÇt,t0 (zi ) =

PÃÇ[T = t0 | X = xi ] 1{ti = t}(yi ‚àí ¬µÃÇt (xi ))
PÃÇ[T = t0 ]

PÃÇ[T = t | X = xi ]

+

1{ti = t0 }¬µÃÇt (xi )
PÃÇ[T = t0 ]

,

(4.2)

where PÃÇ[T = t0 ] is simply the sample frequency En [1{ti = t0 }].
For the first stage estimates appearing in (4.1) and (4.2) we use our results on deep nets, and
Theorem 1 in particular. Specifically, the estimated propensity score, pÃÇ(x), is the estimate that
results from solving (2.4), with the MLP architecture, for the logistic loss (2.3) with T as the
outcome. Similarly, for each status t ‚àà {0, 1}, we can let ¬µÃÇt (x) be the deep-MLP estimate of
f‚àó (x) = E[Y |T = t, X = x], solving (2.4) for least squares loss, (2.2), with outcome Y , using
only observations with ti = t. However, it is worth noting that the theoretically-equivalent joint
estimation of Equation (5.1) performs much better, as the two groups may share features. To state
the results, let Œ≤p and Œ≤¬µ be the smoothness parameters of Assumption 2 for the propensity score
and outcome models, respectively.
We then obtain inference using the following results, essentially taken from Farrell (2015).
24

Similar results are given by Belloni et al. (2017) and Chernozhukov et al. (2018a). All of these
provide high-level conditions for valid inference, and none verify these for deep nets as we do here.
Theorem 3. Suppose that {zi = (yi , ti , x0i )0 }ni=1 are i.i.d. obeying Assumption 4 and the conditions
Theorem 1 hold with Œ≤p ‚àß Œ≤¬µ > d. Further assume that, for t ‚àà {0, 1}, E[(s(X)œàt (Z))2 |X] is
bounded away from zero and, for some Œ¥ > 0, E[(s(X)œàt (Z))4+Œ¥ |X] is bounded. Then the deep
MLP-ReLU network estimators defined above obey the following, for t ‚àà {0, 1},


(a) En [(pÃÇ(xi ) ‚àí p(xi ))2 ] = oP (1) and En (¬µÃÇt (xi ) ‚àí ¬µt (xi ))2 = oP (1),
(b) En [(¬µÃÇt (xi ) ‚àí ¬µt (xi ))2 ]1/2 En [(pÃÇ(xi ) ‚àí p(xi ))2 ]1/2 = oP (n‚àí1/2 ), and
(c) En [(¬µÃÇt (xi ) ‚àí ¬µt (xi ))(1 ‚àí 1{ti = t}/P[T = t|X = xi ])] = oP (n‚àí1/2 ),
and therefore, if pÃÇ(xi ) is bounded inside (0, 1), for a given s(x) and t ‚àà {0, 1}, we have
‚àö

h
i
nEn s(xi )œàÃÇt (zi ) ‚àí s(xi )œàt (zi ) = oP (1)

and

En [(s(xi )œàÃÇt (zi ))2 ]
= oP (1),
En [(s(xi )œàt (zi ))2 ]

h
i
‚àö
nEn œàÃÇt,t0 (zi ) ‚àí œàt,t0 (zi ) = oP (1)

and

En [œàÃÇt,t0 (zi )2 ]
= oP (1).
En [œàt,t0 (zi )2 ]

as well as,

This result, our main inference contribution, shows exactly how deep learning delivers valid
asymptotic inference for our parameters of interest. Theorem 1 (a generic result using Theorem 2
could be stated) proves that the nonparametric estimates converge sufficiently fast, as formalized
by conditions (a), (b), and (c), enabling feasible efficient semiparametric inference. In general, these
are implied by, but may be weaker than, the requirement of that the first step estimates converge
faster than n‚àí1/4 , which our results yield for deep ReLU nets. The first is a mild consistency
requirement. The second requires a rate, but on the product of the two estimates, which can be
satisfied under weaker conditions. Finally, the third condition is the strongest. Intuitively, this
condition arises from a ‚Äúleave-in‚Äù type remainder, and as such, it can be weakened using sample
splitting Chernozhukov et al. (2018a); Newey and Robins (2018). We opt to maintain (c) exactly
because deep nets are not amenable to either simple leave-one-out forms (as are, e.g., classical
kernel regression) or to sample splitting, being a data hungry method the gain in theoretically
weaker rate requirements may not be worth the price paid in constants in finite samples. Instead,
25

we employ our localization analysis, as was used to obtain the results of Section 2, to verify (c)
directly (see Lemma 10); this appears to be a novel application of localization, and this approach
may be useful in future applications of second-step inference using machine learning methods.
From this result we immediately obtain inference for all the causal parameters discussed above.
For the full-population averages, for example, we would form
h
i
œÑÃÇ = En œàÃÇ1 (zi ) ‚àí œàÃÇ0 (zi ) ,
h
i
œÄÃÇ(s) = En s(xi )œàÃÇ1 (zi ) + (1 ‚àí s(xi ))œàÃÇ0 (zi ) ,
h
i
œÄÃÇ(s0 , s0 ) = En [s0 (xi ) ‚àí s0 (xi )]œàÃÇ1 (zi ) ‚àí [s0 (xi ) ‚àí s0 (xi )]œàÃÇ0 (zi ) .

(4.3)

The estimator œÑÃÇ is exactly the doubly/locally robust estimator of the average treatment effect that
is standard in the literature. The estimators for profits can be thought of as the doubly robust
version of the constructs described in Hitsch and Misra (2018). Furthermore, to add a per-unit
cost of treatment/targeting c and a margin m, simply replace œà1 with mœà1 ‚àí c and œà0 with mœà0 .
ÀÜ X , and ‚àÜ
ÀÜ ¬µ would be linear combinations of different œÅÃÇt,t0 = En [œàÃÇt,t0 (zi )].
Similarly, œÑÃÇ1,0 , ‚àÜ
It is immediate from Theorem 3 that all such estimators are asymptotically Normal. The
asymptotic variance can be estimated by simply replacing the sample first moments of (4.3) with
second moments. That is, looking at œÄÃÇ(s) to fix ideas,
‚àö

d

nŒ£ÃÇ‚àí1/2 (œÄÃÇ(s) ‚àí œÄ(s)) ‚Üí N (0, 1),

with

Œ£ÃÇ = En



2 
s(xi )œàÃÇ1 (zi ) + (1 ‚àí s(xi ))œàÃÇ0 (zi )
‚àí œÄÃÇ(s)2 .

The others are similar. Further, Theorem 3 can be generalized straightforwardly to yield uniformly
valid inference, following the approach of Romano (2004), exactly as in Belloni et al. (2014) or
Farrell (2015).
Finally, we note that our focus with Theorem 3 is showcasing the practical utility of deep
learning. Our use of local/double robustness here is toward the aim of attaining feasible inference
without requiring more detailed assumptions on the machine learning step. This comes at the
expense of, for example, stronger-than-minimal smoothness assumptions. That is, the requirement
that Œ≤p ‚àß Œ≤¬µ > d is not minimal, and moreover, neither is the weaker condition Œ≤p ‚àß Œ≤¬µ > d/2 that
would be required after applying Corollary 1 instead of Theorem 1. Obtaining a Gaussian limit, and

26

possibly semiparametric efficiency, under minimal conditions has been studied by many, dating at
least to Bickel and Ritov (1988); see Robins et al. (2009) for recent results and references on optimal
estimation and minimal conditions. For causal inference, Chen et al. (2008) and Athey et al. (2018)
obtain semiparametric efficiency under strictly weaker conditions than ours on p(x) (the former
under minimal smoothness on ¬µt (x) and the latter under a sparsity in a high-dimensional linear
model). Further, as above, cross-fitting (Newey and Robins, 2018) paired with local robustness
may yield weaker smoothness conditions by providing underfitting‚Äù robustness (i.e. weakening biasrelated tuning parameter assumptions). On the other hand, weaker variance-related assumptions,
or ‚Äúoverfitting‚Äù robust inference procedures, (Cattaneo and Jansson, 2018; Cattaneo et al., 2018),
may also be possible following deep learning, but are less automatic at present. Finally, other
methods designed for causal inference under relaxed assumptions may be useful here, such as the
recently developed extensions to doubly robust estimation (Tan, 2018) and inverse weighting (Ma
and Wang, 2018): pursuing these in the context of deep learning is left to future work.

4.1

Inference Under Randomization

Our analysis thus far has focused on observational data, but it is worth spelling out results for randomized experiments. This is particularly important in the Internet age, where experimentation
is common, vast amounts of data are available, and effects are often small in magnitude (Taddy
et al., 2015). Indeed, our empirical illustration, detailed in the next section, stems from an experiment with 300,000 units and hundreds of covariates. When treatment is randomized, inference
can be done directly using the mean outcomes in the treatment and control groups, such as the
difference for the average treatment effect or the corresponding weighted sum for profit. However,
pre-treatment covariates can be used to increase efficiency (Hahn, 2004).
We will focus on the simple situation of a purely randomized binary treatment, but our results
can be extended naturally to other randomization schemes. We formalize this with the following.
Assumption 5 (Randomized Treatment). T is independent of Y (0), Y (1), and X, and is distributed Bernoulli with parameter p‚àó , such that pÃÑ ‚â§ p‚àó ‚â§ 1 ‚àí pÃÑ for some pÃÑ > 0.
Under this assumption, the obvious simplification is that the propensity score need not be
estimated using the covariates, but can be replaced with the (still nonparametric) sample frequency:
27

pÃÇ(xi ) ‚â° pÃÇ = En [ti ]. This is plugged into Equation (4.3) and estimation and inference proceeds as
above. Only rate conditions on the regression functions ¬µÃÇt (x) are needed. Further, conditions (a)
and (b) of Theorem 3 collapse, as pÃÇ is root-n consistent, leaving only condition (c) to be verified.
Again, cross-fitting can be used in theory to remove this condition and thus weaken the requirement
that Œ≤¬µ > d, but we maintain this for simplicity. We collect this into the following result, which is
a trivial corollary of Theorem 3.
Corollary 3. Let the conditions of Theorem 3 hold with Assumption 5 in place of Assumption 4
and only Œ≤¬µ > d. Then deep MLP-ReLU network estimators obey


(a0 ) En (¬µÃÇt (xi ) ‚àí ¬µt (xi ))2 = oP (1) and
(c0 ) En [(¬µÃÇt (xi ) ‚àí ¬µt (xi ))(1 ‚àí 1{ti = t}/p‚àó )] = oP (n‚àí1/2 )
and the conclusions of Theorem 3 hold.

4.2

Sample Splitting

Sample splitting may be used to obtain valid inference in cases, unlike those above, where the
parameter of interest itself is learned from the data. For the causal estimands above, the regression
functions and propensity score must be estimated, but these are nuisance functions. This is not
true in the inference after policy or feature learning (Sections 3.3 and 3.4). For policy learning,
our results can be used to verify the high-level conditions of Athey and Wager (2018), though they
require the additional condition of uniform consistency of the first stage estimators, and for machine
learning estimators this is not clearly innocuous. However, this gives only point estimation.
Sample splitting is used in the obvious way: the first subsample, or more generally, independent
auxiliary data, is used to learn the features or optimal policy, and then Theorem 3 is applied in
the second subsample, conditional on the results of the first. For policy learning this delivers valid
inference on œÄ(sÃÇ) or œÄ(sÃÇ, s0 ), while for the simple example of feature learning in a linear model we
obtain inference on the parameters defined by the ‚Äúmodel‚Äù Y = f (X)0 Œª1 + W 0 Œª2 + Œµ, where f (X)
is estimated from auxiliary data. Care must be taken in interpreting the results. The results of
the first-subsample estimation are effectively conditioned upon in the inference stage, redefining
the target parameter to be in terms of the learned object. In many contexts this may be sufficient

28

(Chernozhukov et al., 2018b), but further assumptions will generally be needed to assume that the
first subsample has recovered the true population object. To fix ideas, consider policy learning:
inference on œÄ(sÃÇ, s0 ), conditional on the map sÃÇ(x) learned in the first subsample, is immediate
and requires no additional assumptions, but inference on œÄ(s? , s0 ) is not obvious without further
conditions.

5

Empirical Application

To illustrate our results, Theorems 1 and 3 in particular, we study, from a marketing point of
view, a randomized experiment from a large US retailer of consumer products. The outcome of
interest is consumer spending and the treatment is a catalog mailing. The firm sells directly to
the customer (as opposed to via retailers) using a variety of channels such as the web and mail.
The data consists of nearly three hundred thousand (292,657) consumers chosen at random from
the retailer‚Äôs database. Of these, 2/3 were randomly chosen to receive a catalog, and in addition
to treatment status, we observe roughly one hundred fifty covariates, including demographics, past
purchase behaviors, interactions with the firm, and other relevant information. For more on the
data and a complete discussion of the decision making issues, we refer the reader to Hitsch and
Misra (2018) (we use the 2015 sample). That paper studied various estimators, both traditional
and modern, of average and heterogeneous causal effects. Importantly, they did not consider neural
networks. Our results show that deep nets are at least as good as (and sometimes better than) the
best methods in Hitsch and Misra (2018).
In terms of motivation, a key element of a firm‚Äôs toolkit is the design and implementation of
targeted marketing instruments. These instruments, aiming to induce demand, often contain advertising and informational content about the firms offerings. The targeting aspect thus boils down
to the selection of which particular customers should be sent the material. This is a particularly
important decision since the costs of creation and dissemination of the material can accumulate
rapidly, particularly over a large customer base. For a typical retailer engaging in direct marketing
the costs of sending out a catalog can be close to a dollar per targeted customer. With millions of
catalogs being sent out, the cost of a typical campaign is quite high.
Given these expenses, an important problem for firms is ascertaining the causal effects of such

29

targeted mailing, and then using these effects to evaluate potential targeting strategies. At a high
level, this approach is very similar to modern personalized medicine where treatments have to be
targeted. In these contexts, both the treatment and the targeting can be costly, and thus careful
assessment of œÄ(s) (interpreted as welfare) is crucial for decision making.
The outcome of interest for the firm is customer spending. This is the total amount of money
that a given customer spends on purchases of the firm‚Äôs products, within a specified time window.
For the experiment in question the firm used a window of three months, and aggregated sales from
all available purchase channels including phone, mail, and the web. In our data 6.2% of customers
made a purchase. Overall mean spending is $7.31; average spending conditional on buying is $117.7,
with a standard deviation of $132.44. The idea then is to examine the incremental effect that the
catalog had on this spending metric. Table 1 presents summary statistics for the outcome and
treatment. Figure 3 displays the complete density of spending conditional on a purchase, which is

2000
1500
1000
500
0

Frequency

2500

3000

quite skewed.

0

200

400

600

800

Spend conditional on purchase

Figure 3: Spend Conditional on Purchase

30

1000

Table 1: Summary Statistics

Purchase
Spend
Spend Conditional on Purchase
Treatment
Purchase | Treatment=1
Purchase | Treatment=0
Spend | Treatment=1
Spend | Treatment=0

5.1

Mean

SD

N

0.062
7.311
117.730
0.669
0.069
0.047
8.158
5.597

0.24
43.55
132.44
0.47
0.25
0.21
44.71
41.04

292657
292657
18174
292657
195821
96836
195821
96836

Implementation Details

We estimated deep neural nets under a variety of architecture choices. In what follows we present
eight examples and focus on one particular architecture to compute various statistics and tests to
illustrate the use of the theory developed above. All computation was done using TensorFlowTM .
For treatment effect and profit estimation we follow Equations (4.1) and (4.3). Because treatment is randomized, we apply Corollary 3, and thus, only require estimates of the regression
functions ¬µt (x) = E[Y (t)|X = x], t ‚àà {0, 1}. An important implementation detail, from a computation point of view (recall Remark 2) is that we will estimate ¬µ0 (x) and œÑ (x) (and thereby ¬µ1 (x))
jointly (results from separate estimation are available). To be precise, recalling Equations (2.2)
and (2.4), we solve
Ô£´

Ô£∂

n
2
X
¬µÃÇ0 (x)
1
Ô£¨
Ô£∑
yi ‚àí ¬µÃÉ0 (xi ) ‚àí œÑÃÉ (xi )ti
Ô£≠
Ô£∏ := arg min
2
¬µÃÉ0 ,œÑÃÉ
œÑÃÇ (x) = ¬µÃÇ1 (x) ‚àí ¬µÃÇ0 (x)
i=1

(5.1)

where the minimization is over the relevant network architecture. Recall that, in the context of
our empirical example yi is the customer‚Äôs spending, xi are her characteristics, and ti indicates
receipt of a catalog. In this format, ¬µ0 (xi ) reflects base spending and œÑ (x) = ¬µ1 (xi ) ‚àí ¬µ0 (xi )
is the conditional average treatment effect of the catalog mailing. In our application, this joint
estimation outperforms separately estimating each ¬µt (x) on the respective samples (though these
two approaches are equivalent theoretically).
The details of the eight deep net architectures are presented in Table 2. See Section 2.1 for an

31

introduction to the terminology and network construction. Most yielded similar results, both in
terms of fit and final estimates. A key measure of fit reported in the final column of the table is the
portion of œÑÃÇ (xi ) that were negative. As argued by Hitsch and Misra (2018), it is implausible under
standard marketing or economic theory that receipt of a catalog causes lower purchasing. On this
metric of fit, deep nets perform as well as, and sometimes better than, the best methods found
by Hitsch and Misra (2018): Causal KNN with Treatment Effect Projections (detailed therein) or
Causal Forests (Wager and Athey, 2018). Figure 4 shows the distribution of œÑÃÇ (xi ) across customers
for each of the eight architectures. While there are differences in the shapes of the densities, the
mean and variance estimates are nonetheless quite similar.
Table 2: Deep Network Architectures

Architecture

Learning
Rate

Widths
[H1 , H2 , ...]

Dropout
[H1 , H2 , ...]

Total
Parameters

Validation
Loss

Training
Loss

Pn [œÑÃÇ (xi ) < 0]

1
2
3
4
5
6
7
8

0.0003
0.0003
0.0001
0.0009
0.0003
0.0003
0.0003
0.00005

[60]
[100]
[30, 20]
[30, 10]
[30, 30]
[30, 30]
[100, 30, 20]
[80, 30, 20]

[0.5]
[0.5]
[0.5, 0]
[0.3, 0.1]
[0, 0]
[0.5, 0]
[0.5, 0.5, 0]
[0.5, 0.5, 0]

8702
14502
4952
4622
5282
5282
17992
14532

1405.62
1406.48
1408.22
1408.56
1403.57
1408.57
1408.62
1413.70

1748.91
1751.87
1751.20
1751.62
1738.59
1755.28
1751.52
1756.93

0.0014
0.0251
0.0072
0.0138
0.0226
0.0066
0.0103
0.0002

Notes: All networks use the ReLU activation function. The width of each layer is shown, e.g. Architecture 3 consists
of two layers, with 30 and 20 hidden units respectively. The final column shows the portion of estimated individual
treatment effects below zero.

5.2

Results

We present now results for treatment effects, utility/profits, and targeting policy evaluations. Table
3 shows the estimates of the average treatment effect from the eight network architectures along with
their respective 95% confidence intervals. These results are constructed following Section 4, using
Equations (4.1) and (4.3) in particular, and valid by Corollary 3. Because this is an experiment,
we can compare to the standard unadjusted difference in means, which yields an average treatment
effect of 2.561.


Turning to expected profits, we estimate œÄ(s) = E s(X)(mY (1) ‚àí c) + (1 ‚àí s(X)) mY (0) ,

32

1.0
0.8
0.6
0.4
0.0

0.2

Density

‚àí5

0

5

10

15

20

Conditional Average Treatment Effect

Figure 4: Conditional Average Treatment Effects Across Architectures
Table 3: Average Treatment Effect Estimates and 95% Confidence Intervals

Architecture
1
2
3
4
5
6
7
8

Average Treatment
Effect (œÑÃÇ )
2.606
2.577
2.547
2.488
2.459
2.430
2.400
2.371

95% Confidence
Interval
[2.273 , 2.932]
[2.252 , 2.901]
[2.223 , 2.872]
[2.160 , 2.817]
[2.127 , 2.791]
[2.093 , 2.767]
[2.057 , 2.744]
[2.021 , 2.721]

adding a profit margin m and a mailing cost c to (3.2) (our NDA with the firm forbids revealing
m and c). We consider three different counterfactual policies s(x): (i) never treat, s(x) ‚â° 0; (ii) a
blanket treatment, s(x) ‚â° 1; (iii) a loyalty policy, s(xi ) = 1 only for those who had purchased in
the prior calendar year. Results are shown in Table 4. It is clear that profits from the three policies
are ordered as œÄ(never) < œÄ(blanket) < œÄ(loyalty).
For both the average effects of Table 3 and the counterfactuals of Table 4 there is broad agree-

33

Table 4: Counterfactual Profits from Three Targeting Strategies

Architecture
1
2
3
4
5
6
7
8

Never Treat
œÄÃÇ(s)
95% CI
2.016 [1.923 , 2.110]
2.022 [1.929 , 2.114]
2.027 [1.934 , 2.120]
2.037 [1.944 , 2.130]
2.043 [1.950 , 2.136]
2.048 [1.954 , 2.142]
2.053 [1.959 , 2.148]
2.059 [1.963 , 2.154]

Blanket Treatment
œÄÃÇ(s)
95% CI
2.234 [2.162 , 2.306]
2.229 [2.157 , 2.301]
2.224 [2.152 , 2.296]
2.213 [2.140 , 2.286]
2.208 [2.135 , 2.281]
2.202 [2.128 , 2.277]
2.197 [2.122 , 2.272]
2.192 [2.116 , 2.268]

Loyalty Policy
œÄÃÇ(s)
95% CI
2.367 [2.292 , 2.443]
2.363 [2.288 , 2.438]
2.358 [2.283 , 2.434]
2.350 [2.274 , 2.425]
2.345 [2.269 , 2.422]
2.341 [2.263 , 2.418]
2.336 [2.258 , 2.414]
2.332 [2.253 , 2.411]

ment among the eight architectures both numerically and substantially. This may be due to the
fact that the data is experimental, so that the propensity score is constant. In true observational
data this may not be the case. We explore this issue in our Monte Carlo analysis below.

5.2.1

Placebo Experiment

We conducted a set of placebo tests to examine whether the deep neural networks we use can truly
recover causal effects. In particular, we take only the untreated customers in the data and randomly
assign half to treated status.2 We then ran the eight architectures of Table 2, as in the true data.
The conditional average treatment effects across the architectures are plotted in Figure 5. We see
that the ‚Äútrue‚Äù zero average effect is recovered precisely and with the expected distribution. The
average treatment effect across all models is estimated to be around -0.024, compared to 2.56 in the
original data. Exercises with different proportions of (placebo) treated customers revealed similar
results.

5.2.2

Optimal Targeting

To explore further, we focus on architecture #3 and study subpopulation treatment targeting
strategies following the ideas of Section 3.3. (The other architectures yield similar results, so
we omit them.) Architecture #3 has depth L = 2 with widths H1 = 30 and H2 = 20. The
learning rate was set at 0.0001 and the specification had a total of 4,952 parameters. For this
architecture, recalling Remark 1, we added dropout for the second layer with a fixed probability
2

We thank Guido Imbens suggesting this analysis.

34

15
10
0

5

Density

‚àí20

‚àí10

0
Conditional Average Treatment Effect

Figure 5: Placebo Test

35

10

20

0.25
0.20
0.15
0.10
0.05
‚àí0.05

0.00

Profit Difference

0

200

400

600

800

1000

1200

Spend

Figure 6: Expected Profits from Threshold Targeting Based on Prior Year Spend
of 1/2. Using this architecture, we compare the blanket strategy (so s0 (x) = 1) to targeting
customers with spend of at least yÃÑ dollars in the prior calendar year (prior spending is one of the
covariates), in $50 increments to $1200. The policy class is therefore S = {s(x) = 1(prior spend >
yÃÑ), yÃÑ = 0, 50, 100, . . . , 1150, 1200}. Figure 6 presents the results. The black dots show the difference

œÄÃÇ(spend > yÃÑ) ‚àí œÄÃÇ(blanket) and the shaded region gives a pointwise 95% confidence band (to ease
presentation, sample splitting is not used). We see that there is a significant difference between
various choices of yÃÑ. Initially, targeting customers with higher spend yields higher profits, as would
be expected, but this effect diminishes beyond a certain yÃÑ, roughly $500, as fewer and fewer are
targeted. The optimal policy estimate is sÃÇ(x) = 1(prior spend > 400). In general, simpler policy
classes may yield better decisions, but it is certainly possible to expand our search to different S
by considering further covariates and/or transformations.

36

6

Monte Carlo Analysis

We conducted a set of Monte Carlo experiments to evaluate our theoretical results. We study
inference on the average treatment effect, œÑ of (3.1), under different data generating processes
(DGPs). In each DGP we take n = 10, 000 i.i.d. samples and use 1,000 replications. For either
d = 20 or 100, X includes a constant term and d independent uniform random variables, U(0, 1).
Treatment assignment is Bernoulli with probability p(x), where p(x) is the propensity score. We
consider both (i) randomized treatments with p(x) = 0.5 and (ii) observational data with p(x) =
(1 + exp(‚àíŒ±0p x))‚àí1 , where Œ±p,1 = 0.09 and the remainder are drawn once as U(‚àí0.55, 0.55), and
then fixed for the replications. For d = 100, we maintain kŒ±p k0 = 20 for the simplicity. These
generate propensities with an approximate range of approximately (0.30, 0.75) and mean roughly
0.5.
Given covariates and treatment assignment, the outcomes are generated according to

yi = ¬µ0 (xi ) + œÑ (xi )ti + Œµi ,

¬µ0 (x) = Œ±0¬µ x + Œ≤¬µ0 œï(x),

œÑ (xi ) = Œ±0œÑ x + Œ≤œÑ0 œï(x),

where Œµi ‚àº N (0, 1) and œï(x) are second-degree polynomials including pairwise interactions. For
¬µ0 (x) and œÑ (x) we consider two cases, linear and nonlinear models. In both cases the intercepts
are Œ±¬µ,1 = 0.09 and Œ±œÑ,1 = ‚àí0.05 and slopes are drawn (once) as Œ±¬µ,k ‚àº N (0.3, 0.7) and Œ±œÑ,k ‚àº
U(0.1, 0.22), k = 2, . . . , d + 1. The linear models set Œ≤¬µ = Œ≤œÑ = 0 while the nonlinear models take
Œ≤¬µ,k ‚àº N (0.01, 0.3) and Œ≤œÑ,k ‚àº U(‚àí0.05, 0.06). Altogether, this yields eight designs: d = 20 or 100,
p(x) constant or not, and outcome models linear or nonlinear.
For each design, we consider a variety of network architectures, all ReLU-based MLPs. These
architectures are variants of the ones used in the empirical application (which were customized for
the application). All networks vary in their depth and width, as spelled out in Table 5.
Tables 6 and 7 show the results for all eight DGPs. Table 6 shows randomized treatment while
Table 7 shows results mimicking observational data. Overall, the results reported show excellent
performance of deep learning based semiparametric inference. The bias is minimal and the coverage
is quite accurate, while the interval length is under control. Notice that the most architectures
yield similar results with no architecture dominating the others. Further, the coverage and interval

37

Table 5: Monte Carlo Architectures Explored
Architecture
1
2
3
4
5
6
7
8
9

Structure
{20, 15, 5}
{60, 30, 20}
{80, 80, 80}
{20, 15, 10, 5}
{60, 30, 20, 10}
{80, 80, 80, 80}
{20, 15, 15, 10, 10, 5}
{60, 30, 20, 20, 10, 5}
{80, 80, 80, 80, 80, 80}

length are fairly similar with the more complex architecture not exhibiting any systematic patterns
of length inflation.
None of the architectures we presented earlier used regularization. In typical empirical applications, including our own, researchers adopt architectures that employ dropout, a common method
of regularization; see Remark 1. Our own preliminary exploration of dropout and other forms of
regularization found expected departures form nonregularized models. In most, but not all, cases
the coverage remained accurate, but with increased bias and interval length compared to Table 6
and 7. The results preach caution when applying regularization in applications.

7

Conclusion

The utility of deep learning in social science applications is still a subject of interest and debate.
While there is an acknowledgment of its predictive power, there has been limited adoption of deep
learning in social sciences such as economics. Some part of the reluctance to adopting these methods
stems from the lack of theory facilitating use and interpretation. We have shown, both theoretically
as well as empirically, that these methods can offer excellent performance.
In this paper, we have given a formal proof that inference can be valid after using deep learning
methods for first-step estimation. To the best of our knowledge, ours is the first inference result
using deep nets. Our results thus contribute directly to the recent explosion in both theoretical
and applied research using machine learning methods in economics, and to the recent adoption of
deep learning in empirical settings. We obtained novel bounds for deep neural networks, speaking
directly to the modern (and empirically successful) practice of using fully-connected feedfoward
38

Table 6: Simulations Results - Constant Propensity Score

Model

Linear

Nonlinear

Architecture

20 Covariates

100 Covariates

Bias

IL

Coverage

Bias

IL

Coverage

1

0.00027

0.079

0.947

0.00067

0.080

0.946

2

-0.00032

0.079

0.951

0.00012

0.080

0.958

3

-0.00025

0.079

0.955

-0.00167

0.080

0.939

4

-0.00068

0.079

0.949

0.00038

0.080

0.949

5

0.00008

0.079

0.945

-0.00219

0.080

0.929

6

0.00007

0.079

0.955

-0.00010

0.080

0.946

7

0.00128

0.079

0.952

-0.00041

0.080

0.944

8

0.00108

0.079

0.949

-0.00088

0.080

0.941

9

0.00021

0.078

0.948

-0.00080

0.081

0.953

1

0.00087

0.081

0.946

-0.00067

0.163

0.940

2

0.00015

0.079

0.954

0.00093

0.153

0.927

3

-0.00072

0.079

0.940

0.00245

0.148

0.926

4

0.00101

0.080

0.945

-0.00087

0.165

0.956

5

0.00027

0.079

0.935

-0.00190

0.154

0.923

6

-0.00025

0.079

0.929

-0.00117

0.146

0.902

7

-0.00052

0.080

0.947

0.00091

0.165

0.941

8

0.00077

0.079

0.938

0.00201

0.153

0.927

9

-0.00013

0.079

0.940

0.00049

0.154

0.936

networks. Our results allow for different network architectures, including fixed width, very deep
networks. Our results cover general nonparametric regression-type loss functions, covering most
nonparametric practice. We used our bounds to deliver fast convergence rates allowing for secondstage inference on a finite-dimensional parameter of interest.
There are practical implications of the theory presented in this paper. We focused on semiparametric causal effects as a concrete illustration, but deep learning is a potentially valuable tool in
many diverse economic settings. Our results allow researchers to embed deep learning into standard econometric models such as linear regressions, generalized linear models, and other forms of
limited dependent variables models (e.g. censored regression). Our theory can also be used as a
starting point for constructing deep learning implementations of two-step estimators in the context
of selection models, dynamic discrete choice, and the estimation of games.
39

Table 7: Simulations Results - Non-constant Propensity Score

Model

Linear

Linear

Architecture

20 Covariates

100 Covariates

Bias

IL

Coverage

Bias

IL

Coverage

1

-0.00202

0.080

0.948

0.0009

0.081

0.955

2

0.00011

0.079

0.946

0.0007

0.081

0.945

3

-0.00130

0.079

0.964

-0.0001

0.081

0.937

4

-0.00106

0.079

0.945

0.0002

0.081

0.933

5

-0.00083

0.079

0.951

-0.0004

0.081

0.944

6

-0.00068

0.079

0.955

0.0001

0.081

0.924

7

-0.00119

0.079

0.953

-0.0001

0.081

0.942

8

-0.00056

0.079

0.952

-0.0008

0.081

0.939

9

-0.00096

0.079

0.948

-0.0007

0.081

0.952

1

-0.00076

0.081

0.946

-0.00279

0.164

0.937

2

-0.00122

0.080

0.939

0.00020

0.155

0.941

3

-0.00074

0.080

0.926

-0.00080

0.148

0.914

4

-0.00171

0.081

0.940

-0.00184

0.166

0.938

5

-0.00135

0.080

0.952

-0.00103

0.154

0.912

6

-0.00075

0.080

0.950

-0.00174

0.147

0.905

7

-0.00153

0.081

0.928

-0.00377

0.165

0.929

8

0.00082

0.080

0.953

0.00031

0.154

0.919

9

-0.00127

0.080

0.931

-0.00094

0.156

0.917

To be clear, we see our paper as a first step in the exploration of deep learning as a tool for
economic applications. There are a number of opportunities, questions, and challenges that remain.
For example, factor models in finance might benefit from the use of auto-encoders and recurrent
neural nets may have applications in time series. For some estimands, it may be crucial to estimate
the density as well, and this problem can be challenging in high dimensions. Deep nets, in the
form of GANs are a promising tool for distribution estimation. There are also interesting questions
remaining as to an optimal network architecture, and if this can be itself learned from the data,
as well as computational and optimization guidance. Research into these further applications and
structures is underway.

40

8

References

Abadie, A. and M. D. Cattaneo (2018): ‚ÄúEconometric Methods for Program Evaluation,‚Äù
Annual Review of Economics, 10, 465‚Äì503.
Anthony, M. and P. L. Bartlett (1999): Neural Network Learning: Theoretical Foundations,
Campbridge University Press.
Athey, S., G. Imbens, T. Pham, and S. Wager (2017): ‚ÄúEstimating average treatment effects: Supplementary analyses and remaining challenges,‚Äù American Economic Review: Papers
& Proceeding, 107, 278‚Äì81.
Athey, S., G. W. Imbens, and S. Wager (2018): ‚ÄúApproximate residual balancing: debiased
inference of average treatment effects in high dimensions,‚Äù Journal of the Royal Statistical Society,
Series B, 80, 597‚Äì623.
Athey, S. and S. Wager (2018): ‚ÄúEfficient Policy Learning,‚Äù arXiv preprint arXiv:1702.02896.
Barron, A. R. (1993): ‚ÄúUniversal approximation bounds for superpositions of a sigmoidal function,‚Äù IEEE Transactions on Information theory, 39, 930‚Äì945.
Bartlett, P. L., O. Bousquet, and S. Mendelson (2005): ‚ÄúLocal rademacher complexities,‚Äù
The Annals of Statistics, 33, 1497‚Äì1537.
Bartlett, P. L., N. Harvey, C. Liaw, and A. Mehrabian (2017): ‚ÄúNearly-tight VCdimension bounds for piecewise linear neural networks,‚Äù in Proceedings of the 22nd Annual
Conference on Learning Theory (COLT 2017).
Bauer, B. and M. Kohler (2017): ‚ÄúOn Deep Learning as a remedy for the curse of dimensionality
in nonparametric regression,‚Äù Tech. rep., Technical report.
Belloni, A., D. Chen, V. Chernozhukov, and C. Hansen (2012): ‚ÄúSparse models and
methods for optimal instruments with an application to eminent domain,‚Äù Econometrica, 80,
2369‚Äì2429.
Belloni, A., V. Chernozhukov, D. Chetverikov, C. Hansen, and K. Kato (2018): ‚ÄúHighDimensional Econometrics and Generalized GMM,‚Äù arXiv preprint arXiv:1806.01888.
Belloni, A., V. Chernozhukov, I. FernaÃÅndez-Val, and C. Hansen (2017): ‚ÄúProgram
Evaluation and Causal Inference With High-Dimensional Data,‚Äù Econometrica, 85, 233‚Äì298.
Belloni, A., V. Chernozhukov, and C. Hansen (2014): ‚ÄúInference on Treatment Effects after
Selection Amongst High-Dimensional Controls,‚Äù Review of Economic Studies, 81, 608‚Äì650.
Belloni, A., V. Chernozhukov, and L. Wang (2011): ‚ÄúSquare-root lasso: pivotal recovery of
sparse signals via conic programming,‚Äù Biometrika, 98, 791‚Äì806.
Bickel, P. J. and Y. Ritov (1988): ‚ÄúEstimating Integrated Squared Density Derivatives: Sharp
Best Order of Convergence Estimates,‚Äù SankhyaÃÑ, 50, 381‚Äì393.
Bickel, P. J., Y. Ritov, and A. B. Tsybakov (2009): ‚ÄúSimultaneous Analysis of LASSO and
Dantzig Selector,‚Äù The Annals of Statistics, 37, 1705‚Äì1732.
41

Blinder, A. (1973): ‚ÄúWage Discrimination: Reduced Form and Structural Estimates,‚Äù Journal
of Human Resources, 8, 436‚Äì455.
Cattaneo, M. D. (2010): ‚ÄúEfficient Semiparametric Estimation of Multi-valued Treatment Effects
under Ignorability,‚Äù Journal of Econometrics, 155, 138‚Äì154.
Cattaneo, M. D. and M. Jansson (2018): ‚ÄúKernel-Based Semiparametric Estimators: Small
Bandwidth Asymptotics and Bootstrap Consistency,‚Äù Econometrica, 86, 955‚Äì995.
Cattaneo, M. D., M. Jansson, and X. Ma (2018): ‚ÄúTwo-step Estimation and Inference with
Possibly Many Included Covariates,‚Äù arXiv:1807.10100, Review of Economic Studies, forthcoming.
Chen, X. (2007): ‚ÄúLarge Sample Sieve Estimation of Semi-Nonparametric Models,‚Äù in Handbook of
Econometrics, ed. by J. Heckman and E. Leamer, Elsevier, vol. 6B of Handbook of Econometrics,
chap. 76.
Chen, X., H. Hong, and A. Tarozzi (2004): ‚ÄúSemiparametric Efficiency in GMM Models of
Nonclassical Measurament Errors, Missing Data and Treatment Effects,‚Äù Cowles Foundation
Discussion Paper No. 1644.
‚Äî‚Äî‚Äî (2008): ‚ÄúSemiparametric Efficiency in GMM Models With Auxiliary Data,‚Äù The Annals of
Statistics, 36, 808‚Äì843.
Chen, X. and X. Shen (1998): ‚ÄúSieve extremum estimates for weakly dependent data,‚Äù Econometrica, 66, 289‚Äì314.
Chen, X. and H. White (1999): ‚ÄúImproved rates and asymptotic normality for nonparametric
neural network estimators,‚Äù IEEE Transactions on Information Theory, 45, 682‚Äì691.
Chernozhukov, V., D. Chetverikov, M. Demirer, E. Duflo, C. Hansen, W. Newey,
and J. Robins (2018a): ‚ÄúDouble/debiased machine learning for treatment and structural parameters,‚Äù The Econometrics Journal, 21, C1‚ÄìC68.
Chernozhukov, V., M. Demirer, E. Duflo, and I. Fernandez-Val (2018b): ‚ÄúGeneric
Machine Learning Inference on Heterogenous Treatment Effects in Randomized Experiments,‚Äù
arXiv preprint arXiv:1712.04802.
Chernozhukov, V., J. C. Escanciano, H. Ichimura, W. K. Newey, and J. M. Robins
(2018c): ‚ÄúLocally Robust Semiparametric Estimation,‚Äù arXiv:1608.00033.
Daniely, A. (2017): ‚ÄúDepth separation for neural networks,‚Äù arXiv preprint arXiv:1702.08489.
Farrell, M. H. (2015): ‚ÄúRobust Inference on Average Treatment Effects with Possibly More
Covariates than Observations,‚Äù arXiv:1309.4686, Journal of Econometrics, 189, 1‚Äì23.
Fortin, N., T. Lemieux, and S. Firpo (2011): ‚ÄúDecomposition Methods in Economics,‚Äù vol. 4
of Handbook of Labor Economics, 1‚Äì102.
Gine, E. and R. Nickl (2016): Mathematical Foundations of Infinite-Dimensional Models, Cambridge.
Goodfellow, I., Y. Bengio, and A. Courville (2016): Deep learning, Cambridge: MIT Press.
42

Hahn, J. (1998): ‚ÄúOn the Role of the Propensity Score in Efficient Semiparametric Estimation of
Average Treatment Effects,‚Äù Econometrica, 66, 315‚Äì331.
‚Äî‚Äî‚Äî (2004): ‚ÄúFunctional restriction and efficiency in causal inference,‚Äù Review of Economics and
Statistics, 84, 73‚Äì76.
Hanin, B. (2017): ‚ÄúUniversal function approximation by deep neural nets with bounded width
and relu activations,‚Äù arXiv preprint arXiv:1708.02691.
Hansen, C., D. Kozbur, and S. Misra (2017): ‚ÄúTargeted Undersmoothing,‚Äù arXiv:1706.07328.
Hartford, J., G. Lewis, K. Leyton-Brown, and M. Taddy (2017): ‚ÄúDeep iv: A flexible
approach for counterfactual prediction,‚Äù in International Conference on Machine Learning, 1414‚Äì
1423.
Hastie, T., R. Tibshirani, and J. Friedman (2009): The elements of statistical learning,
Springer Series in Statistics, New York: Springer-Verlag.
He, K., X. Zhang, S. Ren, and J. Sun (2016): ‚ÄúIdentity mappings in deep residual networks,‚Äù
in European conference on computer vision, Springer, 630‚Äì645.
Heckman, J. and E. J. Vytlacil (2007): ‚ÄúEconometric Evaluation of Social Programs, Part
I,‚Äù in Handbook of Econometrics, vol. VIB, ed. by J. Heckman and E. Leamer, Elsevier Science
B.V., 4780‚Äì4874.
Hirano, K. and J. Porter (2009): ‚ÄúAsymptotics for statistical treatment rules,‚Äù Econometrica,
77, 1683‚Äì1701.
Hitsch, G. J. and S. Misra (2018): ‚ÄúHeterogeneous Treatment Effects and Optimal Targeting
Policy Evaluation,‚Äù SSRN preprint 3111957.
Hornik, K., M. Stinchcombe, and H. White (1989): ‚ÄúMultilayer feedforward networks are
universal approximators,‚Äù Neural networks, 2, 359‚Äì366.
Imbens, G. W. and D. B. Rubin (2015): Causal Inference in Statistics, Social, and Biomedical
Sciences, Cambridge University Press.
Imbens, G. W. and J. M. Wooldridge (2009): ‚ÄúRecent Developments in the Econometrics of
Program Evaluation,‚Äù Journal of Economic Literature, 47, 5‚Äì86.
Javanmard, A. and A. Montanari (2014): ‚ÄúConfidence intervals and hypothesis testing for
high-dimensional regression,‚Äù The Journal of Machine Learning Research, 15, 2869‚Äì2909.
Johansson, F., U. Shalit, and D. Sontag (2016): ‚ÄúLearning representations for counterfactual
inference,‚Äù in International Conference on Machine Learning, 3020‚Äì3029.
Kingma, D. P. and J. Ba (2014): ‚ÄúAdam: A method for stochastic optimization,‚Äù arXiv preprint
arXiv:1412.6980.
Kitagawa, E. M. (1955): ‚ÄúComponents of a Difference Between Two Rates,‚Äù Journal of the
American Statistical Association, 50, 1168‚Äì1194.
Kitagawa, T. and A. Tetenov (2018): ‚ÄúWho should be treated? empirical welfare maximization
methods for treatment choice,‚Äù Econometrica, 86, 591‚Äì616.
43

Koltchinskii, V. (2006): ‚ÄúLocal Rademacher complexities and oracle inequalities in risk minimization,‚Äù The Annals of Statistics, 34, 2593‚Äì2656.
‚Äî‚Äî‚Äî (2011): Oracle Inequalities in Empirical Risk Minimization and Sparse Recovery Problems,
Springer-Verlag.
Koltchinskii, V. and D. Panchenko (2000): ‚ÄúRademacher processes and bounding the risk of
function learning,‚Äù in High dimensional probability II, Springer, 443‚Äì457.
Krizhevsky, A., I. Sutskever, and G. E. Hinton (2012): ‚ÄúImagenet classification with deep
convolutional neural networks,‚Äù in Advances in neural information processing systems, 1097‚Äì
1105.
LeCun, Y., L. Bottou, Y. Bengio, and P. Haffner (1998): ‚ÄúGradient-based learning applied
to document recognition,‚Äù Proceedings of the IEEE, 86, 2278‚Äì2324.
Liang, T. (2018): ‚ÄúOn How Well Generative Adversarial Networks Learn Densities: Nonparametric and Parametric Results,‚Äù arXiv:1811.03179.
Liang, T., A. Rakhlin, and K. Sridharan (2015): ‚ÄúLearning with square loss: Localization
through offset Rademacher complexity,‚Äù in Conference on Learning Theory, 1260‚Äì1285.
Liu, X., D. Lee, and K. Srinivasan (2017): ‚ÄúLarge scale cross category analysis of consumer
review content on sales conversion leveraging deep learning,‚Äù working paper, NYU Stern.
Ma, X. and J. Wang (2018): ‚ÄúRobust Inference Using Inverse Probability Weighting,‚Äù arXiv
preprint arXiv:1810.11397.
Makovoz, Y. (1996): ‚ÄúRandom approximants and neural networks,‚Äù Journal of Approximation
Theory, 85, 98‚Äì109.
Manski, C. F. (2004): ‚ÄúStatistical treatment rules for heterogeneous populations,‚Äù Econometrica,
72, 1221‚Äì1246.
Mendelson, S. (2003): ‚ÄúA few notes on statistical learning theory,‚Äù in Advanced lectures on
machine learning, Springer, 1‚Äì40.
‚Äî‚Äî‚Äî (2014): ‚ÄúLearning without concentration,‚Äù in Conference on Learning Theory, 25‚Äì39.
Mhaskar, H. and T. Poggio (2016a): ‚ÄúDeep vs. shallow networks: An approximation theory
perspective,‚Äù arXiv preprint arXiv:1608.03287.
Mhaskar, H. N. and T. Poggio (2016b): ‚ÄúDeep vs. shallow networks: An approximation theory
perspective,‚Äù Analysis and Applications, 14, 829‚Äì848.
Nair, V. and G. E. Hinton (2010): ‚ÄúRectified linear units improve restricted boltzmann machines,‚Äù in Proceedings of the 27th international conference on machine learning (ICML-10),
807‚Äì814.
Newey, W. K. and J. M. Robins (2018): ‚ÄúCross-fitting and fast remainder rates for semiparametric estimation,‚Äù arXiv preprint arXiv:1801.09138.
Oaxaca, R. (1973): ‚ÄúMale-Female Wage Differentials in Urban Labor Markets,‚Äù International
Economic Review, 14, 693‚Äì709.
44

Pisier, G. (1981): ‚ÄúRemarques sur un reÃÅsultat non publieÃÅ de B. Maurey,‚Äù in SeÃÅminaire Analyse
fonctionnelle (dit‚Äù Maurey-Schwartz‚Äù), 1‚Äì12.
Polson, N. and V. Rockova (2018): ‚ÄúPosterior Concentration for Sparse Deep Learning,‚Äù arXiv
preprint arXiv:1803.09138.
Raghu, M., B. Poole, J. Kleinberg, S. Ganguli, and J. Sohl-Dickstein (2017): ‚ÄúOn the
Expressive Power of Deep Neural Networks,‚Äù in Proceedings of the 34th International Conference
on Machine Learning, ed. by D. Precup and Y. W. Teh, International Convention Centre, Sydney,
Australia: PMLR, vol. 70 of Proceedings of Machine Learning Research, 2847‚Äì2854.
Robins, J., L. Li, R. Mukherjee, E. Tchetgen, and A. van der Vaart (2017): ‚ÄúMinimax
Estimation of a Functional on a Structured High-Dimensional Model,‚Äù The Annals of Statistics,
45, 1951‚Äì1987.
Robins, J., L. Li, E. Tchetgen, and A. van der Vaart (2008): ‚ÄúHigher order influence
functions and minimax estimation of nonlinear functionals,‚Äù in Probability and Statistics: Essays
in Honor of David A. Freedman, ed. by D. Nolan and T. Speed, Beachwood, Ohio, USA: Institute
of Mathematical Statistics, vol. 2.
Robins, J., E. T. Tchetgen, L. Li, and A. van der Vaart (2009): ‚ÄúSemiparametric Minimax
Rates,‚Äù Electronic Journal of Statistics, 3, 1305‚Äì1321.
Robins, J. M., A. Rotnitzky, and L. Zhao (1994): ‚ÄúEstimation of Regression Coefficients
When Some Regressors Are Not Always Observed,‚Äù Journal of the American Statistical Association, 89, 846‚Äì866.
‚Äî‚Äî‚Äî (1995): ‚ÄúAnalysis of Semiparametric Regression Models for Repeated Outcomes in the
Presence of Missing Data,‚Äù Journal of the American Statistical Association, 90, 846‚Äì866.
Romano, J. P. (2004): ‚ÄúOn non-parametric testing, the uniform behaviour of the t-test, and
related problems,‚Äù Scandinavian Journal of Statistics, 31, 567‚Äì584.
Safran, I. and O. Shamir (2016): ‚ÄúDepth separation in relu networks for approximating smooth
non-linear functions,‚Äù arXiv preprint arXiv:1610.09887.
Schmidt-Hieber, J. (2017): ‚ÄúNonparametric regression using deep neural networks with ReLU
activation function,‚Äù arXiv preprint arXiv:1708.06633.
Shalit, U., F. D. Johansson, and D. Sontag (2017): ‚ÄúEstimating individual treatment effect:
generalization bounds and algorithms,‚Äù arXiv preprint arXiv:1606.03976.
Sloczynski, T. and J. M. Wooldridge (2018): ‚ÄúA General Double Robustness Result for
Estimating Average Treatment Effects,‚Äù Econometric Theory, 34, 112‚Äì133.
Stone, C. J. (1982): ‚ÄúOptimal global rates of convergence for nonparametric regression,‚Äù The
annals of statistics, 1040‚Äì1053.
Taddy, M., M. Gardner, L. Chen, and D. Draper (2015): ‚ÄúA nonparametric Bayesian
analysis of heterogeneous treatment effects in digital experimentation,‚Äù Arxiv preprint
arXiv:1412.8563.
Tan, Z. (2018): ‚ÄúModel-assisted inference for treatment effects using regularized calibrated estimation with high-dimensional data,‚Äù arXiv preprint arXiv:1801.09817.
45

Telgarsky, M. (2016): ‚ÄúBenefits of depth in neural networks,‚Äù arXiv preprint arXiv:1602.04485.
Tsiatis, A. A. (2006): Semiparametric Theory and Missing Data, New York: Springer.
van de Geer, S., P. Buhlmann, Y. Ritov, and R. Dezeure (2014): ‚ÄúOn Asymptotically
Optimal Confidence Regions and Tests for High-Dimensional Models,‚Äù The Annals of Statistics,
42, 1166‚Äì1202.
van der Laan, M. and S. Rose (2001): Targeted Learning: Causal Inference for Observational
and Experimental Data, Springer-Verlag.
Wager, S. and S. Athey (2018): ‚ÄúEstimation and Inference of Heterogeneous Treatment Effects
using Random Forests,‚Äù Journal of the American Statistical Association, forthcoming.
Westreich, D., J. Lessler, and M. J. Funk (2010): ‚ÄúPropensity score estimation: neural
networks, support vector machines, decision trees (CART), and meta-classifiers as alternatives
to logistic regression,‚Äù Journal of clinical epidemiology, 63, 826‚Äì833.
White, H. (1989): ‚ÄúLearning in artificial neural networks: A statistical perspective,‚Äù Neural
computation, 1, 425‚Äì464.
‚Äî‚Äî‚Äî (1992): Artificial neural networks: approximation and learning theory, Blackwell Publishers,
Inc.
Yarotsky, D. (2017): ‚ÄúError bounds for approximations with deep ReLU networks,‚Äù Neural
Networks, 94, 103‚Äì114.
‚Äî‚Äî‚Äî (2018): ‚ÄúOptimal approximation of continuous functions by very deep ReLU networks,‚Äù
arXiv preprint arXiv:1802.03620.
Zhang, C., S. Bengio, M. Hardt, B. Recht, and O. Vinyals (2016): ‚ÄúUnderstanding deep
learning requires rethinking generalization,‚Äù arXiv preprint arXiv:1611.03530.

A

Proofs

In this section we provide a proof of Theorems 1 and 2, our main theoretical results for deep
ReLU networks, and their corollaries. The proof proceeds in several steps. We first give the main
breakdown and bound the bias (approximation error) term. We then turn our attention to the
empirical process term, to which we apply our localization. Much of the proof uses a generic
architecture, and thus pertains to both results. We will specialize the architecture to the multilayer perceptron only when needed later on. Other special cases and related results are covered in
Section A.4. Supporting Lemmas are stated in Section B.
The statements of Theorems 1 and 2 assume that n is large enough. Precisely, we require
n > (2eM )2 ‚à® Pdim(FDNN ). For notational simplicity we will denote fbDNN := fÀÜ, see (2.4), and
DNN := n , see Assumption 3. As we are simultaneously consider Theorems 1 and 2, the generic
notation DNN will be used throughout.

46

A.1

Main Decomposition and Bias Term

Referring to Assumption 3, define the best approximation realized by the deep ReLU network class
FDNN as
fn := arg min kf ‚àí f‚àó k‚àû .
f ‚ààFDNN
kf k‚àû ‚â§2M

By definition, n := DNN := kfn ‚àí f‚àó k‚àû .
Recalling the optimality of the estimator in (2.4), we know, as both fn and fÀÜ are in FDNN , that
‚àíEn [`(fÀÜ, z)] + En [`(fn , z)] ‚â• 0.
This result does not hold for f‚àó in place of fn , because f‚àó 6‚àà FDNN . Using the above display and
the curvature of Equation (2.1) (which does not hold with fn in place of f‚àó therein), we obtain
c1 kfÀÜ ‚àí f‚àó k2L2 (x) ‚â§ E[`(fÀÜ, z)] ‚àí E[`(f‚àó , z)]
‚â§ E[`(fÀÜ, z)] ‚àí E[`(f‚àó , z)] ‚àí En [`(fÀÜ, z)] + En [`(fn , z)]
h
i
h
i
= E `(fÀÜ, z) ‚àí `(f‚àó , z) ‚àí En `(fÀÜ, z) ‚àí `(f‚àó , z) + En [`(fn , z) ‚àí `(f‚àó , z)]
h
i
= (E ‚àí En ) `(fÀÜ, z) ‚àí `(f‚àó , z) + En [`(fn , z) ‚àí `(f‚àó , z)] .
(A.1)
Equation (A.1) is the main decomposition that begins the proof. The decomposition must be
done this way because of the above notes regarding f‚àó and fn . The first term is the empirical
process term that will be treated in the subsequent subsection. For the second term in (A.1), the
bias term or approximation error, we apply Bernstein‚Äôs inequality to find that, with probability at
least 1 ‚àí e‚àíŒ≥ÃÉ ,
s

2C`2 kfn ‚àí f‚àó k2‚àû Œ≥ÃÉ 21C` M Œ≥ÃÉ
En [`(fn , z) ‚àí `(f‚àó , z)] ‚â§ E [`(fn , z) ‚àí `(f‚àó , z)] +
+
n
3n
s


2C`2 kfn ‚àí f‚àó k2‚àû Œ≥ÃÉ 7C` M Œ≥ÃÉ
+
‚â§ c2 E kfn ‚àí f‚àó k2 +
n
n
s
2C`2 Œ≥ÃÉ 7C` M Œ≥ÃÉ
‚â§ c2 2n + n
+
,
n
n

(A.2)



using the Lipschitz and curvature of the loss function defined in Equation (2.1) and E kfn ‚àí f‚àó k2 ‚â§
kfn ‚àí f‚àó k2‚àû , along with the definition of 2n .
Once the empirical process term is controlled (in Section A.2), the two bounds will be brought
back together to compute the final result, see Section A.3.

47

A.2

Localization Analysis

We now turn to bounding the first term in (A.1) (the empirical processes term) using a localized
analysis that derives bounds based on scale insensitive complexity measure. The ideas of our
localization are rooted in Koltchinskii and Panchenko (2000) and Bartlett et al. (2005), and related
to Koltchinskii (2011). Localization analysis extending to the unbounded f case has been developed
in Mendelson (2014); Liang et al. (2015). This proof section proceeds in several steps.
A key quantity is the Rademacher complexity of the function class at hand. Given i.i.d.
Rademacher draws, Œ∑i = ¬±1 with equal probability independent of the data, the random variable Rn F, for a function class F, is defined as
n

1X
Rn F := sup
Œ∑i f (xi ).
f ‚ààF n
i=1

Intuitively, Rn F measures how flexible the function class is for predicting random signs. Taking
the expectation of Rn F conditioned on the data we obtain the empirical Rademacher complexity,
denoted EŒ∑ [Rn F]. When the expectation is taken over both the data and the draws Œ∑i , ERn F, we
get the Rademacher complexity.
A.2.1

Step I: Quadratic Process

The first step is to show that, with high probability, the empirical L2 norm of the error (f ‚àí
f‚àó ) is at most twice the population L2 norm bound for the same error, for certain functions f
outside a certain critical radius. This will be an ingredient to be used later on. Denote kf kn :=

1 Pn
2 1/2 to be the empirical L norm. To do so, we study the quadratic process
2
i=1 f (xi )
n
kf ‚àí f‚àó k2n ‚àí kf ‚àí f‚àó k2L2 (x) = En (f ‚àí f‚àó )2 ‚àí E(f ‚àí f‚àó )2 .
We will apply the symmetrization of Lemma 5 to g = (f ‚àí f‚àó )2 restricted to a radius kf ‚àí
f‚àó kL2 (x) ‚â§ r. This function g has variance bounded as
V[g] ‚â§ E[g 2 ] ‚â§ E((f ‚àí f‚àó )4 ) ‚â§ 9M 2 r2 .
Writing g = (f + f‚àó )(f ‚àí f‚àó ), we see that by Assumption 1, |g| ‚â§ 3M |f ‚àí f‚àó | ‚â§ 9M 2 , where the
first inequality verifies that g has a Lipschitz constant of 3M (when viewed as a function of its
argument f ), and second that g itself is bounded. We therefore apply Lemma 5, to obtain, with
probability at least 1 ‚àí exp(‚àíŒ≥ÃÉ), that for any f ‚àà F with kf ‚àí f‚àó kL2 (x) ‚â§ r,
En (f ‚àí f‚àó )2 ‚àí E(f ‚àí f‚àó )2
r

2Œ≥ÃÉ 36M 2 Œ≥ÃÉ
‚â§ 3ERn {g = (f ‚àí f‚àó ) : f ‚àà F, kf ‚àí f‚àó kL2 (x) ‚â§ r} + 3M r
+
n
3 n
r
2
2Œ≥ÃÉ 12M Œ≥ÃÉ
‚â§ 18M ERn {f ‚àí f‚àó : f ‚àà F, kf ‚àí f‚àó kL2 (x) ‚â§ r} + 3M r
+
,
n
n
2

48

(A.3)

where the second inequality applies Lemma 2 to the Lipschitz functions {g} (as a function of the
real values f (x)) and iterated expectations.
Suppose the radius r satisfies
r2 ‚â• 18M ERn {f ‚àí f‚àó : f ‚àà F, kf ‚àí f‚àó kL2 (x) ‚â§ r}

(A.4)

‚àö
6 6M 2 Œ≥ÃÉ
r ‚â•
.
n

(A.5)

and
2

Then we conclude from from (A.3) that
r
2

2

2

En (f ‚àí f‚àó ) ‚â§ r + r + 3M r

2Œ≥ÃÉ 12M 2 Œ≥ÃÉ
+
‚â§ (2r)2
n
n

(A.6)

where the first inequality uses (A.4) and the second line uses (A.5). This means that for r above
the ‚Äúcritical radius‚Äù (see Step III), the empirical L2 -norm is at most twice the population one
with probability at least 1 ‚àí exp(‚àíŒ≥ÃÉ).
A.2.2

Step II: One Step Improvement

In this step we will show that given a bound on kfÀÜ‚àí f‚àó kL2 (x) we can use this bound as information
to obtain a tighter bound, if the initial bound is loose as made precise at the end of this step. This
tightening will then be pursued to its limit in Step III, which leads to the final rate obtained in
Step IV. Step I will be used herein.
Suppose we know that for some r0 , kfÀÜ ‚àí f‚àó kL2 (x) ‚â§ r0 . We may always start with r0 = 3M
given Assumption 1 and (2.4). Apply Lemma 5 with G := {g = `(f, z) ‚àí `(f‚àó , z) : f ‚àà FDNN , kf ‚àí
f‚àó kL2 (x) ‚â§ r0 }, we find that, with probability at least 1 ‚àí 2e‚àíŒ≥ÃÉ , the empirical process term of (A.1)
is bounded as
s
h
i
(E ‚àí En ) `(fÀÜ, z) ‚àí `(f‚àó , z) ‚â§ 6EŒ∑ Rn G +

2C`2 r02 Œ≥ÃÉ 23 ¬∑ 3M C` Œ≥ÃÉ
+
,
n
3
n

(A.7)

where the middle term is due to the following variance calculation (recall Equation (2.1))
V[g] ‚â§ E[g 2 ] = E[|`(f, z) ‚àí `(f‚àó , z)|2 ] ‚â§ C`2 E(f ‚àí f‚àó )2 ‚â§ C`2 r02
Here the fact that Lemma 5 is variance dependent, and that the variance depends on the radius r0 ,
is important. It is this property which enables a sharpening of the rate with step-by-step reductions
in the variance bound, as in Section A.2.4.
For the empirical Rademacher complexity term, the first term of (A.7), Lemma 2, Step I, and

49

Lemma 3, yield
EŒ∑ Rn G = EŒ∑ Rn {g : g = `(f, z) ‚àí `(f‚àó , z), f ‚àà FDNN , kf ‚àí f‚àó k ‚â§ r0 }
‚â§ 2C` EŒ∑ Rn {f ‚àí f‚àó : f ‚àà FDNN , kf ‚àí f‚àó k ‚â§ r0 }
‚â§ 2C` EŒ∑ Rn {f ‚àí f‚àó : f ‚àà FDNN , kf ‚àí f‚àó kn ‚â§ 2r0 }


Z
12 2r0 p
log N (Œ¥, FDNN , k ¬∑ kn )dŒ¥
4Œ± + ‚àö
‚â§ 2C` inf
0<Œ±<2r0
n Œ±


Z
q
12 2r0
log N (Œ¥, FDNN |x1 ,...,xn , ‚àû)dŒ¥ ,
‚â§ 2C` inf
4Œ± + ‚àö
0<Œ±<2r0
n Œ±
with probability 1 ‚àí exp(‚àíŒ≥ÃÉ) (when applying Step I). Recall Lemma 4, one can further upper
bound the entropy integral when n > Pdim(FDNN ),


Z
q
12 2r0
inf
4Œ± + ‚àö
log N (Œ¥, FDNN |x1 ,...,xn , ‚àû)dŒ¥
0<Œ±<2r0
n Œ±
s
)
(
Z
2eM n
12 2r0
Pdim(FDNN ) log
dŒ¥
‚â§ inf
4Œ± + ‚àö
0<Œ±<2r0
Œ¥ ¬∑ Pdim(FDNN )
n Œ±
s


Pdim(FDNN )
2eM
3
‚â§ 32r0
log
+ log n
n
r0
2
with a particular choice of Œ± = 2r0

p
Pdim(FDNN )/n < 2r0 . Therefore, whenever r0 ‚â• 1/n and

n ‚â• (2eM )2 ,
r
EŒ∑ Rn G ‚â§ 128C` r0

Pdim(FDNN )
log n.
n

Applying this bound to (A.7), we have
h
i
(E ‚àí En ) `(fÀÜ, z) ‚àí `(f‚àó , z) ‚â§ Kr0

r

s
Pdim(FDNN )
log n + r0
n

2C`2 Œ≥ÃÉ 23M C` Œ≥ÃÉ
+
n
n

(A.8)

where K = 6 √ó 128C` .
Going back now to the main decomposition, plug (A.8) and (A.2) into (A.1), and we overall
have found that, with probability at least 1 ‚àí 4 exp(‚àíŒ≥ÃÉ), the following holds:
c1 kfÀÜ ‚àí f‚àó k2L2 (x)
Ô£´
Ô£∂
s
s
r
2 Œ≥ÃÉ
2 Œ≥ÃÉ
2C
2C
Pdim(FDNN )
23M C` Œ≥ÃÉ Ô£≠ 2
7C` M Œ≥ÃÉ Ô£∏
`
`
‚â§ Kr0
log n + r0
+
+ c2 n + n
+
n
n
n
n
n
Ô£´ r
Ô£∂
s
s
2 Œ≥ÃÉ
2C
2C`2 Œ≥ÃÉ
Pdim(F
)
Œ≥ÃÉ
DNN
` Ô£∏
‚â§ r0 ¬∑ Ô£≠K
log n +
+ c2 2n + n
+ 30M C`
n
n
n
n

50

Ô£´

‚àö

‚â§ r0 ¬∑ Ô£≠K C

r

s
W L log W
log n +
n

Ô£∂
s
2C`2 Œ≥ÃÉ
2C`2 Œ≥ÃÉ
Œ≥ÃÉ
Ô£∏ + c2 2n + n
+ 30M C` .
n
n
n

(A.9)

q

W L log W
The last line applies Lemma 6. Therefore, whenever n  r0 and
log n  r0 , the
n
ÀÜ
ÀÜ
knowledge that kf ‚àí f‚àó kL (x) ‚â§ r0 implies that (with high probability) kf ‚àí f‚àó kL (x) ‚â§ r1 , for
2

2

r1  r0 . One can recursively improve the bound r to a fixed point/radius r‚àó , which describes the
fundamental difficulty of the problem. This is done in the course of the next two steps.
A.2.3

Step III: Critical Radius

We now use the tightening of Step II to obtain the critical radius for this problem that is then
used as an input in the final rate derivation of Step IV. Formally, define the critical radius r‚àó to
be the largest fixed point

r‚àó = inf r > 0 : 18M ERn {f ‚àí f‚àó : f ‚àà F, kf ‚àí f‚àó kL2 (x) ‚â§ s} < s2 , ‚àÄs ‚â• r .
By construction this obeys (A.4), and thus so does 2r‚àó . Denote the event E (depending on the
data) to be

E = kf ‚àí f‚àó kn ‚â§ 4r‚àó , for all f ‚àà F and kf ‚àí f‚àó kL2 (x) ‚â§ 2r‚àó
and 1E to be the indicator that event E holds. We know from (A.6) that P(1E = 1) ‚â• 1 ‚àí n‚àí1 ,
p
‚àö
provided r‚àó ‚â• 18M log n/n to satisfy (A.5).
We can now give an upper bound for the the critical radius r‚àó . Using the logic of Step II to
bound the empirical Rademacher complexity, and then applying Lemma 6, we find that

r‚àó2 ‚â§ 18M ERn f ‚àí f‚àó : f ‚àà F, kf ‚àí f‚àó kL2 (x) ‚â§ r‚àó

‚â§ 18M ERn f ‚àí f‚àó : f ‚àà F, kf ‚àí f‚àó kL2 (x) ‚â§ 2r‚àó

‚â§ 18M E EŒ∑ Rn {f ‚àí f‚àó : f ‚àà F, kf ‚àí f‚àó kn ‚â§ 4r‚àó }1E + 3M (1 ‚àí 1E )
r
‚àö
W L log W
1
log n + 36M 2
‚â§ 36M K C ¬∑ r‚àó
n
n
r
‚àö
W L log W
‚â§ 72M K C ¬∑ r‚àó
log n,
n
p
‚àö
with the last line relying on the above restriction that r‚àó ‚â• 18M log n/n. Dividing through by
r‚àó yields the final bound:
‚àö

r‚àó ‚â§ 72M K C

r

W L log W
log n.
n

51

(A.10)

A.2.4

Step IV: Localization

We are now able to derive the final rate using a localization argument. This applies the results
of Step I and Step II repeatedly. Divide the space FDNN into shells of increasing radius by
intersecting it with the L2 balls
B(f‚àó , rÃÑ), B(f‚àó , 2rÃÑ)\B(f‚àó , rÃÑ), . . . B(f‚àó , 2l rÃÑ)\B(f‚àó , 2l‚àí1 rÃÑ)
where l ‚â• 1 is chosen to be the largest integer no greater than log2 ‚àö 2M

(log n)/n

(A.11)
. We will proceed to

find a bound on rÃÑ which determines the final rate results.
Suppose rÃÑ > r‚àó . Then for each shell, Step I and the union bound imply that with probability
at least 1 ‚àí 2l exp(‚àíŒ≥ÃÉ),
kf ‚àí f‚àó kL2 (x) ‚â§ 2j rÃÑ ‚áí kf ‚àí f‚àó kn ‚â§ 2j+1 rÃÑ.

(A.12)

Further, suppose that for some j ‚â§ l
fÀÜ ‚àà B(f‚àó , 2j rÃÑ)\B(f‚àó , 2j‚àí1 rÃÑ).

(A.13)

Then applying the one step improvement argument in Step II (again the variance dependence
captured in Lemma 5 is crucial, here reflected in the variance within each shell), Equation (A.9)
yields that with probability at least 1 ‚àí 4 exp(‚àíŒ≥ÃÉ),
Ô£±
Ô£º
Ô£´
Ô£∂
s
s
r
Ô£≤
2
2
‚àö
2C` t
2C` Œ≥ÃÉ
1
W L log W
Œ≥ÃÉ Ô£Ω
Ô£∏ + c2 2n + n
kfÀÜ ‚àí f‚àó k2L2 (x) ‚â§
2j rÃÑ ¬∑ Ô£≠K C
log n +
+ 30M C`
c1 Ô£≥
n
n
n
nÔ£æ
‚â§ 22j‚àí2 rÃÑ2 ,
if the following two conditions hold:
Ô£´
Ô£∂
s
r
2C`2 Œ≥ÃÉ
1 Ô£≠ ‚àö
W L log W
Ô£∏ ‚â§ 1 2j rÃÑ
K C
log n +
c1
n
n
2
Ô£´
Ô£∂
s
2C`2 Œ≥ÃÉ
1 Ô£≠ 2
Œ≥ÃÉ
1
c2 n + n
+ 26M C` Ô£∏ ‚â§ 22j rÃÑ2 .
c1
n
n
4
It is easy to see that these two hold for all j if we choose
Ô£´

8 Ô£≠ ‚àö
K C
rÃÑ =
c1

r

s
W L log W
log n +
n

Ô£∂ Ô£´s
Ô£∂
r
2C`2 Œ≥ÃÉ
Ô£∏ + Ô£≠ 2(c2 ‚à® 1) n + 120M C` Œ≥ÃÉ Ô£∏ + r‚àó . (A.14)
n
c1
c1
n

Therefore with probability at least 1 ‚àí 6l exp(‚àíŒ≥ÃÉ), we can perform shell-by-shell argument

52

combining the results in Step I and Step II:
kfÀÜ ‚àí f‚àó kL2 (x) ‚â§ 2l rÃÑ and kfÀÜ ‚àí f‚àó kn ‚â§ 2l+1 rÃÑ
implies kfÀÜ ‚àí f‚àó kL2 (x) ‚â§ 2l‚àí1 rÃÑ and kfÀÜ ‚àí f‚àó kn ‚â§ 2l rÃÑ
......
implies kfÀÜ ‚àí f‚àó kL2 (x) ‚â§ 20 rÃÑ and kfÀÜ ‚àí f‚àó kn ‚â§ 21 rÃÑ.
The ‚Äúand‚Äù part of each line follows from Step I and the implication uses the above argument
following Step II. Therefore in the end, we conclude with probability at least 1 ‚àí 6l exp(‚àíŒ≥ÃÉ),
kfÀÜ ‚àí f‚àó kL2 (x) ‚â§ rÃÑ ,

(A.15)

kfÀÜ ‚àí f‚àó kn ‚â§ 2rÃÑ .

(A.16)

Therefore choose Œ≥ = ‚àí log(6l) + Œ≥ÃÉ, we know from (A.14), and the upper bound on r‚àó in (A.10)
Ô£´
rÃÑ ‚â§

‚àö

8 Ô£≠
K C
c1

‚â§ C0

s

r

Ô£∂

2C`2 (log log n + Œ≥)
Ô£∏

W L log W
log n +
n
n
Ô£´s
Ô£∂
r
2(c2 ‚à® 1)
120M C` log log n + Œ≥ Ô£∏
+Ô£≠
n +
+ r‚àó
c2
c1
n
!
r
r
W L log W
log log n + Œ≥
log n +
+ n ,
n
n

(A.17)

with some constant C 0 > 0 that does not depend on n. This completes the proof of Theorem 2.

A.3

Final Steps for the MLP case

For the multi-layer perceptron, W ‚â§ C ¬∑ H 2 L, and plugging this into the bound (A.17), we obtain
r
C

0

H 2 L2 log(H 2 L)
log n +
n

r

log log n + Œ≥
+ n
n

!

To optimize this upper bound on rÃÑ, we need to specify the trade-offs in n and H and L. To do
so, we utilize the MLP-specific approximation rate of Lemma 7 and the embedding of Lemma 1.
Lemma 1 implies that, for any n , one can embed the approximation class FDNN given by Lemma
7 into a standard MLP architecture FMLP , where specifically
‚àíd

H = H(n ) ‚â§ W (n )L(n ) ‚â§ C 2 n Œ≤ (log(1/n ) + 1)2 ,
L = L(n ) ‚â§ C ¬∑ (log(1/n ) + 1).

53

For standard MLP architecture FMLP ,
‚àí 2d

H 2 L2 log(H 2 L) ‚â§ CÃÉ ¬∑ n Œ≤ (log(1/n ) + 1)7 .
Thus we can optimize the upper bound
Ô£´s
Ô£¨
rÃÑ ‚â§ C 0 Ô£≠

by choosing n = n

Œ≤
‚àí 2(Œ≤+d)

‚àí 2d
n Œ≤ (log(1/n ) + 1)7

n

Ô£∂
r
log n +

log log n + Œ≥
Ô£∑
+ n Ô£∏
n

d

, H  ¬∑n 2(Œ≤+d) log2 n, L  ¬∑ log n. This gives
rÃÑ ‚â§ C

n

Œ≤
‚àí 2(Œ≤+d)

r
4

log n +

log log n + t0
n

!
.

Hence putting everything together, with probability at least 1 ‚àí exp(‚àíŒ≥),


Œ≤
log log n + Œ≥
‚àí Œ≤+d
2
2
8
ÀÜ
E(f ‚àí f‚àó ) ‚â§ rÃÑ ‚â§ C n
log n +
,
n


log log n + Œ≥
‚àí Œ≤
En (fÀÜ ‚àí f‚àó )2 ‚â§ (2rÃÑ)2 ‚â§ 4C n Œ≤+d log8 n +
.
n
This completes the proof of Theorem 1.

A.4

Proof of Corollaries 1 and 2

For Corollary 1, we want to optimize
W L log U
log log n + Œ≥
log n +
+ 2DNN .
n
n
Yarotsky (2017, Theorem 1) shows that for the approximation error DNN to obey DNN ‚â§ , it
‚àí Œ≤d

suffices to choose W, U ‚àù 

(log(1/) + 1) and L ‚àù (log(1/) + 1), given the specific architecture

described therein. Therefore, we attain   n‚àíŒ≤/(2Œ≤+d) by setting W, U  nd/(2Œ≤+d) and L  log n,
yielding the desired result.
For Corollary 2, we need to optimize
H 2 L2 log(HL)
log log n + Œ≥
log n +
+ 2MLP .
n
n
Yarotsky (2018, Theorem 1) shows that for the approximation error MLP to obey MLP ‚â§ , it
d

suffices to choose H ‚àù 2d + 10 and L ‚àù ‚àí 2 , given the specific architecture described therein.
Thus, for   n‚àí1/(2+d) we take L  n‚àíd/(4+2d) , and the result follows.

54

B

Supporting Lemmas

First, we show that one can embed a feedforward network into the multi-layer perceptron architecture by adding auxiliary hidden nodes. This idea is due to Yarotsky (2018).
Lemma 1 (Embedding). For any function f ‚àà FDNN , there is a g ‚àà FMLP , with H ‚â§ W L + U ,
such that g = f .

Figure 7: Illustration of how to embed a feedforward network into a multi-layer perceptron, with
auxiliary hidden nodes (shown in yellow).
Proof. The idea is illustrated in Figure 7. For the edges in the directed graph of f ‚àà FDNN
that connect nodes not in adjacent layers (shown in yellow in Figure 7), one can insert auxiliary
hidden units in order to simply ‚Äúpass forward‚Äù the information. The number of such auxiliary
‚Äúpassforward units‚Äù is at most the number of offending edges times the depth L (i.e. for each edge,
at most L auxiliary nodes are required), and this is bounded by W L. Therefore the width of the
MLP network that subsumes the original is upper bounded by W L + U while still maintaining the
required embedding that for any fŒ∏ ‚àà FDNN , there is a gŒ∏0 ‚àà FMLP such that gŒ∏0 = fŒ∏ . In order to
match modern practice we only need to show that auxiliary units can be implemented with ReLU
activation. This can be done by setting the constant (‚Äúbias‚Äù) term b of each auxiliary unit large
enough to ensure œÉ(xÃÉ0 w + b) = xÃÉ0 w + b, and then subtracting the same b in the last receiving unit
along the path.
Next, we give two properties of the Rademacher complexity that we require (see Mendelson,
2003).
Lemma 2 (Contraction). Let œÜ : R ‚Üí R be a Lipschitz function |œÜ(f1 ) ‚àí œÜ(f2 )| ‚â§ L|f1 ‚àí f2 |, then
EŒ∑ Rn {œÜ ‚ó¶ f : f ‚àà F} ‚â§ 2LEŒ∑ Rn F.
55

Lemma 3 (Dudley‚Äôs Chaining). Let N (Œ¥, F, k ¬∑ kn ) denote the metric entropy for class F (with
covering radius Œ¥ and metric k ¬∑ kn ), then


Z
12 r p
4Œ± + ‚àö
EŒ∑ Rn {f : f ‚àà F, kf kn ‚â§ r} ‚â§ inf
log N (Œ¥, F, k ¬∑ kn )dŒ¥ .
0<Œ±<r
n Œ±
Furthermore, because kf kn ‚â§ maxi |f (xi )|, and therefore N (Œ¥, F, k ¬∑ kn ) ‚â§ N (Œ¥, F|x1 ,...,xn , ‚àû) and
so the upper bound in the conclusions also holds with N (Œ¥, F|x1 ,...,xn , ‚àû).
The next two results, Theorems 12.2 and 14.1 in Anthony and Bartlett (1999), show that the
metric entropy may be bounded in terms of the pseudo-dimension and that the latter is bounded
by the Vapnik-Chervonenkis (VC) dimension.
Lemma 4. Assume for all f ‚àà F, kf k‚àû ‚â§ M . Denote the pseudo-dimension of F as Pdim(F),
then for n ‚â• Pdim(F), we have for any Œ¥,

N (Œ¥, F|x1 ,...,xn , ‚àû) ‚â§

2eM ¬∑ n
Œ¥ ¬∑ Pdim(F)

Pdim(F )
.

The following symmetrization lemma bounds the empirical processes term using Rademacher
complexity, and is thus a crucial piece of our localization. This is a standard result based on
Talagrand‚Äôs concentration, but here special care is taken with the dependence on the variance.
Lemma 5 (Symmetrization, Theorem 2.1 in Bartlett et al. (2005)). For any g ‚àà G, assume that
|g| ‚â§ G and V[g] ‚â§ V . Then for every Œ≥ > 0, with probability at least 1 ‚àí e‚àíŒ≥
r

2V Œ≥ 4G Œ≥
+
,
n
3 n

r

2V Œ≥ 23G Œ≥
+
.
n
3 n

sup {Eg ‚àí En g} ‚â§ 3ERn G +
g‚ààG

and with probability at least 1 ‚àí 2e‚àít
sup {Eg ‚àí En g} ‚â§ 6EŒ∑ Rn G +
g‚ààG

The same result holds for supg‚ààG {En g ‚àí Eg}.
When bounding the complexity of FDNN , we use the following result. Bartlett et al. (2017) also
verify these bounds for the VC-dimension.
Lemma 6 (Theorem 6 in Bartlett et al. (2017), ReLU case). Consider a ReLU network architecture
F = FDNN (W, L, U ), then the pseudo-dimension is sandwiched as
c ¬∑ W L log(W/L) ‚â§ Pdim(F) ‚â§ C ¬∑ W L log W,
with some universal constants c, C > 0.

56

For multi-layer perceptrons we use the following approximation result, Theorem 1 of Yarotsky
(2017).
Lemma 7. There exists a network class FDNN , with ReLU activation, such that for any  > 0:
(a) FDNN approximates the W Œ≤,‚àû ([‚àí1, 1]d ) in the sense for any f‚àó ‚àà W Œ≤,‚àû ([‚àí1, 1]d ), there exists
a fn () := fn ‚àà FDNN such that
kfn ‚àí f‚àó k‚àû ‚â§ ,
‚àí Œ≤d

(b) and FDNN has L() ‚â§ C ¬∑ (log(1/) + 1) and W (), U () ‚â§ C ¬∑ 

(log(1/) + 1).

Here C only depends on d and Œ≤.
For completeness, we verify the requirements on the loss functions, Equation (2.1), for several
examples. We first treat least squares and logistic losses, in slightly more detail, as these are used
in our subsequent inference results and empirical application.
Lemma 8. Both the least squares (2.2) and logistic (2.3) loss functions obey the requirements of
Equation (2.1). For least squares, c1 = c2 = 1/2 and C` = M . For logistic regression, c1 =
(2(exp(M ) + exp(‚àíM ) + 2))‚àí1 , c2 = 1/8 and C` = 1.
Proof. The Lipschitz conditions are trivial. For least squares, using iterated expectations


2E`(f, Z) ‚àí 2E`(f‚àó , Z) = E ‚àí2Y f + f 2 + 2Y f‚àó ‚àí f‚àó2


= E ‚àí2f‚àó f (x) + f 2 + 2(f‚àó )2 ‚àí f‚àó2


= E (f ‚àí f‚àó )2 .
For logistic regression,



exp(f‚àó )
1 + exp(f )
E[`(f, Z)] ‚àí E[`(f‚àó , Z)] = E ‚àí
(f ‚àí f‚àó ) + log
.
1 + exp(f‚àó )
1 + exp(f‚àó )
exp(a)
Define ha (b) = ‚àí 1+exp(a)
(b ‚àí a) + log



1+exp(b)
1+exp(a)



, then

1
ha (b) = ha (a) + h0a (a)(b ‚àí a) + h00a (Œæa + (1 ‚àí Œæ)b) (b ‚àí a)2
2
1
and h00a (b) = exp(b)+exp(‚àíb)+2
‚â§ 14 . The lower bound holds as |Œæf‚àó + (1 ‚àí Œæ)f | ‚â§ M .

Beyond least squares and logistic regression, we give three further examples, discussed in the
general language of generalized linear models. Note that in the final example we move beyond a
simple scalar outcome.
Lemma 9. For a convex function g(¬∑) : R ‚Üí R, consider the generalized linear loss function
`(f, z) = ‚àíhy, f (x)i + g(f (x)). The curvature and the Lipschitz conditions in (2.1) will hold given
specific g(¬∑). In each case, the loss function corresponds to the negative log likelihood function.
57

(a) Poisson: g(t) = exp(t), with f‚àó (x) = log E[y|X = x].
(b) Gamma: g(t) = ‚àí log t, with f‚àó (x) = ‚àí(E[y|X = x])‚àí1 .
(c) Multinomial Logistic, K + 1 classes: g(t) = log(1 +
[k]

exp(f‚àó (x))/(1 +

X

P

k‚ààK exp(t

[k] )), with

[k0 ]

exp(f‚àó (x))) = E[y [k] |X = x].

k0 ‚ààK

Here v [k] denotes the k-th coordinate of a vector v.
Proof. Denote ‚àág, Hessian[g] to be the gradient and Hessian of the convex function g. By the
convexity of g, the optimal f‚àó satisfies E[‚àÇ`(f‚àó , Z)/‚àÇf |X = x] = 0, which implies
‚àág(f‚àó ) = E[Y |X = x].
If 2c0  Hessian[g(f )]  2c1 for all f of interest, then the curvature condition in (2.1) holds, because
E[`(f, Z)] ‚àí E[`(f‚àó , Z)] = E[‚àíh‚àág(f‚àó ), f ‚àí f‚àó i + g(f ) ‚àí g(f‚àó )]
1
= Ehf ‚àí f‚àó , Hessian[g(fÀú)]f ‚àí f‚àó i
2
‚â• c0 Ekf ‚àí f‚àó k2 ,
and the parallel argument for ‚â§ c1 Ekf ‚àíf‚àó k2 . The Lipschitz condition is equivalent to k‚àág(f )k ‚â§ C`0
for all f of interest, with bounded Y .
For our three examples in particular, we have the following.
(a) For Poisson regression:
k‚àác(f )k = | exp(f )| ‚â§ exp(M ),

Hessian[c(f )] = exp(f ) ‚àà [exp(‚àíM ), exp(M )].

(b) For Gamma regression, bounding ‚àíY above and below is equivalent to 1/M ‚â§ kf k ‚â§ M and
therefore:
k‚àác(f )k = |1/f | ‚â§ M, Hessian[c(f )] = 1/f 2 ‚àà [1/M 2 , M 2 ].
(c) For multinomial logistic regression, with general K, we know
k‚àác(f )k ‚â§ 1
Hessian[c(f )] = diag{z} ‚àí zz > where z [k] :=

58

exp(f [k] )
P
.
1 + k0 f [k0 ]

One can easily verify that the eigenvalues are bounded in the following sense, for bounded f ,
1
exp(M )
‚â§ Œª(Hessian[c(f )]) ‚â§
.
(1 + K exp(M ))2
1 + (K ‚àí 1) exp(‚àíM ) + exp(M )
This completes the proof.
Our last result is to verify condition (c) of Theorem 3. We do so using our localization, which
may be of future interest in second-step inference with machine learning methods.
Lemma 10. Let the conditions of Theorem 3 hold. Then

En


(¬µÃÇt (xi ) ‚àí ¬µt (xi )) 1 ‚àí

1{ti = t}
P[T = t|X = xi ]







Œ≤
log log n
‚àí Œ≤+d
8
= oP n
log n +
= oP n‚àí1/2 .
n

Proof. Without loss of generality we can take pÃÑ < 1/2. The only estimated function here is ¬µt (x),
which plays the role of f‚àó here. For function(als) L(¬∑) of the form

L(f ) := (f (xi ) ‚àí f‚àó (xi )) 1 ‚àí

1{ti = t}
P[T = t|X = xi ]


,

it is true that


E[1{ti = t}|xi ]
E[L(f )] = E (f (X) ‚àí f‚àó (X)) 1 ‚àí
=0
P[T = t|X = xi ]


and
h
i
V[L(f )] ‚â§ (1/pÃÑ ‚àí 1)2 E (f (X) ‚àí f‚àó (X))2 ‚â§ (1/pÃÑ ‚àí 1)2 rÃÑ2
|L(f )| ‚â§ (1/pÃÑ ‚àí 1) 2M.
For rÃÑ defined in (A.14),
18M ERn {f ‚àí f‚àó : f ‚àà F, kf ‚àí f‚àó kL2 (x) ‚â§ rÃÑ} ‚â§ rÃÑ2
ERn {L(f ) : f ‚àà F, kf ‚àí f‚àó kL2 (x) ‚â§ rÃÑ} ‚â§ 2 (1/pÃÑ ‚àí 1) ERn {f ‚àí f‚àó : f ‚àà F, kf ‚àí f‚àó kL2 (x) ‚â§ rÃÑ}
where the first line is due to rÃÑ > r‚àó , and second line uses Lemma 2.
Then by the localization analysis and Lemma 5, for all f ‚àà F, kf ‚àí f‚àó kL2 (x) ‚â§ rÃÑ, L(f ) obeys
s

4 (1/pÃÑ ‚àí 1)2 t 8 (1/pÃÑ ‚àí 1) 3M t
En [L(f )] = En [L(f )] ‚àí E[L(f )] ‚â§ 6C rÃÑ2 + rÃÑ
+
‚â§ 4C rÃÑ2
n
3
n


Œ≤
log log n
‚àí Œ≤+d
8
‚â§C¬∑ n
log n +
,
n


log log n
‚àí Œ≤
sup
En [L(f )] ‚â§ C ¬∑ n Œ≤+d log8 n +
.
n
f ‚ààF ,kf ‚àíf‚àó kL (x) ‚â§rÃÑ
2

59

d

With probability at least 1 ‚àí exp(‚àín Œ≤+d log8 n), fÀÜMLP lies in this set of functions, and therefore


b
ÀÜ
En [L(fMLP )] = En (fn,H,L (x) ‚àí f‚àó (x)) 1 ‚àí

1(T = t)
P (T = t|x = x)

as claimed.

60




‚â§C¬∑ n

Œ≤
‚àí Œ≤+d

log log n
log n +
n
8


,

