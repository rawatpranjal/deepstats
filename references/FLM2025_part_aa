Deep Learning for Individual Heterogeneity ∗

arXiv:2010.14694v3 [econ.EM] 25 Apr 2025

Max H. Farrell†

Tengyuan Liang‡

Sanjog Misra§

April 28, 2025

Abstract
This paper integrates deep neural networks (DNNs) into structural economic models
to increase flexibility and capture rich heterogeneity while preserving interpretability.
Economic structure and machine learning are complements in empirical modeling, not
substitutes: DNNs provide the capacity to learn complex, non-linear heterogeneity
patterns, while the structural model ensures the estimates remain interpretable and
suitable for decision making and policy analysis. We start with a standard parametric
structural model and then enrich its parameters into fully flexible functions of observables,
which are estimated using a particular DNN architecture whose structure reflects the
economic model. We illustrate our framework by studying demand estimation in
consumer choice. We show that by enriching a standard demand model we can capture
rich heterogeneity, and further, exploit this heterogeneity to create a personalized pricing
strategy. This type of optimization is not possible without economic structure, but
cannot be heterogeneous without machine learning. Finally, we provide theoretical
justification of each step in our proposed methodology. We first establish non-asymptotic
bounds and convergence rates of our structural deep learning approach. Next, a novel
and quite general influence function calculation allows for feasible inference via double
machine learning in a wide variety of contexts. These results may be of interest in many
other contexts, as they generalize prior work.

Keywords: Deep Learning, Structural Modeling, Heterogeneity, Machine Learning,
Neural Networks, Influence Functions, Neyman Orthogonality, Semiparametric Inference,
Double Machine Learning.

∗

We thank Whitney K. Newey for helpful comments and suggestions, as well as participants at several
seminars. Janani Sekar and Kirill Skobelev provided outstanding research assistance.
†
Department of Economics, UC Santa Barbara
‡
Booth School of Business, University of Chicago
§
Booth School of Business, University of Chicago

1

Introduction

Structural economic models are designed to obey theoretical and domain-specific restrictions
emanating from the discipline. As a consequence, when taken to data, estimates and inferences
from structural models are (economically) interpretable and directly useful for decisions,
counterfactuals, and policy making. Crucially, in many economic decision problems, the
structure of the model and the discipline it imposes afford meaningful interpolation and
extrapolation which, in turn, facilitates the construction of counterfactuals and ultimately
optimization.
Structural models are, however, potentially incomplete. Theory often does not specify
every aspect of the framework we need to conduct empirical analysis. One critical element,
and our focus in this paper, is the specification is heterogeneity. Even in cases where the
presence of heterogeneity is implied by economic reasoning, the form and type is often
unknown. As such, researchers must choose, or search for, a specification for heterogeneity
that respects the structural assumptions of the model, as well as the practical considerations
imposed by the data.
Often, the choice of the manner in which heterogeneity is modeled reflects the underlying
objectives of the researcher. There are scenarios where one wishes to merely “control” for
heterogeneity, since the key constructs of interest are not directly tied to it. In these cases,
heterogeneity is treated as a nuisance parameter (or function). More recently, however, there
has been a push to “exploit” heterogeneity by constructing individualized or personalized
policies. Here, heterogeneity itself becomes a key quantity of interest. In these cases, we also
will need to predict heterogeneity (for a new observation), and, as such, usual control-type
methods (e.g. fixed effects) become less relevant. Targeting and personalization must be
done on the basis of observable characteristics.
We argue that both the discipline of economic structure and the flexibility of machine learning (ML) tools are essential for heterogeneity-dependent constructs such as personalization
policies, targeting, and counterfactuals. Both have their strengths and weaknesses, but, by
combining them appropriately, one can use the strengths of each to remedy the shortcomings
of the other. On one hand, theory and structure make optimization feasible and reliable,
but do not dictate heterogeneity patterns. On the other hand, ML cannot learn economic
structure with finite data (as argued and demonstrated below). Therefore, ML cannot take
the place of economic theory and constraints, but flexibly learning patterns and heterogeneity
is precisely the strength of modern ML. Thus, while ML cannot replace structural modeling,
it can be useful to augment and enrich structure. We argue that economic structure and
machine learning are complements in empirical modeling, not substitutes.

1

We show how to embed machine learning, in the form of modern deep neural networks
(DNNs), into economic modeling. Our approach begins with a structural model imposed by
the researcher. The model relates the outcomes yi to the covariates of interest ti (treatments
or policy-relevant variables) and depends on parameters of interest θ, which are to be
estimated from the data. We suppose this structural model is captured by a loss function
ℓ(yi , ti , θ). The structural model encodes restrictions and constraints on the data generating
process and the parameters θ have direct economic interpretations. From an econometric
point of view, this is a standard parametric estimation and inference problem.
We then enrich the model by changing the parameters θ into parameter functions θ(x),
which are fully flexible functions of a vector of observed characteristics xi . The new model
is then ℓ(yi , ti , θ(xi )) (see Figure 1 for a visual depiction of our framework). This change
adds flexibility while maintaining all the structure and meaning of the original model. The
incompleteness of theory is reflected in the flexibility of treating θ(xi ) as a fully unknown
function. This is exactly where the strength of machine learning is exploited, and how ML
complements economic structure. Either one alone is insufficient: structure without machine
learning would lose the richness and fail to capture important patterns in the data, while
naively applying machine learning without structure would lose the interpretability and the
utility in policy/decision making. As an aside, we note that this framework does nest some
prior approaches to controlling for heterogeneity, but that is not our focus here.
The structural model allows for optimization, but the ML-enriched structure allows for
individual-level optimization (i.e. for each unique x). This is useful for decision making
and policy analysis at the individual level, answering “who gets what” questions regarding
targeting and “what who gets” questions of personalization. The former is useful, for example,
in deciding to which type of individuals we allocate a scarce resource or treatment, while the
latter focuses on the assignment of different treatments to each individual.
We introduce a novel yet simple structural deep learning approach to estimate the
parameter functions. The key to this approach is the architecture is depicted in Figure 1.
Neural networks allow the structure of the model to be directly encoded in the estimation
through the network architecture. The expressive power of deep neural networks comes
from the hidden layers. In standard approaches this flexibility is used to learn regression
functions or form predictions, which determine both the target and the loss functions. In
our approach, the hidden layers are directed through a parameter layer so that the power of
ML is used entirely for learning the parameter functions. These then enter the model layer
according to the structural model in order to optimize the loss. The required change to the
architecture is intuitive and simple to implement. (See Figure B.1 for a direct comparison
to regression.) The neural network optimizes the parameter functions to minimize the
2

Inputs

Hidden
layers

Parameter Model
layer
layer
θb1 (x)

x1

y

ℓ
xdX

b
ℓ y, t, θ(x)



t
θbdθ (x)

Figure 1: Structural deep learning. A schematic depiction of the deep neural network
architecture for estimating the parameter functions θ(x) in the structural economic model
ℓ(Y , T , θ(X)) defined in Eqn. (3.5).
same structural loss, not prediction loss. This formalizes exactly how machine learning and
structure are complementary. The ML enriches the economic model, filling in gaps left by
theory. Simultaneously the economics aids the implementation of ML to estimate structural
objects with economic meaning and interpretation. Our method allows for learning any
economically interesting parameter, such as coefficients, variances, elasticities, et cetera, all
as rich, heterogeneous functions.
To further build intuition, consider optimization of the network in Figure 1. Neural
network estimation relies on using gradients to find optima, proceeding by back propagation
through the network. Our idea is simply to structure the final layers of this network, as
shown in Figure 1, optimizing the loss through the parameter functions, instead of optimizing
prediction directly, but the computation is the same. This intuition also goes the other
direction: parametric structural estimation of ℓ(Y , T , θ) would be optimized in exactly the
same way, but the gradients would stop at the parameter layer. Our idea simply extends this
through additional layers to add heterogeneity.
We theoretically justify this part of the methodology by proving nonasymptotic bounds,
and implied convergence rates, for the structured deep neural networks (Theorem 1). This
result generalizes Farrell, Liang, and Misra (2021). These rates depend only on the dimension
of the heterogeneity xi , because the relationship of ti to yi is not learned from the data. Our
bounds apply to many settings of interest, requiring only mild conditions on the loss function
ℓ(·) and standard smoothness requirements, and thus may be of interest outside structural
modeling.
3

We obtain valid inference by applying the the double machine learning methodology of
Chernozhukov et al. (2018a), where the requisite orthogonal score is obtained from a novel
and general influence function calculation (Theorem 2). Our insight here exploits the fact
that we explicitly enrich a parametric model to obtain a two-step semiparametric setting,
and in this case we show that ordinary derivatives can continue to characterize the influence
function, just as in the parametric case. The influence function is thus known for a very
wide class of models and parameters, and need not be newly derived in each case. Further,
the only thing required is the value of these derivatives at the data points, not as functions
in general. These may be known in advance or, importantly for practice, obtained using
automatic differentiation engine built into neural network estimation without any human
derivations needed. Collectively, this means we can deliver the influence function, and thus
obtain valid inference via double machine learning, in more contexts than previously possible.
This hopefully lessens the barriers to obtaining post-ML inference in practice.
We illustrate our methods and results by revisiting and extending the analysis of Bertrand
et al. (2010). The data is from a large scale field experiment run on behalf of a financial
institution in South Africa. Consumers were sent marketing material for short terms loans
where a number of features of the advertising content and the interest rate offered were
randomized. As in the original paper, we employ a binary choice model, one of the workhorse
models in applied economics, but we enrich the model to capture heterogeneity. We then
model demand as a function of prices and marketing efforts, and once the demand function
is estimated, we compute optimal prices as a function of characteristics (third degree price
discrimination) and conduct inference on quantities such as counterfactual profits. This
involves applying double machine learning ideas to objects not available in closed form (the
solution to a fixed point problem), which illustrates the generality of our inference method.
The rest of the paper is organized as follows. We next review some related literature.
Section 2 then shows a simple, motivating example demonstrating why neither structural
models nor machine learning individually suffice. Our framework for structured deep learning
and subsequent inference is described in Section 3. We then apply these ideas in Section 4.
Section 5 summarizes the theoretical results and Section 6 concludes. The Appendix gives all
proofs, further discussion of related contexts, and thorough discussion of the important and
intuitive special case of generalized linear models in order to illustrate the ideas and connect
to other results.

4

1.1

Related Literature

Our work touches on several areas of economics, econometrics, and statistics, to which we
cannot hope to do justice in a few paragraphs. We will give an overview here, with some
further discussion is given throughout the paper.
First, at a broad level, our work is related to the recent interest in integrating ML into
economic research. A large part, if not all, of this work has used ML to learn regression
functions or make predictions, even if these feature in other economic contexts. ML is often
a substitute for nonparametric regression, and succeeds where classical methods fail due to
complexities or limitations of the data. Outside of pure prediction, causal inference is often
the end goal, but our work, being motivated by structure, extends to a breath of economic
contexts. For recent reviews in this space, with differing focuses, see Varian (2014), Bajari
et al. (2015), Mullainathan and Spiess (2017), and Athey and Imbens (2019).
Although it is not our focus per se, our work is related to literature on heterogeneous
treatment effects (Athey and Imbens, 2016; Chernozhukov et al., 2018b) and through it to
the topics of personalization and targeting (Dubé and Misra, 2023; Agrawal et al., 2022;
Hitsch, Misra, and Zhang, 2024). Our embedding of deep neural networks into structural
models affords the estimation of heterogeneous parameter functions across a broad array of
applications and the potential to generate personalized policies. We demonstrate its use in
the context of personalized interest rates in our application section.
On the theoretical front, the structured deep learning approach we propose, and the
accompanying nonasymptotic bounds, connect our work to the recent literature studying
the statistical properties of ML methods. Our theoretical results build directly upon and
generalize Farrell, Liang, and Misra (2021). Conceptually, Athey, Tibshirani, and Wager
(2019) and Foster and Syrgkanis (2023) may be the most closely related, as both focus
on quantities other than prediction functions, use models beyond regression, and utilize
orthogonal scores as key ingredients. The contexts and results are different, as we focus on
deep learning and semiparametric inference. Athey, Tibshirani, and Wager (2019) study
random forests and nonparametric inference, while Foster and Syrgkanis (2023) are concerned
with risk bounds under weak conditions.
The use of influence functions explicitly with the goal of obtaining valid inference under
weaker conditions is not new, whether for classical nonparametrics (Cattaneo, 2010) or
machine learning (Farrell, 2015). Our work contributes most directly to the recent literature
applying the double machine learning (DML) method of Chernozhukov et al. (2018a). DML
combines sample splitting with an orthogonal score, which an influence function provides.
Many applications of DML stick to contexts with well-known influence functions or handderive a new score. Our contribution here is the derivation of a generic influence function
5

that covers a broad class of models and can be computed automatically. We share this goal
with the “auto-DML” method (see Remark 3.4 below); our approach is different and has
relative strengths and weaknesses.

2

The Importance of Structure

Most modern applications of personalized policy design require the ability to recover heterogeneous responses to some treatment(s) from data, and the facility to use those in an
optimization framework to obtain improvements in the counterfactual. While the former can
be done via use of the ML toolkit, the latter requires that the estimated response functions
obey certain economic curvature and shape restrictions. As such, our choices are to either
learn these constraints from data or impose them via structure. We demonstrate, with a
simple example, that learning economic structure from finite date can be impossible but that
structure is invaluable for decision making problems. To make the point clear we ignore
heterogeneity and rely on the reader to extrapolate to the fact that the lessons here become
even more important when recovering heterogeneity or personalization, because this creates
greater demands on data. Our key point is that, structure imposes appropriate economic
restrictions and constraints, and consequently data is used more efficiently to learn the
heterogeneity.
The value of structure in economic modeling is well understood. Indeed, the half-century
old Lucas Critique is a (major) example of the value of structural thinking. With the power
of ML, there is a renewed temptation to learn everything from data alone. This view has
its origins in the machine learning literature, which is focused on prediction, but has seeped
into other contexts. Zadrozny (2004) writes, in the context of classification, that the ML
community is “interested in the predictive performance of the model and not in making
conclusions about the underlying mechanisms that generate the data.” So successful is
machine learning, and deep learning in particular, at prediction tasks, that it can be tempting
to assume that any problem can be tackled with accurate enough predictions. In the modern
age of large language models and generative AI, this approach is even more appealing. But in
contrast with those contexts, where prediction is sufficient, economic decision making cannot
proceed without the “underlying mechanisms”, that is, without structure and economics. The
danger in naively applying ML in decision making is that economic assumptions and structure
are replaced with the statistical and computational assumptions and structure of estimators.
To illustrate, we will use the data of Bertrand et al. (2010), which described and analyzed
more fully in Section 4. This context is ideal because, by focusing on a subset of the
data, we find that this is simultaneously a straightforward demand estimation problem and
6

(a) Random Forests

(b) Neural Networks

(c) Structural Logit

(d) Extrapolating to [0,20]%

(e) Extrapolating to [0,200]%

(f) Implied Revenue Functions

Figure 2: Structural modeling and machine learning for demand functions. Panels
(a), (b), and (c) show estimated demand functions using random forests, neural networks,
and a structural binary choice model, respectively, using the data of Bertrand et al. (2010).
Panels (d) and (e) show the extrapolated demand functions and (f) the implied revenue
functions.
straightforward prediction task, making both structural modeling and machine learning
well-motivated. The context is demand for a short-term loan given its interest rate, and our
goal is to estimate demand as a function of price and then derive the optimal interest rate
offer. For 40,507 individuals we observe the binary outcome yi ∈ {0, 1} indicating a loan
application decision and the policy relevant variable ti = ri giving the interest rate offered.
The good being “purchased” is the loan and its price is captured by the interest rate. The
interest rate ti = ri is randomized, so there are no endogeneity concerns, and is furthermore
continuous, taking on 46 unique values. With 40,507 observations on two variables, this is a
tailor-made classification task, and the naive ML view would hold that if conversion for any
interest rate offered can be predicted accurately, then demand function can be recovered, and
finally the interest rate can be optimized. However, as we will see, neglecting the “underlying
mechanisms that generate the data” will lead to a failure. If the unstructured approach fails
in this context, adding heterogeneity is only more difficult.

7

Figure 2 shows three different approaches to estimating the demand function in this data.
In each case, the dots show the empirical application rate (average purchasing decision) at
each offered interest rate. First we consider two ML approaches which treat this as purely a
predication/classification task, and are thus based on the model
P[Y = 1 | R = r] = η(r)

(2.1)

for an unknown function η(r) to be nonparametrically estimated. Panel (a) shows two random
forest estimates using the ranger package in R. The dotted orange line uses the default
settings. This appears to be undersmoothed and thus the blue dashed line forces a smoother
fit using an ad-hoc restriction on the tree depth. It is easy to (by eye) reject the default
random forest as an unreasonable estimate of the demand function, although it should be
noted that even this judgment requires economic theory, not statistical/ML criteria: nothing
in machine learning says the curve should be smooth or downward-sloping. Panel (b) shows
two neural network fits. The dotted orange line uses three hidden layers with 20, 50, and 20
nodes, respectively, for a flexible fit. Note that despite the richness, the fit does not appear to
be overly complex. The blue dashed line uses two layers with 10 nodes each for a smoother
fit (matching Section 4). Finally, panel (c) replaces (2.1) with the basic workhorse structural
model in industrial organization, namely a linear randomly utility model with a logistic errors,
so that
1
,
(2.2)
P[Y = 1 | R = r] = G (θ1⋆ + θ2⋆ r) :=
1 + exp (− [θ1⋆ + θ2⋆ r])
where G(u) is the logit function and θ1⋆ and θ2⋆ are the intercept and slope, respectively.
The smoothed-out forest and both neural networks produce curves that could reasonably
be demand functions. However, as we continue with the pricing problem, it becomes clear
how different are these fits. Panels (d) and (e) show the extrapolated estimated demand
functions of the smoothed forest, the (10,10) neural network, and the binary choice model to
interest rates of zero to 20% (panel (d)) and zero to 200% (panel (e)). Panel (f) then shows
the implied revenue for each demand function estimate. While this does not include costs,
it is clear what each estimate implies for an optimal price (see Section 4 for more realistic
profit optimization).
The forest fit is perhaps the most striking. Demand is completely flat for any higher
interest rate, and therefore revenue grows without bound, and so the optimal price is infinite.
This is both mechanical and a general phenomenon: a forest is an average of trees, which
are piecewise constant, and therefore all extrapolation is based on a flat line, no matter the
context or data. The neural network closely resembles the binary choice model on the support
of the data, but when we extrapolate we see that the invisible complexities of the network
8

yield very different demand and revenue curves. In contrast to both of these, the structural
model gives well-behaved demand and revenue curves and yields a reasonable optimal price.
To conclude this example, several remarks are in order. (i) Students of all fields, including
econometrics, statistics, and machine learning, are taught never to extrapolate a statistical
fit outside the support of the data, and here we see why in dramatic fashion. Note that the
same logic applies to interpolation. However, our goal of price optimization requires interand extra-polation, which by definition is based on assumptions and not upon the data. (ii)
One can dismiss this simple example with the argument that no reasonable decision maker
sets an infinite price or that the demand/revenue functions look obviously “wrong”, but again,
these conclusions come from using economic structure to inform the ML and data, not the
other way around. (iii) The lessons in this example are unrelated to statistical uncertainty.
Different data would yield different fits, but the issues would persist, or perhaps manifest
differently (except the forest, which always yields infinite prices, and thus has zero statistical
uncertainty). (iv) Finally, it is worth noting that structural reasoning is commonly used in
machine learning to improve prediction tasks. The structure of image recognition tasks is
directly encoded into convolutional neural networks. The transformer architecture (Vaswani
et al., 2017) is behind the recent success of large language models. In sum, structure at its
core means constraints and restrictions, and this example shows that statistical structure
and assumptions may not be appropriate or sufficient for economic decision making.

3

Embedding Deep Learning in Structural Models

3.1

Enriched Structural Model

We now turn to our approach to enrich structural models with machine learning. We describe
our structured deep neural network architecture, which bakes the model into the ML, and
how second-step inference can be done using double machine learning. The prior section
shows that ML is not suitable for learning structure, which is by definition the opposite of
flexibility. Our goal here is to use ML for what it is good at, recovering complex heterogeneity,
while retaining all the advantages of the structural model.
The starting point is a standard parametric structural model, described by
θ ⋆ = arg min E [ℓ(Y , T , θ)] ,

(3.1)

θ∈Θ

where the loss function ℓ(y, t, θ) encodes the researcher’s economic restrictions on how the
outcomes Y ∈ RdY relate to the variables of interest T ∈ RdT , depending on parameters

9

θ ∈ Θ ⊂ Rdθ .1 The “treatment” T variables can be randomized or not and can be continuous,
discrete, or mixed; the model can be causal or not. In the first step the structural parameters
θ are estimated from the data by solving the empirical analogue of (3.1) over the appropriate
parameter space Θ.
In a second step, inference is conducted on an object of the form
E[H(X, θ, t̃)],

(3.2)

for a known function H, that depends on the parameters, covariates, and possibly some
value of interest t̃ for the policy relevant variables. The function H can be the parameters
themselves, quantities such as marginal effects, elasticities, measures of surplus, and can
encompass optimization and other such operations. For example, t̃ can be an optimal price,
as in our empirical application in Section 4. Technically t̃ can be subsumed into the definition
of H, but it is expositionally useful to make explicit.
Other than restricting to per-observation losses and smooth functions (so that derivatives
exist), the combination of Equations (3.1) and (3.2) encompass a wide variety of parametric
models, including many M- and Z- estimation problems, such regression models, quasi/pseudolikelihoods, or generalized estimating equations and accompanying second step objects like
marginal effects, average elasticities, and other economic quantities (Newey and McFadden,
1994; Ackerberg et al., 2007).
The positives of structural modeling are widely appreciated, and demonstrated above. The
model incorporates theory-based restrictions and constraints. This means that the estimated
parameters, or transformations thereof, are interpretable and directly useful useful in policy
analysis, decision making, and the formation of counterfactuals. The two-step nature of
the problem reflects exactly these types of analyses, where second step counterfactuals are
obtained after estimating primitives of the model in the first stage. It also means that the
data is used more efficiently, since structure implies restrictions, which will be crucial when
embedding machine learning or nonparametrics.
The major drawback of Eqn. (3.1) is rigidity: it does not allow for any heterogeneity in the
relationship between Y and T . We want to allow for heterogeneity in a way that is flexible but
maintains the economic structure. This could be for robustness, as results will be biased and
subsequent decisions or analyses will be erroneous if heterogeneity exists but is neglected. This
type of concern has been the focus of much recent research, particularly in program evaluation
(Chernozhukov et al., 2018b; Wager and Athey, 2018; De Chaisemartin and d’Haultfoeuille,
1

Notation. Vectors and matrices will be written in boldface. Capital letters are used for population random
variables; lower case for realizations. The expectation operator with respect to the true data generating process
is denoted E[·]. True values use a superscript ⋆. The L2 norm for a function g(x) is ∥g∥2 = E[g(X)2 ]1/2 .

10

2023). Further, as mentioned earlier, capturing and exploiting heterogeneity is key in modern
targeting and personalization contexts, which is also an active area of research.
Our approach recasts the parameters θ as parameter functions θ(X) that are fully flexible
in observed covariates X ∈ RdX . That is, we assume the true first stage structural model is


θ ⋆ (·) = arg min E ℓ Y , T , θ(X) ,

(3.3)

θ∈F

for a function class F, which obeys standard restrictions (Assumption 2 below). Equation (3.3)
is a specific, though quite general, formulation of nonparametric M-estimation (Gallant and
Nychka, 1987). Crucially all the economic structure is maintained: whatever the interpretation
of the parameter θ, the same holds for θ ⋆ (x) for individuals of “type” X = x. These are
not “nuisance” functions. The view, particularly common in the realm of inference after ML,
is that first step functions are literally a nuisance, i.e. something annoying that must be
dealt with, but are not interesting. We object to this view: in many applications the learned
heterogeneity is actually the most interesting part, and because the θ ⋆ (x) are interpretable
functions in our framework, using them is straightforward.
The second step parameter of interest is correspondingly enriched, to be


µ⋆ = E H X, θ ⋆ (X), t̃ .

(3.4)

To keep exposition simple we focus on the case of averages, i.e. where the parameter of interest
is available in closed form. Replacing this step with parametric GMM is straightforward but
notationally more cumbersome (see Remark 3.1). Our results may be useful for nonparametric
inference as well (Remark 3.2). Equations (3.3) and (3.4) together define a broad class of
two-step semiparametric settings, matching the generality of (3.1) and (3.2). Appendix C
shows some examples, both familiar and new. Finally, note that µ⋆ is defined using θ ⋆ (X),
but in some decision making cases this may not be appropriate, as discussed in Remark 4.1.

3.2

Structural Deep Learning

To estimate the parameter function θ ⋆ (x) we solve the empirical analogue of (3.3) where
minimization is over a class of structural deep neural networks, i.e. those with architecture
shown in Figure 1 as discussed above. We define
n
1X
b
ℓ(yi , ti , θ(xi )),
θ(·) = arg min
θ∈Fdnn n i=1

11

(3.5)

where Fdnn is the class of deep neural networks that encodes not only the overall architecture
but also tuning parameter choices of the width and depth of the network and the shape
parameter given by the activation function. We focus on the standard fully connected
feedforward neural network with ReLU activation for the hidden layers, but the same idea
applies to other cases. To save space, we will not review deep learning basics here. Recent
textbook treatments include: Goodfellow, Bengio, and Courville (2016) and James et al.
(2021) for introductions and examples, Roberts, Yaida, and Hanin (2022) for theory, and
Keydana (2023) for implementation. Theorem 1 in Section 5 gives the convergence of these
b
θ(·).
Our structural deep learning approach can be applied to any model/loss ℓ(y, t, θ(x)) and
is easy to implement. Only a few lines of code need be changed relative standard neural
network implementations to enforce the structural model and force the power of the network
to learn the parameter functions rather than optimize the loss directly. This “structural
compatibility” is one argument in favor of using deep neural networks for this modeling.
There are several further reasons why neural networks are well suited to this task. (1) Perhaps
most obviously, deep learning is a state of the art ML method and brings with it all the
advantages: expressive power, the capacity for high dimensionality and flexible interactions,
and moreover, the ability to use novel data such as images or text, which could be included
in the definition of X. (2) From a practical point of view, neural networks can handle
discrete covariates seamlessly without affecting the convergence rate nor the implementation.
While in theory including discrete covariates does not impact the convergence rate for many
nonparametric estimators, obtaining these estimates in practice typically requires special care
and custom methods (Racine and Li, 2004; Ma, Racine, and Yang, 2015). (3) Deep learning is
built on automatic differentiation, which allows us to obtain the necessary influence function
computationally for free for any µ⋆ from (3.4) based on any first step (3.3) (see Section 3.3)
making double machine learning conceptually straightforward to apply in any problem.
Of course, DNNs are not the only method that could be used to recover the parameter
functions, nor do we claim any formal optimality property. The economic model holds globally,
and we wish to match this in estimation, as it is important to learn counterfactual quantities
in the second step, but other methods have the same property, including global series methods
such as splines or polynomials and modern basis-function style methods including ridge
regression and lasso.2 All these estimators impose the structural model globally and in a
computationally simple way. This may be one reason why series methods were often used
2
The term “global” is slightly ambiguous, because methods like splines or partitioning are global smoothers
and are structurally compatible, but use only data local to the evaluation point, more like a traditional kernel
(Cattaneo and Farrell, 2013; Cattaneo, Farrell, and Feng, 2020).

12

in classic nonparametric M estimation (see Gallant and Nychka (1987) for pioneering early
work and Chen (2007) for further theory). Also Chen (2007) views neural networks as sieves
for generic nonparametric M estimation, as do we. But these methods lack advantages (1),
(2), and (3) from the prior paragraph. Classical methods often do not work for modern
applications with more than several covariates. Selection methods require pre-specification of
bases, including interaction effects.
Although it is not always as straightforward, it is possible to use “local” methods to learn
the value of the parameter functions at a point, i.e. θ ⋆ (x) for some x, and much recent work
has been done in this area. Fan and Zhang (2008) discuss kernels and local polynomials
while Zeileis, Hothorn, and Hornik (2008), Athey, Tibshirani, and Wager (2019), Nekipelov,
Novosad, and Ryan (2019), and Chatla and Shmueli (2020) use trees and random forests; all
share our goal of learning non-prediction function. Athey, Tibshirani, and Wager (2019) and
Foster and Syrgkanis (2023) are perhaps the most recent closest antecedents to our work,
as both move ML away from prediction and both use orthogonal scores as a key ingredient.
Athey, Tibshirani, and Wager (2019) study random forests in a similar class of problems
to (3.3). Random forests also handle higher dimensional problems and flexible interactions,
sharing advantage (1) above, and by their nature share advantage (2). As such, the forests of
Athey, Tibshirani, and Wager (2019) are probably the closest substitute for neural networks
in our setting. The goal in that paper is inference on the nonparametric object θ ⋆ (x) at a
point X = x, which is different than our two-step problem. Influence functions based on
ordinary derivatives are used there, though in the estimation step to aid with tree splitting,
rather than for inference per se, as we do below. Foster and Syrgkanis (2023) are focused
on risk minimization, as opposed to inference, but consider a similar class of models and
use orthogonal scores to obtain improved properties. Their estimation target more closely
resembles our two step problem, though a key innovation in their case is performing this in
one step, rather than estimating the primitives and then studying different counterfactual
questions. Their first and second step parameters can also be more general objects than ours
in some ways.
Finally, other recent work has considered the combination of deep learning with structural
models. Examples in different contexts include Norets (2012), Igami (2020), Chen, Didisheim,
and Scheidegger (2021), Kaji, Manresa, and Pouliot (2023), and Wei and Jiang (2025). Often,
the goal is estimation of a parametric structural model and deep learning methods are applied
to learn the mapping of data to parameters, which is quite different from our use of neural
networks to enrich parameters for heterogeneity and personalization.

13

3.3

Inference

For second step inference on µ⋆ = E[H(X, θ ⋆ (X), t̃)], we apply and contribute to the
recent semiparametric inference literature and in particular the now-common double machine
learning (DML) method of Chernozhukov et al. (2018a). DML is a generic method for
obtaining semiparametric inference that combines two ingredients: sample splitting and
a Neyman orthogonal score. Asymptotic Normality then follows under weak conditions
on first step estimators, which is particularly important for ML-based first steps, because
relatively little is known of their mathematical and statistical properties. Typically, L2
convergence rates are sufficient (along with mild regularity conditions). To save space, we
defer to Chernozhukov et al. (2018a), Newey and Robins (2018), and Chernozhukov et al.
(2022a) for complete discussion of the method and further references. As usual with DML, our
results apply to any first-step estimator that has fast enough rates. However, an automatic
differentiation engine may be convenient for computing the influence function when it is not
known from prior work.
While sample splitting is straightforward both conceptually and in applications, orthogonal
scores are not, and this is where our contribution lies. The key characterization of a Neyman
orthogonal score is that is has a zero derivative with respect to the first stage parameters
(see Section 5), which translates to less sensitivity to first step error. Though more general
conceptually, Neyman orthogonal scores often come from influence functions where this
zero derivative property is ensured. Knowing the influence function, or orthogonal score, is
required for using DML, and this is often a hurdle in applications. For this reason many
papers stick to known examples (e.g., average treatment effects or partially linear models),
list references where scores are derived, or hand-derive new scores (typical examples are
Belloni, Chernozhukov, and Hansen (2014), Farrell (2015), and Chernozhukov et al. (2022a)).
We contribute to this inference method by showing that an influence function for µ⋆ is
automatically available for any combination of ℓ(·) and H(·) in Equations (3.3) and (3.4)
provided their ordinary derivatives exist. We give the form of this influence function and
discuss how it can be automatically computed in applications, even when it cannot be
derived precisely or written down. This makes it easy to deploy in practice, because ℓ(·)
and H(·) are defined by the researcher, and the rest can proceed automatically, requiring
only nonparametric regression. A key insight into applicability is that the orthogonal score
need not be known as a function per se, we only require that its value be computable at
each observation. Our goal and results here are closest to the series of work on “auto-DML”
(Remark 3.4).
To state the influence function result, and thus estimation and inference for µ⋆ , we define
relevant derivatives, which may be known or found with automatic differentiation. Let
14

Hθ (x, ·; t̃) be the dµ × dθ Jacobian of H with respect to θ. Let ℓθ (y, t, ·) be the gradient of
ℓ with respect to θ and ℓθθ (y, t, ·) be the matrix of second derivatives. These are ordinary
derivatives, not functional derivatives, and are thus computable automatically, even if they are
evaluated at the value of the function θ ⋆ (x).3 Then an influence function that applies to any
combination of enriched model (3.3) and parameter of interest (3.4) is ψ(y, t, x, θ, Λ) − µ⋆ ,
with

ψ(y, t, θ, Λ) = H x, θ(x); t̃ − Hθ (x, θ(x); t̃)Λ(x)−1 ℓθ (y, t, θ(x)),
(3.6)
where Λ(x) = E[ℓθθ (y, t, θ(x)) | X = x] the population conditional Hessian of ℓ, all
evaluated at θ = θ(x) and is the “other” nonparametric object that generally arises in the
correction term. The (inverse) propensity score is perhaps the most familiar example, and
our requirements on Λ(x) mirror that case exactly. Theorem 2 below justifies this result and
discusses it further. Influence functions are being used in a wide variety of contexts, and our
derivation may be of independent interest (Remark 5.3).
To build intuition, it is useful to note that our result has precisely the same form as
its parametric counterpart, but appropriately generalized. For the parametric two step

inference problem of (3.1) and (3.2), the influence function is known to be H x, θ; t̃ −
Hθ (x, θ; t̃)Λ−1 ℓθ (y, t, θ), where Λ = E[ℓθθ (y, t, θ)] (Newey and McFadden, 1994). Equation
(3.6) is the same, and equally general, but enriched and hence conditional on X = x. This
tight connection helps with implementation, because if the original structural model is
understood by the researcher, so is the enriched version. For example, identification in the
parametric case typically requires Λ−1 to exist, and in the enriched version this must hold
for all “types” x. This is often a matter of assuming a positive conditional variance, rather
than marginal variance. Appendix C gives some examples and special cases.
b and standard errors follows standard ideas of DML (CherObtaining the estimate µ
nozhukov et al., 2018a). The only wrinkle is that Λ(x) generally depends on θ(x), and so
3

To be precise, ℓθ (y, t, θ(x)) is the dθ -vector of first derivatives with respect to the parameter, evaluated
at the number θ(x), as in

∂ℓ y, t, b
ℓθ (y, t, θ(x)) =
.
∂b
b=θ(x)

The Jacobian Hθ (x, ·; t̃) is similar. Then ℓθθ (y, t, θ(x)) is the dθ × dθ matrix of second derivatives, with
{k1 , k2 } element given by
h
i
∂ 2 ℓ (y, t, b)
ℓθθ (y, t, θ(x))
=
,
∂bk1 ∂bk2 b=θ(x)
k1 ,k2
where bk1 and bk2 are the respective elements of the place-holder b. The use of standard differentiation
in these contexts has been used in some prior work, though to our knowledge not paired with automatic
differentiation to obtain feasible inference in such a broad set of models. One can obtain the influence function
using functional derivatives and then evaluate these at the true function θ ⋆ (·), but our point is that one only
needs the ordinary derivative evaluated at the number θ ⋆ (x) (or the data points more specifically).

15

three-way splitting is technically required. Note that throughout, particularly when paired
with automatic differentiation, note that we do not need to known the functions Hθ (x, θ(x); t̃),
b
b i ); t̃), ℓθ (yi , ti , θ(x
b i )),
ℓθ (y, t, θ(x)), and ℓθθ (y, t, θ(x));
it suffices to know the values Hθ (xi , θ(x
b i )), which are readily available.
and ℓθθ (yi , ti , θ(x
DML is now quite standard, so we keep the description brief. The data are divided into
K disjoint subsets, denoted Ik , of equal size. Then
K

1 X
bk ,
b=
µ
µ
K k=1

bk =
µ

1 X
b k (xi )),
ψ(yi , ti , θbk (xi ), Λ
|Ik | i∈I

(3.7)

k

b k (xi ) are obtained using observations in I c . If Λ(x) depends
where |Ik | and θbk (xi ) and Λ
k
c
b
b
on θ(x), then Ik is divided in two and θk (xi ) and Λk (xi ) are obtained on separate samples.
This further splitting is required in theory in general (see Remark 3.3 and Appendix B
for common exceptions) but in practice may make no difference. Also, short stacking may
yield improved performance (Ahrens et al., 2025). From the optimization of (3.5), we can
b i )) automatically. In other words, ℓθθ (yi , ti , θ(x
b i )) is simply
obtain the values ℓθθ (yi , ti , θ(x
b i ). This is identical
a column of data which we nonparametrically regress on xi to obtain Λ(x
to the more standard practice of obtaining the functional derivatives, characterizing the
nonparametric object that is Λ(x), and then estimating it: we are not doing a numerical
approximation. Numerical differentiation can be even simpler and faster, however. The same
ideas apply to all other (potentially) unknown functions. Finally, the asymptotic variance
Ψ = V[ψ(Y , T , X, θ ⋆ , Λ)] can be consistently estimated by
K
2
1 X 1 X
b
b
b
b
Ψ=
ψ(yi , ti , θk (xi ), Λk (xi )) − µk .
K k=1 |Ik | i∈I

(3.8)

k

Theorem 2 validates this procedure. In theory, the size of each subset is proportional to n
and thus sample splitting does not impact convergence rates or precision, but in practice this
can be a poor approximation. For classical kernel estimators, Velez (2024) proves that larger
K yields better results in an asymptotic framework with K → ∞. For machine learning
methods, this can be computationally costly, but we find in applications that large K is more
stable. With small K, the different θbk (x) can be quite different. Finally, sample splitting
is not always needed, as shown by Farrell, Liang, and Misra (2021) for post-DNN average
treatment effects (under essentially the same assumptions as here). It would be useful to
extend that argument to more general models; see Chen, Syrgkanis, and Austern (2022) for
relevant theory.

16

Remark 3.1 (Two Step GMM). Our first-step correction can also be used in GMM settings.
The second step may be a set of moment conditions E[H̃(X, θ ⋆ (X), µ⋆ , t̃)] = 0 for some H̃.
The correction factor then takes the form ϕ(y, t, x, Λ, θ) = H̃θ (x, θ(x), µ⋆ , t̃)Λ(x)−1 ℓθ (y, t, θ(x)).
Following Chernozhukov et al. (2022a), adding this correction to the original moments yields
the orthogonal moment conditions E[H̃(X, θ ⋆ (X), µ⋆ , t̃) − ϕ(y, t, x, Λ, θ)] = 0. Chernozhukov et al. (2022a), extending Chernozhukov et al. (2018a), show that the advantages of
DML carry over to GMM based on these moments. Our methodology applies here, including
the use of automatic differentiation if needed. Asymptotic normality will follow by applying
Chernozhukov et al. (2022a) instead of Chernozhukov et al. (2018a).
♣

Remark 3.2. Semenova and Chernozhukov (2021) and Colangelo and Lee (2023) use
orthogonal scores for two-step nonparametric inference (following ML) and we conjecture
that the same could be done in the enriched structural models considered here. This would
be valuable for future research. See also Remark B.3.
♣

Remark 3.3 (Notes on Λ(x)). The function Λ(x) is a nuisance in the truest sense: it is
required only because we use influence functions as a tool to obtain valid inference. In our
case, Λ(x) is always low-dimensional and consists only of regressions, not conditional density
functions. In some cases obtaining Λ(x) is simplified. If T is randomly assigned, or more
generally independent of X, then Λ(x) can often be computed or estimated more simply,
though it may remain a function of x. Even if not randomly assigned, if T is known to be
assigned based on a subvector of X, such as in targeting problems, this can be imposed on
the estimation. Inverse functions are standard in semiparametric inference, and in practice
this piece is often the most difficult. The challenge is generally model- and data- specific,
unrelated to the choice of nonparametric/ML method. Often some form of regularization is
used. The most widely known is trimming the propensity score, which has received rigorous
study (Ma and Wang, 2020). Extending this to our setting would be useful.
♣

Remark 3.4 (Auto-DML). The recent work on “auto-DML” shares our goal of applying the
DML method without having to derive a new orthogonal score each time. This recent work
(Chernozhukov, Newey, and Singh, 2022a,b; Chernozhukov et al., 2022a,b, 2023, 2024a,b)
shows that the correction term of the influence function satisfies certain moment conditions
which can then be taken to data to obtain feasible inference. In the context of (3.6), this
amounts to estimating the quantity Hθ (x, θ(x); t̃)Λ(x)−1 . There are different versions of this
method, but broadly speaking it is as general and widely applicable as ours. Our approach
17

can be applied to many second stage parameters, since the first step correction need only be
estimated once. We do require that θ ⋆ (x) enter the second stage only through its evaluation
at a data point. Both methods require a second nonparametric estimation of a nuisance
function. The auto-DML method estimates the inverse directly, which might yield more
stable performance than estimating Λ(x) and then inverting. On the other hand, examining
b
Λ(x)
is often a key step in the analysis and helps diagnose identification, such as examining
estimated propensity scores to evaluate the overlap assumption.
♣

4

Application: Advertising and Personalized Interest
Rates

4.1

Empirical Context

In this section we use our framework to replicate and extend Bertrand et al. (2010). The data
is from a large scale field experiment run on behalf of a financial institution in South Africa.
Consumers were sent marketing material for short terms loans where a number of features of
the advertising content and the interest rate offered were randomized (full details are left
to that paper). For the purposes of our analysis we will focus on the interest rate offered
as the treatment variable, and denote the scalar T = R. We will treat the characteristics
of the advertising assigned to customer