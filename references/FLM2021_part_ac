e, and K. Srinivasan (2017): â€œLarge scale cross category analysis of consumer
review content on sales conversion leveraging deep learning,â€ working paper, NYU Stern.
Ma, X. and J. Wang (2018): â€œRobust Inference Using Inverse Probability Weighting,â€ arXiv
preprint arXiv:1810.11397.
Makovoz, Y. (1996): â€œRandom approximants and neural networks,â€ Journal of Approximation
Theory, 85, 98â€“109.
Manski, C. F. (2004): â€œStatistical treatment rules for heterogeneous populations,â€ Econometrica,
72, 1221â€“1246.
Mendelson, S. (2003): â€œA few notes on statistical learning theory,â€ in Advanced lectures on
machine learning, Springer, 1â€“40.
â€”â€”â€” (2014): â€œLearning without concentration,â€ in Conference on Learning Theory, 25â€“39.
Mhaskar, H. and T. Poggio (2016a): â€œDeep vs. shallow networks: An approximation theory
perspective,â€ arXiv preprint arXiv:1608.03287.
Mhaskar, H. N. and T. Poggio (2016b): â€œDeep vs. shallow networks: An approximation theory
perspective,â€ Analysis and Applications, 14, 829â€“848.
Nair, V. and G. E. Hinton (2010): â€œRectified linear units improve restricted boltzmann machines,â€ in Proceedings of the 27th international conference on machine learning (ICML-10),
807â€“814.
Newey, W. K. and J. M. Robins (2018): â€œCross-fitting and fast remainder rates for semiparametric estimation,â€ arXiv preprint arXiv:1801.09138.
Oaxaca, R. (1973): â€œMale-Female Wage Differentials in Urban Labor Markets,â€ International
Economic Review, 14, 693â€“709.
44

Pisier, G. (1981): â€œRemarques sur un reÌsultat non publieÌ de B. Maurey,â€ in SeÌminaire Analyse
fonctionnelle (ditâ€ Maurey-Schwartzâ€), 1â€“12.
Polson, N. and V. Rockova (2018): â€œPosterior Concentration for Sparse Deep Learning,â€ arXiv
preprint arXiv:1803.09138.
Raghu, M., B. Poole, J. Kleinberg, S. Ganguli, and J. Sohl-Dickstein (2017): â€œOn the
Expressive Power of Deep Neural Networks,â€ in Proceedings of the 34th International Conference
on Machine Learning, ed. by D. Precup and Y. W. Teh, International Convention Centre, Sydney,
Australia: PMLR, vol. 70 of Proceedings of Machine Learning Research, 2847â€“2854.
Robins, J., L. Li, R. Mukherjee, E. Tchetgen, and A. van der Vaart (2017): â€œMinimax
Estimation of a Functional on a Structured High-Dimensional Model,â€ The Annals of Statistics,
45, 1951â€“1987.
Robins, J., L. Li, E. Tchetgen, and A. van der Vaart (2008): â€œHigher order influence
functions and minimax estimation of nonlinear functionals,â€ in Probability and Statistics: Essays
in Honor of David A. Freedman, ed. by D. Nolan and T. Speed, Beachwood, Ohio, USA: Institute
of Mathematical Statistics, vol. 2.
Robins, J., E. T. Tchetgen, L. Li, and A. van der Vaart (2009): â€œSemiparametric Minimax
Rates,â€ Electronic Journal of Statistics, 3, 1305â€“1321.
Robins, J. M., A. Rotnitzky, and L. Zhao (1994): â€œEstimation of Regression Coefficients
When Some Regressors Are Not Always Observed,â€ Journal of the American Statistical Association, 89, 846â€“866.
â€”â€”â€” (1995): â€œAnalysis of Semiparametric Regression Models for Repeated Outcomes in the
Presence of Missing Data,â€ Journal of the American Statistical Association, 90, 846â€“866.
Romano, J. P. (2004): â€œOn non-parametric testing, the uniform behaviour of the t-test, and
related problems,â€ Scandinavian Journal of Statistics, 31, 567â€“584.
Safran, I. and O. Shamir (2016): â€œDepth separation in relu networks for approximating smooth
non-linear functions,â€ arXiv preprint arXiv:1610.09887.
Schmidt-Hieber, J. (2017): â€œNonparametric regression using deep neural networks with ReLU
activation function,â€ arXiv preprint arXiv:1708.06633.
Shalit, U., F. D. Johansson, and D. Sontag (2017): â€œEstimating individual treatment effect:
generalization bounds and algorithms,â€ arXiv preprint arXiv:1606.03976.
Sloczynski, T. and J. M. Wooldridge (2018): â€œA General Double Robustness Result for
Estimating Average Treatment Effects,â€ Econometric Theory, 34, 112â€“133.
Stone, C. J. (1982): â€œOptimal global rates of convergence for nonparametric regression,â€ The
annals of statistics, 1040â€“1053.
Taddy, M., M. Gardner, L. Chen, and D. Draper (2015): â€œA nonparametric Bayesian
analysis of heterogeneous treatment effects in digital experimentation,â€ Arxiv preprint
arXiv:1412.8563.
Tan, Z. (2018): â€œModel-assisted inference for treatment effects using regularized calibrated estimation with high-dimensional data,â€ arXiv preprint arXiv:1801.09817.
45

Telgarsky, M. (2016): â€œBenefits of depth in neural networks,â€ arXiv preprint arXiv:1602.04485.
Tsiatis, A. A. (2006): Semiparametric Theory and Missing Data, New York: Springer.
van de Geer, S., P. Buhlmann, Y. Ritov, and R. Dezeure (2014): â€œOn Asymptotically
Optimal Confidence Regions and Tests for High-Dimensional Models,â€ The Annals of Statistics,
42, 1166â€“1202.
van der Laan, M. and S. Rose (2001): Targeted Learning: Causal Inference for Observational
and Experimental Data, Springer-Verlag.
Wager, S. and S. Athey (2018): â€œEstimation and Inference of Heterogeneous Treatment Effects
using Random Forests,â€ Journal of the American Statistical Association, forthcoming.
Westreich, D., J. Lessler, and M. J. Funk (2010): â€œPropensity score estimation: neural
networks, support vector machines, decision trees (CART), and meta-classifiers as alternatives
to logistic regression,â€ Journal of clinical epidemiology, 63, 826â€“833.
White, H. (1989): â€œLearning in artificial neural networks: A statistical perspective,â€ Neural
computation, 1, 425â€“464.
â€”â€”â€” (1992): Artificial neural networks: approximation and learning theory, Blackwell Publishers,
Inc.
Yarotsky, D. (2017): â€œError bounds for approximations with deep ReLU networks,â€ Neural
Networks, 94, 103â€“114.
â€”â€”â€” (2018): â€œOptimal approximation of continuous functions by very deep ReLU networks,â€
arXiv preprint arXiv:1802.03620.
Zhang, C., S. Bengio, M. Hardt, B. Recht, and O. Vinyals (2016): â€œUnderstanding deep
learning requires rethinking generalization,â€ arXiv preprint arXiv:1611.03530.

A

Proofs

In this section we provide a proof of Theorems 1 and 2, our main theoretical results for deep
ReLU networks, and their corollaries. The proof proceeds in several steps. We first give the main
breakdown and bound the bias (approximation error) term. We then turn our attention to the
empirical process term, to which we apply our localization. Much of the proof uses a generic
architecture, and thus pertains to both results. We will specialize the architecture to the multilayer perceptron only when needed later on. Other special cases and related results are covered in
Section A.4. Supporting Lemmas are stated in Section B.
The statements of Theorems 1 and 2 assume that n is large enough. Precisely, we require
n > (2eM )2 âˆ¨ Pdim(FDNN ). For notational simplicity we will denote fbDNN := fË†, see (2.4), and
DNN := n , see Assumption 3. As we are simultaneously consider Theorems 1 and 2, the generic
notation DNN will be used throughout.

46

A.1

Main Decomposition and Bias Term

Referring to Assumption 3, define the best approximation realized by the deep ReLU network class
FDNN as
fn := arg min kf âˆ’ fâˆ— kâˆ .
f âˆˆFDNN
kf kâˆ â‰¤2M

By definition, n := DNN := kfn âˆ’ fâˆ— kâˆ .
Recalling the optimality of the estimator in (2.4), we know, as both fn and fË† are in FDNN , that
âˆ’En [`(fË†, z)] + En [`(fn , z)] â‰¥ 0.
This result does not hold for fâˆ— in place of fn , because fâˆ— 6âˆˆ FDNN . Using the above display and
the curvature of Equation (2.1) (which does not hold with fn in place of fâˆ— therein), we obtain
c1 kfË† âˆ’ fâˆ— k2L2 (x) â‰¤ E[`(fË†, z)] âˆ’ E[`(fâˆ— , z)]
â‰¤ E[`(fË†, z)] âˆ’ E[`(fâˆ— , z)] âˆ’ En [`(fË†, z)] + En [`(fn , z)]
h
i
h
i
= E `(fË†, z) âˆ’ `(fâˆ— , z) âˆ’ En `(fË†, z) âˆ’ `(fâˆ— , z) + En [`(fn , z) âˆ’ `(fâˆ— , z)]
h
i
= (E âˆ’ En ) `(fË†, z) âˆ’ `(fâˆ— , z) + En [`(fn , z) âˆ’ `(fâˆ— , z)] .
(A.1)
Equation (A.1) is the main decomposition that begins the proof. The decomposition must be
done this way because of the above notes regarding fâˆ— and fn . The first term is the empirical
process term that will be treated in the subsequent subsection. For the second term in (A.1), the
bias term or approximation error, we apply Bernsteinâ€™s inequality to find that, with probability at
least 1 âˆ’ eâˆ’Î³Ìƒ ,
s

2C`2 kfn âˆ’ fâˆ— k2âˆ Î³Ìƒ 21C` M Î³Ìƒ
En [`(fn , z) âˆ’ `(fâˆ— , z)] â‰¤ E [`(fn , z) âˆ’ `(fâˆ— , z)] +
+
n
3n
s


2C`2 kfn âˆ’ fâˆ— k2âˆ Î³Ìƒ 7C` M Î³Ìƒ
+
â‰¤ c2 E kfn âˆ’ fâˆ— k2 +
n
n
s
2C`2 Î³Ìƒ 7C` M Î³Ìƒ
â‰¤ c2 2n + n
+
,
n
n

(A.2)



using the Lipschitz and curvature of the loss function defined in Equation (2.1) and E kfn âˆ’ fâˆ— k2 â‰¤
kfn âˆ’ fâˆ— k2âˆ , along with the definition of 2n .
Once the empirical process term is controlled (in Section A.2), the two bounds will be brought
back together to compute the final result, see Section A.3.

47

A.2

Localization Analysis

We now turn to bounding the first term in (A.1) (the empirical processes term) using a localized
analysis that derives bounds based on scale insensitive complexity measure. The ideas of our
localization are rooted in Koltchinskii and Panchenko (2000) and Bartlett et al. (2005), and related
to Koltchinskii (2011). Localization analysis extending to the unbounded f case has been developed
in Mendelson (2014); Liang et al. (2015). This proof section proceeds in several steps.
A key quantity is the Rademacher complexity of the function class at hand. Given i.i.d.
Rademacher draws, Î·i = Â±1 with equal probability independent of the data, the random variable Rn F, for a function class F, is defined as
n

1X
Rn F := sup
Î·i f (xi ).
f âˆˆF n
i=1

Intuitively, Rn F measures how flexible the function class is for predicting random signs. Taking
the expectation of Rn F conditioned on the data we obtain the empirical Rademacher complexity,
denoted EÎ· [Rn F]. When the expectation is taken over both the data and the draws Î·i , ERn F, we
get the Rademacher complexity.
A.2.1

Step I: Quadratic Process

The first step is to show that, with high probability, the empirical L2 norm of the error (f âˆ’
fâˆ— ) is at most twice the population L2 norm bound for the same error, for certain functions f
outside a certain critical radius. This will be an ingredient to be used later on. Denote kf kn :=

1 Pn
2 1/2 to be the empirical L norm. To do so, we study the quadratic process
2
i=1 f (xi )
n
kf âˆ’ fâˆ— k2n âˆ’ kf âˆ’ fâˆ— k2L2 (x) = En (f âˆ’ fâˆ— )2 âˆ’ E(f âˆ’ fâˆ— )2 .
We will apply the symmetrization of Lemma 5 to g = (f âˆ’ fâˆ— )2 restricted to a radius kf âˆ’
fâˆ— kL2 (x) â‰¤ r. This function g has variance bounded as
V[g] â‰¤ E[g 2 ] â‰¤ E((f âˆ’ fâˆ— )4 ) â‰¤ 9M 2 r2 .
Writing g = (f + fâˆ— )(f âˆ’ fâˆ— ), we see that by Assumption 1, |g| â‰¤ 3M |f âˆ’ fâˆ— | â‰¤ 9M 2 , where the
first inequality verifies that g has a Lipschitz constant of 3M (when viewed as a function of its
argument f ), and second that g itself is bounded. We therefore apply Lemma 5, to obtain, with
probability at least 1 âˆ’ exp(âˆ’Î³Ìƒ), that for any f âˆˆ F with kf âˆ’ fâˆ— kL2 (x) â‰¤ r,
En (f âˆ’ fâˆ— )2 âˆ’ E(f âˆ’ fâˆ— )2
r

2Î³Ìƒ 36M 2 Î³Ìƒ
â‰¤ 3ERn {g = (f âˆ’ fâˆ— ) : f âˆˆ F, kf âˆ’ fâˆ— kL2 (x) â‰¤ r} + 3M r
+
n
3 n
r
2
2Î³Ìƒ 12M Î³Ìƒ
â‰¤ 18M ERn {f âˆ’ fâˆ— : f âˆˆ F, kf âˆ’ fâˆ— kL2 (x) â‰¤ r} + 3M r
+
,
n
n
2

48

(A.3)

where the second inequality applies Lemma 2 to the Lipschitz functions {g} (as a function of the
real values f (x)) and iterated expectations.
Suppose the radius r satisfies
r2 â‰¥ 18M ERn {f âˆ’ fâˆ— : f âˆˆ F, kf âˆ’ fâˆ— kL2 (x) â‰¤ r}

(A.4)

âˆš
6 6M 2 Î³Ìƒ
r â‰¥
.
n

(A.5)

and
2

Then we conclude from from (A.3) that
r
2

2

2

En (f âˆ’ fâˆ— ) â‰¤ r + r + 3M r

2Î³Ìƒ 12M 2 Î³Ìƒ
+
â‰¤ (2r)2
n
n

(A.6)

where the first inequality uses (A.4) and the second line uses (A.5). This means that for r above
the â€œcritical radiusâ€ (see Step III), the empirical L2 -norm is at most twice the population one
with probability at least 1 âˆ’ exp(âˆ’Î³Ìƒ).
A.2.2

Step II: One Step Improvement

In this step we will show that given a bound on kfË†âˆ’ fâˆ— kL2 (x) we can use this bound as information
to obtain a tighter bound, if the initial bound is loose as made precise at the end of this step. This
tightening will then be pursued to its limit in Step III, which leads to the final rate obtained in
Step IV. Step I will be used herein.
Suppose we know that for some r0 , kfË† âˆ’ fâˆ— kL2 (x) â‰¤ r0 . We may always start with r0 = 3M
given Assumption 1 and (2.4). Apply Lemma 5 with G := {g = `(f, z) âˆ’ `(fâˆ— , z) : f âˆˆ FDNN , kf âˆ’
fâˆ— kL2 (x) â‰¤ r0 }, we find that, with probability at least 1 âˆ’ 2eâˆ’Î³Ìƒ , the empirical process term of (A.1)
is bounded as
s
h
i
(E âˆ’ En ) `(fË†, z) âˆ’ `(fâˆ— , z) â‰¤ 6EÎ· Rn G +

2C`2 r02 Î³Ìƒ 23 Â· 3M C` Î³Ìƒ
+
,
n
3
n

(A.7)

where the middle term is due to the following variance calculation (recall Equation (2.1))
V[g] â‰¤ E[g 2 ] = E[|`(f, z) âˆ’ `(fâˆ— , z)|2 ] â‰¤ C`2 E(f âˆ’ fâˆ— )2 â‰¤ C`2 r02
Here the fact that Lemma 5 is variance dependent, and that the variance depends on the radius r0 ,
is important. It is this property which enables a sharpening of the rate with step-by-step reductions
in the variance bound, as in Section A.2.4.
For the empirical Rademacher complexity term, the first term of (A.7), Lemma 2, Step I, and

49

Lemma 3, yield
EÎ· Rn G = EÎ· Rn {g : g = `(f, z) âˆ’ `(fâˆ— , z), f âˆˆ FDNN , kf âˆ’ fâˆ— k â‰¤ r0 }
â‰¤ 2C` EÎ· Rn {f âˆ’ fâˆ— : f âˆˆ FDNN , kf âˆ’ fâˆ— k â‰¤ r0 }
â‰¤ 2C` EÎ· Rn {f âˆ’ fâˆ— : f âˆˆ FDNN , kf âˆ’ fâˆ— kn â‰¤ 2r0 }


Z
12 2r0 p
log N (Î´, FDNN , k Â· kn )dÎ´
4Î± + âˆš
â‰¤ 2C` inf
0<Î±<2r0
n Î±


Z
q
12 2r0
log N (Î´, FDNN |x1 ,...,xn , âˆ)dÎ´ ,
â‰¤ 2C` inf
4Î± + âˆš
0<Î±<2r0
n Î±
with probability 1 âˆ’ exp(âˆ’Î³Ìƒ) (when applying Step I). Recall Lemma 4, one can further upper
bound the entropy integral when n > Pdim(FDNN ),


Z
q
12 2r0
inf
4Î± + âˆš
log N (Î´, FDNN |x1 ,...,xn , âˆ)dÎ´
0<Î±<2r0
n Î±
s
)
(
Z
2eM n
12 2r0
Pdim(FDNN ) log
dÎ´
â‰¤ inf
4Î± + âˆš
0<Î±<2r0
Î´ Â· Pdim(FDNN )
n Î±
s


Pdim(FDNN )
2eM
3
â‰¤ 32r0
log
+ log n
n
r0
2
with a particular choice of Î± = 2r0

p
Pdim(FDNN )/n < 2r0 . Therefore, whenever r0 â‰¥ 1/n and

n â‰¥ (2eM )2 ,
r
EÎ· Rn G â‰¤ 128C` r0

Pdim(FDNN )
log n.
n

Applying this bound to (A.7), we have
h
i
(E âˆ’ En ) `(fË†, z) âˆ’ `(fâˆ— , z) â‰¤ Kr0

r

s
Pdim(FDNN )
log n + r0
n

2C`2 Î³Ìƒ 23M C` Î³Ìƒ
+
n
n

(A.8)

where K = 6 Ã— 128C` .
Going back now to the main decomposition, plug (A.8) and (A.2) into (A.1), and we overall
have found that, with probability at least 1 âˆ’ 4 exp(âˆ’Î³Ìƒ), the following holds:
c1 kfË† âˆ’ fâˆ— k2L2 (x)
ï£«
ï£¶
s
s
r
2 Î³Ìƒ
2 Î³Ìƒ
2C
2C
Pdim(FDNN )
23M C` Î³Ìƒ ï£­ 2
7C` M Î³Ìƒ ï£¸
`
`
â‰¤ Kr0
log n + r0
+
+ c2 n + n
+
n
n
n
n
n
ï£« r
ï£¶
s
s
2 Î³Ìƒ
2C
2C`2 Î³Ìƒ
Pdim(F
)
Î³Ìƒ
DNN
` ï£¸
â‰¤ r0 Â· ï£­K
log n +
+ c2 2n + n
+ 30M C`
n
n
n
n

50

ï£«

âˆš

â‰¤ r0 Â· ï£­K C

r

s
W L log W
log n +
n

ï£¶
s
2C`2 Î³Ìƒ
2C`2 Î³Ìƒ
Î³Ìƒ
ï£¸ + c2 2n + n
+ 30M C` .
n
n
n

(A.9)

q

W L log W
The last line applies Lemma 6. Therefore, whenever n  r0 and
log n  r0 , the
n
Ë†
Ë†
knowledge that kf âˆ’ fâˆ— kL (x) â‰¤ r0 implies that (with high probability) kf âˆ’ fâˆ— kL (x) â‰¤ r1 , for
2

2

r1  r0 . One can recursively improve the bound r to a fixed point/radius râˆ— , which describes the
fundamental difficulty of the problem. This is done in the course of the next two steps.
A.2.3

Step III: Critical Radius

We now use the tightening of Step II to obtain the critical radius for this problem that is then
used as an input in the final rate derivation of Step IV. Formally, define the critical radius râˆ— to
be the largest fixed point

râˆ— = inf r > 0 : 18M ERn {f âˆ’ fâˆ— : f âˆˆ F, kf âˆ’ fâˆ— kL2 (x) â‰¤ s} < s2 , âˆ€s â‰¥ r .
By construction this obeys (A.4), and thus so does 2râˆ— . Denote the event E (depending on the
data) to be

E = kf âˆ’ fâˆ— kn â‰¤ 4râˆ— , for all f âˆˆ F and kf âˆ’ fâˆ— kL2 (x) â‰¤ 2râˆ—
and 1E to be the indicator that event E holds. We know from (A.6) that P(1E = 1) â‰¥ 1 âˆ’ nâˆ’1 ,
p
âˆš
provided râˆ— â‰¥ 18M log n/n to satisfy (A.5).
We can now give an upper bound for the the critical radius râˆ— . Using the logic of Step II to
bound the empirical Rademacher complexity, and then applying Lemma 6, we find that

râˆ—2 â‰¤ 18M ERn f âˆ’ fâˆ— : f âˆˆ F, kf âˆ’ fâˆ— kL2 (x) â‰¤ râˆ—

â‰¤ 18M ERn f âˆ’ fâˆ— : f âˆˆ F, kf âˆ’ fâˆ— kL2 (x) â‰¤ 2râˆ—

â‰¤ 18M E EÎ· Rn {f âˆ’ fâˆ— : f âˆˆ F, kf âˆ’ fâˆ— kn â‰¤ 4râˆ— }1E + 3M (1 âˆ’ 1E )
r
âˆš
W L log W
1
log n + 36M 2
â‰¤ 36M K C Â· râˆ—
n
n
r
âˆš
W L log W
â‰¤ 72M K C Â· râˆ—
log n,
n
p
âˆš
with the last line relying on the above restriction that râˆ— â‰¥ 18M log n/n. Dividing through by
râˆ— yields the final bound:
âˆš

râˆ— â‰¤ 72M K C

r

W L log W
log n.
n

51

(A.10)

A.2.4

Step IV: Localization

We are now able to derive the final rate using a localization argument. This applies the results
of Step I and Step II repeatedly. Divide the space FDNN into shells of increasing radius by
intersecting it with the L2 balls
B(fâˆ— , rÌ„), B(fâˆ— , 2rÌ„)\B(fâˆ— , rÌ„), . . . B(fâˆ— , 2l rÌ„)\B(fâˆ— , 2lâˆ’1 rÌ„)
where l â‰¥ 1 is chosen to be the largest integer no greater than log2 âˆš 2M

(log n)/n

(A.11)
. We will proceed to

find a bound on rÌ„ which determines the final rate results.
Suppose rÌ„ > râˆ— . Then for each shell, Step I and the union bound imply that with probability
at least 1 âˆ’ 2l exp(âˆ’Î³Ìƒ),
kf âˆ’ fâˆ— kL2 (x) â‰¤ 2j rÌ„ â‡’ kf âˆ’ fâˆ— kn â‰¤ 2j+1 rÌ„.

(A.12)

Further, suppose that for some j â‰¤ l
fË† âˆˆ B(fâˆ— , 2j rÌ„)\B(fâˆ— , 2jâˆ’1 rÌ„).

(A.13)

Then applying the one step improvement argument in Step II (again the variance dependence
captured in Lemma 5 is crucial, here reflected in the variance within each shell), Equation (A.9)
yields that with probability at least 1 âˆ’ 4 exp(âˆ’Î³Ìƒ),
ï£±
ï£¼
ï£«
ï£¶
s
s
r
ï£²
2
2
âˆš
2C` t
2C` Î³Ìƒ
1
W L log W
Î³Ìƒ ï£½
ï£¸ + c2 2n + n
kfË† âˆ’ fâˆ— k2L2 (x) â‰¤
2j rÌ„ Â· ï£­K C
log n +
+ 30M C`
c1 ï£³
n
n
n
nï£¾
â‰¤ 22jâˆ’2 rÌ„2 ,
if the following two conditions hold:
ï£«
ï£¶
s
r
2C`2 Î³Ìƒ
1 ï£­ âˆš
W L log W
ï£¸ â‰¤ 1 2j rÌ„
K C
log n +
c1
n
n
2
ï£«
ï£¶
s
2C`2 Î³Ìƒ
1 ï£­ 2
Î³Ìƒ
1
c2 n + n
+ 26M C` ï£¸ â‰¤ 22j rÌ„2 .
c1
n
n
4
It is easy to see that these two hold for all j if we choose
ï£«

8 ï£­ âˆš
K C
rÌ„ =
c1

r

s
W L log W
log n +
n

ï£¶ ï£«s
ï£¶
r
2C`2 Î³Ìƒ
ï£¸ + ï£­ 2(c2 âˆ¨ 1) n + 120M C` Î³Ìƒ ï£¸ + râˆ— . (A.14)
n
c1
c1
n

Therefore with probability at least 1 âˆ’ 6l exp(âˆ’Î³Ìƒ), we can perform shell-by-shell argument

52

combining the results in Step I and Step II:
kfË† âˆ’ fâˆ— kL2 (x) â‰¤ 2l rÌ„ and kfË† âˆ’ fâˆ— kn â‰¤ 2l+1 rÌ„
implies kfË† âˆ’ fâˆ— kL2 (x) â‰¤ 2lâˆ’1 rÌ„ and kfË† âˆ’ fâˆ— kn â‰¤ 2l rÌ„
......
implies kfË† âˆ’ fâˆ— kL2 (x) â‰¤ 20 rÌ„ and kfË† âˆ’ fâˆ— kn â‰¤ 21 rÌ„.
The â€œandâ€ part of each line follows from Step I and the implication uses the above argument
following Step II. Therefore in the end, we conclude with probability at least 1 âˆ’ 6l exp(âˆ’Î³Ìƒ),
kfË† âˆ’ fâˆ— kL2 (x) â‰¤ rÌ„ ,

(A.15)

kfË† âˆ’ fâˆ— kn â‰¤ 2rÌ„ .

(A.16)

Therefore choose Î³ = âˆ’ log(6l) + Î³Ìƒ, we know from (A.14), and the upper bound on râˆ— in (A.10)
ï£«
rÌ„ â‰¤

âˆš

8 ï£­
K C
c1

â‰¤ C0

s

r

ï£¶

2C`2 (log log n + Î³)
ï£¸

W L log W
log n +
n
n
ï£«s
ï£¶
r
2(c2 âˆ¨ 1)
120M C` log log n + Î³ ï£¸
+ï£­
n +
+ râˆ—
c2
c1
n
!
r
r
W L log W
log log n + Î³
log n +
+ n ,
n
n

(A.17)

with some constant C 0 > 0 that does not depend on n. This completes the proof of Theorem 2.

A.3

Final Steps for the MLP case

For the multi-layer perceptron, W â‰¤ C Â· H 2 L, and plugging this into the bound (A.17), we obtain
r
C

0

H 2 L2 log(H 2 L)
log n +
n

r

log log n + Î³
+ n
n

!

To optimize this upper bound on rÌ„, we need to specify the trade-offs in n and H and L. To do
so, we utilize the MLP-specific approximation rate of Lemma 7 and the embedding of Lemma 1.
Lemma 1 implies that, for any n , one can embed the approximation class FDNN given by Lemma
7 into a standard MLP architecture FMLP , where specifically
âˆ’d

H = H(n ) â‰¤ W (n )L(n ) â‰¤ C 2 n Î² (log(1/n ) + 1)2 ,
L = L(n ) â‰¤ C Â· (log(1/n ) + 1).

53

For standard MLP architecture FMLP ,
âˆ’ 2d

H 2 L2 log(H 2 L) â‰¤ CÌƒ Â· n Î² (log(1/n ) + 1)7 .
Thus we can optimize the upper bound
ï£«s
ï£¬
rÌ„ â‰¤ C 0 ï£­

by choosing n = n

Î²
âˆ’ 2(Î²+d)

âˆ’ 2d
n Î² (log(1/n ) + 1)7

n

ï£¶
r
log n +

log log n + Î³
ï£·
+ n ï£¸
n

d

, H  Â·n 2(Î²+d) log2 n, L  Â· log n. This gives
rÌ„ â‰¤ C

n

Î²
âˆ’ 2(Î²+d)

r
4

log n +

log log n + t0
n

!
.

Hence putting everything together, with probability at least 1 âˆ’ exp(âˆ’Î³),


Î²
log log n + Î³
âˆ’ Î²+d
2
2
8
Ë†
E(f âˆ’ fâˆ— ) â‰¤ rÌ„ â‰¤ C n
log n +
,
n


log log n + Î³
âˆ’ Î²
En (fË† âˆ’ fâˆ— )2 â‰¤ (2rÌ„)2 â‰¤ 4C n Î²+d log8 n +
.
n
This completes the proof of Theorem 1.

A.4

Proof of Corollaries 1 and 2

For Corollary 1, we want to optimize
W L log U
log log n + Î³
log n +
+ 2DNN .
n
n
Yarotsky (2017, Theorem 1) shows that for the approximation error DNN to obey DNN â‰¤ , it
âˆ’ Î²d

suffices to choose W, U âˆ 

(log(1/) + 1) and L âˆ (log(1/) + 1), given the specific architecture

described therein. Therefore, we attain   nâˆ’Î²/(2Î²+d) by setting W, U  nd/(2Î²+d) and L  log n,
yielding the desired result.
For Corollary 2, we need to optimize
H 2 L2 log(HL)
log log n + Î³
log n +
+ 2MLP .
n
n
Yarotsky (2018, Theorem 1) shows that for the approximation error MLP to obey MLP â‰¤ , it
d

suffices to choose H âˆ 2d + 10 and L âˆ âˆ’ 2 , given the specific architecture described therein.
Thus, for   nâˆ’1/(2+d) we take L  nâˆ’d/(4+2d) , and the result follows.

54

B

Supporting Lemmas

First, we show that one can embed a feedforward network into the multi-layer perceptron architecture by adding auxiliary hidden nodes. This idea is due to Yarotsky (2018).
Lemma 1 (Embedding). For any function f âˆˆ FDNN , there is a g âˆˆ FMLP , with H â‰¤ W L + U ,
such that g = f .

Figure 7: Illustration of how to embed a feedforward network into a multi-layer perceptron, with
auxiliary hidden nodes (shown in yellow).
Proof. The idea is illustrated in Figure 7. For the edges in the directed graph of f âˆˆ FDNN
that connect nodes not in adjacent layers (shown in yellow in Figure 7), one can insert auxiliary
hidden units in order to simply â€œpass forwardâ€ the information. The number of such auxiliary
â€œpassforward unitsâ€ is at most the number of offending edges times the depth L (i.e. for each edge,
at most L auxiliary nodes are required), and this is bounded by W L. Therefore the width of the
MLP network that subsumes the original is upper bounded by W L + U while still maintaining the
required embedding that for any fÎ¸ âˆˆ FDNN , there is a gÎ¸0 âˆˆ FMLP such that gÎ¸0 = fÎ¸ . In order to
match modern practice we only need to show that auxiliary units can be implemented with ReLU
activation. This can be done by setting the constant (â€œbiasâ€) term b of each auxiliary unit large
enough to ensure Ïƒ(xÌƒ0 w + b) = xÌƒ0 w + b, and then subtracting the same b in the last receiving unit
along the path.
Next, we give two properties of the Rademacher complexity that we require (see Mendelson,
2003).
Lemma 2 (Contraction). Let Ï† : R â†’ R be a Lipschitz function |Ï†(f1 ) âˆ’ Ï†(f2 )| â‰¤ L|f1 âˆ’ f2 |, then
EÎ· Rn {Ï† â—¦ f : f âˆˆ F} â‰¤ 2LEÎ· Rn F.
55

Lemma 3 (Dudleyâ€™s Chaining). Let N (Î´, F, k Â· kn ) denote the metric entropy for class F (with
covering radius Î´ and metric k Â· kn ), then


Z
12 r p
4Î± + âˆš
EÎ· Rn {f : f âˆˆ F, kf kn â‰¤ r} â‰¤ inf
log N (Î´, F, k Â· kn )dÎ´ .
0<Î±<r
n Î±
Furthermore, because kf kn â‰¤ maxi |f (xi )|, and therefore N (Î´, F, k Â· kn ) â‰¤ N (Î´, F|x1 ,...,xn , âˆ) and
so the upper bound in the conclusions also holds with N (Î´, F|x1 ,...,xn , âˆ).
The next two results, Theorems 12.2 and 14.1 in Anthony and Bartlett (1999), show that the
metric entropy may be bounded in terms of the pseudo-dimension and that the latter is bounded
by the Vapnik-Chervonenkis (VC) dimension.
Lemma 4. Assume for all f âˆˆ F, kf kâˆ â‰¤ M . Denote the pseudo-dimension of F as Pdim(F),
then for n â‰¥ Pdim(F), we have for any Î´,

N (Î´, F|x1 ,...,xn , âˆ) â‰¤

2eM Â· n
Î´ Â· Pdim(F)

Pdim(F )
.

The following symmetrization lemma bounds the empirical processes term using Rademacher
complexity, and is thus a crucial piece of our localization. This is a standard result based on
Talagrandâ€™s concentration, but here special care is taken with the dependence on the variance.
Lemma 5 (Symmetrization, Theorem 2.1 in Bartlett et al. (2005)). For any g âˆˆ G, assume that
|g| â‰¤ G and V[g] â‰¤ V . Then for every Î³ > 0, with probability at least 1 âˆ’ eâˆ’Î³
r

2V Î³ 4G Î³
+
,
n
3 n

r

2V Î³ 23G Î³
+
.
n
3 n

sup {Eg âˆ’ En g} â‰¤ 3ERn G +
gâˆˆG

and with probability at least 1 âˆ’ 2eâˆ’t
sup {Eg âˆ’ En g} â‰¤ 6EÎ· Rn G +
gâˆˆG

The same result holds for supgâˆˆG {En g âˆ’ Eg}.
When bounding the complexity of FDNN , we use the following result. Bartlett et al. (2017) also
verify these bounds for the VC-dimension.
Lemma 6 (Theorem 6 in Bartlett et al. (2017), ReLU case). Consider a ReLU network architecture
F = FDNN (W, L, U ), then the pseudo-dimension is sandwiched as
c Â· W L log(W/L) â‰¤ Pdim(F) â‰¤ C Â· W L log W,
with some universal constants c, C > 0.

56

For multi-layer perceptrons we use the following approximation result, Theorem 1 of Yarotsky
(2017).
Lemma 7. There exists a network class FDNN , with ReLU activation, such that for any  > 0:
(a) FDNN approximates the W Î²,âˆ ([âˆ’1, 1]d ) in the sense for any fâˆ— âˆˆ W Î²,âˆ ([âˆ’1, 1]d ), there exists
a fn () := fn âˆˆ FDNN such that
kfn âˆ’ fâˆ— kâˆ â‰¤ ,
âˆ’ Î²d

(b) and FDNN has L() â‰¤ C Â· (log(1/) + 1) and W (), U () â‰¤ C Â· 

(log(1/) + 1).

Here C only depends on d and Î².
For completeness, we verify the requirements on the loss functions, Equation (2.1), for several
examples. We first treat least squares and logistic losses, in slightly more detail, as these are used
in our subsequent inference results and empirical application.
Lemma 8. Both the least squares (2.2) and logistic (2.3) loss functions obey the requirements of
Equation (2.1). For least squares, c1 = c2 = 1/2 and C` = M . For logistic regression, c1 =
(2(exp(M ) + exp(âˆ’M ) + 2))âˆ’1 , c2 = 1/8 and C` = 1.
Proof. The Lipschitz conditions are trivial. For least squares, using iterated expectations


2E`(f, Z) âˆ’ 2E`(fâˆ— , Z) = E âˆ’2Y f + f 2 + 2Y fâˆ— âˆ’ fâˆ—2


= E âˆ’2fâˆ— f (x) + f 2 + 2(fâˆ— )2 âˆ’ fâˆ—2


= E (f âˆ’ fâˆ— )2 .
For logistic regression,



exp(fâˆ— )
1 + exp(f )
E[`(f, Z)] âˆ’ E[`(fâˆ— , Z)] = E âˆ’
(f âˆ’ fâˆ— ) + log
.
1 + exp(fâˆ— )
1 + exp(fâˆ— )
exp(a)
Define ha (b) = âˆ’ 1+exp(a)
(b âˆ’ a) + log



1+exp(b)
1+exp(a)



, then

1
ha (b) = ha (a) + h0a (a)(b âˆ’ a) + h00a (Î¾a + (1 âˆ’ Î¾)b) (b âˆ’ a)2
2
1
and h00a (b) = exp(b)+exp(âˆ’b)+2
â‰¤ 14 . The lower bound holds as |Î¾fâˆ— + (1 âˆ’ Î¾)f | â‰¤ M .

Beyond least squares and logistic regression, we give three further examples, discussed in the
general language of generalized linear models. Note that in the final example we move beyond a
simple scalar outcome.
Lemma 9. For a convex function g(Â·) : R â†’ R, consider the generalized linear loss function
`(f, z) = âˆ’hy, f (x)i + g(f (x)). The curvature and the Lipschitz conditions in (2.1) will hold given
specific g(Â·). In each case, the loss function corresponds to the negative log likelihood function.
57

(a) Poisson: g(t) = exp(t), with fâˆ— (x) = log E[y|X = x].
(b) Gamma: g(t) = âˆ’ log t, with fâˆ— (x) = âˆ’(E[y|X = x])âˆ’1 .
(c) Multinomial Logistic, K + 1 classes: g(t) = log(1 +
[k]

exp(fâˆ— (x))/(1 +

X

P

kâˆˆK exp(t

[k] )), with

[k0 ]

exp(fâˆ— (x))) = E[y [k] |X = x].

k0 âˆˆK

Here v [k] denotes the k-th coordinate of a vector v.
Proof. Denote âˆ‡g, Hessian[g] to be the gradient and Hessian of the convex function g. By the
convexity of g, the optimal fâˆ— satisfies E[âˆ‚`(fâˆ— , Z)/âˆ‚f |X = x] = 0, which implies
âˆ‡g(fâˆ— ) = E[Y |X = x].
If 2c0  Hessian[g(f )]  2c1 for all f of interest, then the curvature condition in (2.1) holds, because
E[`(f, Z)] âˆ’ E[`(fâˆ— , Z)] = E[âˆ’hâˆ‡g(fâˆ— ), f âˆ’ fâˆ— i + g(f ) âˆ’ g(fâˆ— )]
1
= Ehf âˆ’ fâˆ— , Hessian[g(fËœ)]f âˆ’ fâˆ— i
2
â‰¥ c0 Ekf âˆ’ fâˆ— k2 ,
and the parallel argument for â‰¤ c1 Ekf âˆ’fâˆ— k2 . The Lipschitz condition is equivalent to kâˆ‡g(f )k â‰¤ C`0
for all f of interest, with bounded Y .
For our three examples in particular, we have the following.
(a) For Poisson regression:
kâˆ‡c(f )k = | exp(f )| â‰¤ exp(M ),

Hessian[c(f )] = exp(f ) âˆˆ [exp(âˆ’M ), exp(M )].

(b) For Gamma regression, bounding âˆ’Y above and below is equivalent to 1/M â‰¤ kf k â‰¤ M and
therefore:
kâˆ‡c(f )k = |1/f | â‰¤ M, Hessian[c(f )] = 1/f 2 âˆˆ [1/M 2 , M 2 ].
(c) For multinomial logistic regression, with general K, we know
kâˆ‡c(f )k â‰¤ 1
Hessian[c(f )] = diag{z} âˆ’ zz > where z [k] :=

58

exp(f [k] )
P
.
1 + k0 f [k0 ]

One can easily verify that the eigenvalues are bounded in the following sense, for bounded f ,
1
exp(M )
â‰¤ Î»(Hessian[c(f )]) â‰¤
.
(1 + K exp(M ))2
1 + (K âˆ’ 1) exp(âˆ’M ) + exp(M )
This completes the proof.
Our last result is to verify condition (c) of Theorem 3. We do so using our localization, which
may be of future interest in second-step inference with machine learning methods.
Lemma 10. Let the conditions of Theorem 3 hold. Then

En


(ÂµÌ‚t (xi ) âˆ’ Âµt (xi )) 1 âˆ’

1{ti = t}
P[T = t|X = xi ]







Î²
log log n
âˆ’ Î²+d
8
= oP n
log n +
= oP nâˆ’1/2 .
n

Proof. Without loss of generality we can take pÌ„ < 1/2. The only estimated function here is Âµt (x),
which plays the role of fâˆ— here. For function(als) L(Â·) of the form

L(f ) := (f (xi ) âˆ’ fâˆ— (xi )) 1 âˆ’

1{ti = t}
P[T = t|X = xi ]


,

it is true that


E[1{ti = t}|xi ]
E[L(f )] = E (f (X) âˆ’ fâˆ— (X)) 1 âˆ’
=0
P[T = t|X = xi ]


and
h
i
V[L(f )] â‰¤ (1/pÌ„ âˆ’ 1)2 E (f (X) âˆ’ fâˆ— (X))2 â‰¤ (1/pÌ„ âˆ’ 1)2 rÌ„2
|L(f )| â‰¤ (1/pÌ„ âˆ’ 1) 2M.
For rÌ„ defined in (A.14),
18M ERn {f âˆ’ fâˆ— : f âˆˆ F, kf âˆ’ fâˆ— kL2 (x) â‰¤ rÌ„} â‰¤ rÌ„2
ERn {L(f ) : f âˆˆ F, kf âˆ’ fâˆ— kL2 (x) â‰¤ rÌ„} â‰¤ 2 (1/pÌ„ âˆ’ 1) ERn {f âˆ’ fâˆ— : f âˆˆ F, kf âˆ’ fâˆ— kL2 (x) â‰¤ rÌ„}
where the first line is due to rÌ„ > râˆ— , and second line uses Lemma 2.
Then by the localization analysis and Lemma 5, for all f âˆˆ F, kf âˆ’ fâˆ— kL2 (x) â‰¤ rÌ„, L(f ) obeys
s

4 (1/pÌ„ âˆ’ 1)2 t 8 (1/pÌ„ âˆ’ 1) 3M t
En [L(f )] = En [L(f )] âˆ’ E[L(f )] â‰¤ 6C rÌ„2 + rÌ„
+
â‰¤ 4C rÌ„2
n
3
n


Î²
log log n
âˆ’ Î²+d
8
â‰¤CÂ· n
log n +
,
n


log log n
âˆ’ Î²
sup
En [L(f )] â‰¤ C Â· n Î²+d log8 n +
.
n
f âˆˆF ,kf âˆ’fâˆ— kL (x) â‰¤rÌ„
2

59

d

With probability at least 1 âˆ’ exp(âˆ’n Î²+d log8 n), fË†MLP lies in this set of functions, and therefore


b
Ë†
En [L(fMLP )] = En (fn,H,L (x) âˆ’ fâˆ— (x)) 1 âˆ’

1(T = t)
P (T = t|x = x)

as claimed.

60




â‰¤CÂ· n

Î²
âˆ’ Î²+d

log log n
log n +
n
8


,

