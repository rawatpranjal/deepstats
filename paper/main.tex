\documentclass[11pt]{article}

% Packages
\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{natbib}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{threeparttable}  % For table notes
\usepackage{enumerate}       % For numbered assumptions
\usepackage{float}           % For H placement

% Note: tablenotes environment provided by threeparttable package

% Code styling
\lstset{
    language=Python,
    basicstyle=\ttfamily\small,
    keywordstyle=\color{blue},
    commentstyle=\color{gray},
    stringstyle=\color{orange},
    showstringspaces=false,
    frame=single,
    breaklines=true,
}

% Theorem environments
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{assumption}{Assumption}
\theoremstyle{definition}
\newtheorem{definition}{Definition}
\theoremstyle{remark}
\newtheorem{remark}{Remark}

% Title
\title{Influence Functions for Neural Network Inference:\\
Validation Across Exponential Family Models}

\author{
  Author Name\\
  \texttt{author@email.com}
}

\date{\today}

\begin{document}

\maketitle

\begin{abstract}
We implement and validate the influence function approach of
\citet{farrell2021deep} for valid inference with neural network estimators.
Neural networks estimate structural parameters directly---baseline effects
$\alpha(X)$ and treatment effects $\beta(X)$---while influence functions
correct for regularization bias to yield valid confidence intervals.

The \texttt{deepstats} package implements this framework for nine exponential
family models: linear (Gaussian), gamma, Gumbel, Poisson, logit (Bernoulli),
Tobit (censored), negative binomial, Weibull, and heteroskedastic linear.
$K$-fold cross-fitting with influence function corrections produces
$\sqrt{n}$-consistent and asymptotically normal estimators for the average
treatment effect $\mu^* = E[\beta(X)]$.

Monte Carlo simulations validate the approach. On linear and logit models,
the influence function method achieves 90--93\% coverage of nominal 95\%
confidence intervals with well-calibrated standard errors (SE ratio near 1.0).
In contrast, naive estimation without influence corrections achieves only
3--10\% coverage, severely underestimating uncertainty. These results
demonstrate that influence function corrections are essential for valid
inference with regularized neural network estimators.
\end{abstract}

%------------------------------------------------------------------------------
% Sections (ordered per econometrics journal structure)
%------------------------------------------------------------------------------

% Section 1: Introduction (problem, limitations, contributions, organization)
\input{sections/introduction}

% Section 2: Model and Assumptions
\input{sections/model}

% Section 3: Estimation (cross-fitting, training, ATE/ITE estimation)
\input{sections/estimation}

% Section 4: Asymptotic Theory (main theorem, proof sketch)
\input{sections/theory}

% Section 5: Monte Carlo Simulations
\input{sections/simulations}

% Section 6: Software
\input{sections/software}

% Section 7: Conclusion
\input{sections/conclusion}

%------------------------------------------------------------------------------
% References
%------------------------------------------------------------------------------
\bibliographystyle{apalike}
\bibliography{references}

\end{document}
