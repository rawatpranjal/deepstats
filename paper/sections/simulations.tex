%------------------------------------------------------------------------------
\section{Monte Carlo Validation}
\label{sec:simulations}
%------------------------------------------------------------------------------

We validate the influence function approach on linear and logit families.

\paragraph{Setup.}
$M=30$ simulations, $N=10{,}000$ observations, $K=50$ folds.
Covariates $X \sim \text{Uniform}(-1,1)^{10}$ with nonlinear $\alpha^*(X)$
and heterogeneous $\beta^*(X)$. True $\mu^* = E[\beta^*(X)] \approx -0.168$.

\paragraph{Methods.}
\begin{itemize}
    \item \textbf{Naive}: Train on full data, SE = std($\hat{\beta}$)/$\sqrt{n}$
    \item \textbf{Influence}: $K$-fold cross-fitting with influence scores,
          SE = std($\psi$)/$\sqrt{n}$
\end{itemize}

\begin{table}[h]
\centering
\caption{Monte Carlo Results ($M=30$, $N=10{,}000$, $K=50$)}
\label{tab:mc}
\begin{tabular}{llccc}
\toprule
Family & Method & Coverage & SE Ratio & Grade \\
\midrule
Linear & Influence & 93.3\% & 1.03 & PASS \\
Linear & Naive & 10.0\% & 0.09 & FAIL \\
\midrule
Logit & Influence & 90.0\% & 0.93 & WARNING \\
Logit & Naive & 3.3\% & 0.03 & FAIL \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Key Finding.}
The influence function method achieves 90--93\% coverage with SE ratios near
1.0. Naive estimation achieves only 3--10\% coverage, underestimating standard
errors by 91--97\%. This confirms that the influence function correction is
essential for valid inference with regularized neural networks.
