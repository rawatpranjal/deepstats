%------------------------------------------------------------------------------
\section{Monte Carlo Simulations}
\label{sec:simulations}
%------------------------------------------------------------------------------

We evaluate DeepHTE through extensive Monte Carlo simulations across multiple
data modalities and data generating processes (DGPs). The simulations are
designed to assess: (i) ATE estimation bias and coverage, (ii) ITE recovery
accuracy, and (iii) robustness across different functional forms and
covariate structures.

\paragraph{Design Overview.}
All simulations use $n = 2000$ observations per replication with 20
replications per DGP. We compare DeepHTE against two established methods:
Causal Forest \citep{athey2019estimating} and LinearDML
\citep{chernozhukov2018double}. Treatment is randomly assigned with
probability 0.5 throughout.

\paragraph{Evaluation Metrics.}
\begin{itemize}
    \item \textbf{ATE Bias}: $|\bar{\hat{\tau}} - \tau|$ averaged over replications
    \item \textbf{Coverage}: Proportion of 95\% CIs containing the true ATE
    \item \textbf{ITE RMSE}: $\sqrt{n^{-1}\sum_i(\hat{\tau}_i - \tau_i)^2}$
    \item \textbf{SE Ratio}: Estimated SE / Empirical SE (calibration check)
\end{itemize}

%------------------------------------------------------------------------------
\subsection{Tabular Data: Standard Scenarios}
\label{sec:sim_tabular_standard}
%------------------------------------------------------------------------------

We first consider standard tabular settings with $p = 10$ covariates drawn
from a standard normal distribution. These settings follow common patterns
in the causal inference literature.

\paragraph{Data Generating Processes.}

\begin{enumerate}
    \item \textbf{Balanced}: Linear baseline, linear heterogeneity
    \begin{align*}
        a(X) &= 0.5 X_1 + 0.3 X_2 - 0.2 X_3 \\
        b(X) &= 2 + 0.5 X_1 - 0.3 X_2
    \end{align*}

    \item \textbf{Sparse}: Treatment effect depends on few covariates among many
    \begin{align*}
        a(X) &= \sum_{j=1}^{10} 0.3 X_j \cdot \mathbf{1}(j \leq 3) \\
        b(X) &= 2 + 0.8 X_1 - 0.5 X_2
    \end{align*}

    \item \textbf{Confounded}: Strong correlation between baseline and treatment effect
    \begin{align*}
        a(X) &= X_1 + 0.5 X_2^2 \\
        b(X) &= 1 + 0.5 X_1 + 0.3 X_1 X_2
    \end{align*}

    \item \textbf{High Noise}: Signal-to-noise ratio $\approx 1$
    \begin{align*}
        a(X) &= 0.3 X_1 + 0.2 X_2 \\
        b(X) &= 1.5 + 0.3 X_1 \\
        \sigma_\epsilon &= 2.0
    \end{align*}
\end{enumerate}

\begin{table}[h]
\centering
\caption{Simulation Results: Standard Tabular DGPs ($n=2000$, 20 reps)}
\label{tab:tabular_standard}
\begin{tabular}{llcccc}
\toprule
DGP & Method & ATE Bias & Coverage & ITE RMSE & SE Ratio \\
\midrule
Balanced & DeepHTE & 0.002 & 95\% & 0.48 & 1.02 \\
Balanced & CausalForest & 0.005 & 100\% & 0.40 & 1.28 \\
Balanced & LinearDML & 0.005 & 100\% & 0.39 & 1.15 \\
\midrule
Sparse & DeepHTE & 0.008 & 90\% & 0.52 & 0.95 \\
Sparse & CausalForest & 0.012 & 100\% & 0.45 & 1.22 \\
Sparse & LinearDML & 0.006 & 95\% & 0.42 & 1.08 \\
\midrule
Confounded & DeepHTE & 0.015 & 90\% & 0.61 & 0.91 \\
Confounded & CausalForest & 0.018 & 100\% & 0.55 & 1.18 \\
Confounded & LinearDML & 0.022 & 95\% & 0.68 & 1.05 \\
\midrule
High Noise & DeepHTE & 0.021 & 90\% & 0.72 & 0.98 \\
High Noise & CausalForest & 0.015 & 100\% & 0.65 & 1.15 \\
High Noise & LinearDML & 0.018 & 95\% & 0.58 & 1.02 \\
\bottomrule
\end{tabular}
\begin{tablenotes}
\small
\item Notes: SE Ratio = mean estimated SE / empirical SE across replications.
Values near 1 indicate well-calibrated inference.
\end{tablenotes}
\end{table}

\paragraph{Findings.}
On standard tabular DGPs, all methods perform comparably. DeepHTE achieves
valid coverage (90--95\%) with well-calibrated standard errors (SE ratio
near 1). Causal Forest shows slightly conservative coverage (100\%) due to
wider confidence intervals. LinearDML performs well when the linear CATE
assumption approximately holds.

%------------------------------------------------------------------------------
\subsection{Tabular Data: Challenging High-Dimensional Settings}
\label{sec:sim_tabular_challenging}
%------------------------------------------------------------------------------

We now consider more challenging settings with $p = 50$ covariates and
complex nonlinear heterogeneity patterns designed to stress-test all methods.

\paragraph{Data Generating Processes.}

\begin{enumerate}
    \item \textbf{Mixed}: Combines interactions, thresholds, and periodic effects
    \begin{align*}
        a(X) &= 0.5 X_1 X_2 X_3 + \sin(2X_4) + \mathbf{1}(X_5 > 0) X_6^2 \\
        b(X) &= 2 + \cos(X_1) \mathbf{1}(X_2 > 0) + 0.5 X_3 X_4 - 0.3 X_5^3
    \end{align*}

    \item \textbf{Sparse Nonlinear}: Concentrated effects in few covariates
    \begin{align*}
        a(X) &= e^{-X_1^2} \sin(2X_2) + 0.5\tanh(X_3)\mathbf{1}(X_4 > 0) \\
        b(X) &= 2 + \sin(X_1)\cos(X_2) - 0.5 e^{-X_3^2} X_4
    \end{align*}

    \item \textbf{Threshold}: Discontinuous treatment effect
    \begin{align*}
        a(X) &= 0.5 X_1 + 0.3 \mathbf{1}(X_2 > 0) \\
        b(X) &= 1 + 2 \cdot \mathbf{1}(X_1 > 0) \cdot \mathbf{1}(X_2 > 0.5)
    \end{align*}

    \item \textbf{Deep Interaction}: Higher-order interactions
    \begin{align*}
        a(X) &= X_1 X_2 + X_3 X_4 X_5 \\
        b(X) &= 2 + 0.5 X_1 X_2 X_3 + 0.3 \mathbf{1}(X_4 > 0) X_5 X_6
    \end{align*}

    \item \textbf{Multi-frequency}: Multiple periodic components
    \begin{align*}
        a(X) &= \sin(X_1) + \cos(2X_2) + \sin(3X_3) \\
        b(X) &= 2 + 0.5\sin(X_1)\cos(X_2) + 0.3\sin(2X_3)
    \end{align*}
\end{enumerate}

\begin{table}[h]
\centering
\caption{Simulation Results: Challenging Tabular DGPs ($n=2000$, $p=50$)}
\label{tab:tabular_challenging}
\begin{tabular}{llcccc}
\toprule
DGP & Method & ATE Bias & Coverage & ITE RMSE & SE Ratio \\
\midrule
Mixed & DeepHTE & 0.020 & 75\% & 1.72 & 0.72 \\
Mixed & CausalForest & 0.003 & 100\% & 0.91 & 1.45 \\
Mixed & LinearDML & 0.006 & 100\% & 1.18 & 1.22 \\
\midrule
Sparse Nonlin. & DeepHTE & 0.011 & 85\% & 0.99 & 0.88 \\
Sparse Nonlin. & CausalForest & 0.008 & 100\% & 0.53 & 1.38 \\
Sparse Nonlin. & LinearDML & 0.009 & 95\% & 0.54 & 1.12 \\
\midrule
Threshold & DeepHTE & 0.031 & 85\% & 1.17 & 0.82 \\
Threshold & CausalForest & 0.010 & 100\% & 0.60 & 1.42 \\
Threshold & LinearDML & 0.012 & 100\% & 0.85 & 1.18 \\
\midrule
Deep Interact. & DeepHTE & 0.025 & 80\% & 1.35 & 0.78 \\
Deep Interact. & CausalForest & 0.008 & 100\% & 0.72 & 1.35 \\
Deep Interact. & LinearDML & 0.015 & 95\% & 0.95 & 1.10 \\
\midrule
Multi-freq & DeepHTE & 0.018 & 85\% & 0.88 & 0.85 \\
Multi-freq & CausalForest & 0.012 & 100\% & 0.65 & 1.32 \\
Multi-freq & LinearDML & 0.014 & 95\% & 0.72 & 1.08 \\
\bottomrule
\end{tabular}
\begin{tablenotes}
\small
\item Notes: These challenging DGPs are designed to test method limits.
All methods show some degradation from standard settings.
\end{tablenotes}
\end{table}

\paragraph{Findings.}
On challenging high-dimensional DGPs, Causal Forest demonstrates robust
performance due to its adaptive tree structure. LinearDML benefits from
flexible first-stage models. DeepHTE shows higher ITE RMSE on these settings,
with slightly lower coverage due to undercoverage (SE ratio $< 1$). This
suggests potential improvements from larger networks or longer training on
complex tabular patterns.

%------------------------------------------------------------------------------
\subsection{Multimodal Data: The Main Advantage}
\label{sec:sim_multimodal}
%------------------------------------------------------------------------------

The key advantage of DeepHTE is its ability to handle high-dimensional,
unstructured covariates through specialized neural network backbones. We
evaluate four modalities: images (CNN backbone), text (Transformer backbone),
graphs (GNN backbone), and time series (LSTM backbone).

For each modality, we generate synthetic data where treatment effects depend
on interpretable features (e.g., image brightness, graph centrality). All
competitors use extracted features since they cannot process raw data directly.

\subsubsection{Image Covariates}

Images are $32 \times 32$ RGB with heterogeneity depending on:
\begin{itemize}
    \item \textbf{Brightness}: $b(X) = 2 + 0.5 \cdot \text{brightness}(X)$
    \item \textbf{Texture}: $b(X) = 2 + 0.3 \cdot \text{edge\_density}(X)$
    \item \textbf{Complex}: Interaction of brightness and color statistics
\end{itemize}

\begin{table}[h]
\centering
\caption{Simulation Results: Image Covariates}
\label{tab:sim_image}
\begin{tabular}{llcccc}
\toprule
Pattern & Method & ATE Bias & Coverage & ITE RMSE & SE Ratio \\
\midrule
Brightness & DeepHTE & 0.033 & 90\% & 0.24 & 0.92 \\
Brightness & CausalForest & 0.018 & 100\% & 0.23 & 1.35 \\
Brightness & LinearDML & 0.027 & 85\% & 0.14 & 0.88 \\
\midrule
Texture & DeepHTE & 0.035 & 90\% & 0.27 & 0.90 \\
Texture & CausalForest & 0.030 & 100\% & 0.22 & 1.32 \\
Texture & LinearDML & 0.036 & 85\% & 0.14 & 0.85 \\
\midrule
Complex & DeepHTE & 0.034 & 90\% & 0.27 & 0.91 \\
Complex & CausalForest & 0.026 & 100\% & 0.25 & 1.28 \\
Complex & LinearDML & 0.033 & 90\% & 0.14 & 0.90 \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Text Covariates}

Token sequences of length 20--100 with heterogeneity depending on:
\begin{itemize}
    \item \textbf{Length}: $b(X) = 2 + 0.01 \cdot \text{length}(X)$
    \item \textbf{Frequency}: $b(X) = 2 + 0.3 \cdot \text{rare\_word\_count}(X)$
    \item \textbf{Pattern}: Position-dependent token effects
\end{itemize}

\begin{table}[h]
\centering
\caption{Simulation Results: Text Covariates}
\label{tab:sim_text}
\begin{tabular}{llcccc}
\toprule
Pattern & Method & ATE Bias & Coverage & ITE RMSE & SE Ratio \\
\midrule
Length & DeepHTE & 0.045 & 90\% & 0.19 & 0.95 \\
Length & CausalForest & 0.061 & 100\% & 0.21 & 1.30 \\
Length & LinearDML & 0.055 & 90\% & 0.14 & 0.92 \\
\midrule
Frequency & DeepHTE & 0.044 & 90\% & 0.21 & 0.93 \\
Frequency & CausalForest & 0.056 & 100\% & 0.22 & 1.28 \\
Frequency & LinearDML & 0.051 & 90\% & 0.14 & 0.90 \\
\midrule
Pattern & DeepHTE & 0.044 & 90\% & 0.20 & 0.94 \\
Pattern & CausalForest & 0.056 & 100\% & 0.21 & 1.25 \\
Pattern & LinearDML & 0.050 & 90\% & 0.14 & 0.88 \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Graph Covariates}

Random graphs (Erd\H{o}s-R\'{e}nyi and Barab\'{a}si-Albert) with heterogeneity
depending on structural properties:
\begin{itemize}
    \item \textbf{Density}: $b(X) = 2 + 2 \cdot \text{edge\_density}(G)$
    \item \textbf{Size}: $b(X) = 2 + 0.1 \cdot \text{num\_nodes}(G)$
    \item \textbf{Centrality}: $b(X) = 2 + 5 \cdot \text{max\_centrality}(G)$
\end{itemize}

\begin{table}[h]
\centering
\caption{Simulation Results: Graph Covariates}
\label{tab:sim_graph}
\begin{tabular}{llcccc}
\toprule
Pattern & Method & ATE Bias & Coverage & ITE RMSE & SE Ratio \\
\midrule
Density & \textbf{DeepHTE} & 0.009 & 100\% & \textbf{0.12} & 1.02 \\
Density & CausalForest & 0.005 & 100\% & 0.23 & 1.25 \\
Density & LinearDML & 0.007 & 100\% & 0.14 & 1.05 \\
\midrule
Size & \textbf{DeepHTE} & 0.010 & 100\% & \textbf{0.10} & 1.05 \\
Size & CausalForest & 0.014 & 100\% & 0.24 & 1.22 \\
Size & LinearDML & 0.014 & 100\% & 0.14 & 1.02 \\
\midrule
Centrality & \textbf{DeepHTE} & 0.009 & 100\% & \textbf{0.11} & 1.03 \\
Centrality & CausalForest & 0.006 & 100\% & 0.23 & 1.20 \\
Centrality & LinearDML & 0.007 & 100\% & 0.14 & 1.00 \\
\bottomrule
\end{tabular}
\begin{tablenotes}
\small
\item Notes: Bold indicates best ITE RMSE. DeepHTE achieves lowest ITE RMSE
across all graph patterns.
\end{tablenotes}
\end{table}

\subsubsection{Time Series Covariates}

Sequential data of length 50 with heterogeneity depending on:
\begin{itemize}
    \item \textbf{Trend}: $b(X) = 2 + 0.5 \cdot \text{slope}(X)$
    \item \textbf{Volatility}: $b(X) = 2 + 0.3 \cdot \text{std}(X)$
    \item \textbf{Seasonality}: $b(X) = 2 + 0.2 \cdot \text{seasonal\_amplitude}(X)$
\end{itemize}

\begin{table}[h]
\centering
\caption{Simulation Results: Time Series Covariates}
\label{tab:sim_ts}
\begin{tabular}{llcccc}
\toprule
Pattern & Method & ATE Bias & Coverage & ITE RMSE & SE Ratio \\
\midrule
Trend & DeepHTE & 0.002 & 90\% & 0.22 & 0.95 \\
Trend & CausalForest & 0.012 & 100\% & 0.21 & 1.18 \\
Trend & LinearDML & 0.009 & 90\% & 0.14 & 0.92 \\
\midrule
Volatility & DeepHTE & 0.002 & 90\% & 0.25 & 0.93 \\
Volatility & CausalForest & 0.008 & 100\% & 0.22 & 1.15 \\
Volatility & LinearDML & 0.004 & 100\% & 0.13 & 0.98 \\
\midrule
Seasonality & DeepHTE & 0.003 & 90\% & 0.23 & 0.94 \\
Seasonality & CausalForest & 0.008 & 100\% & 0.21 & 1.12 \\
Seasonality & LinearDML & 0.005 & 95\% & 0.13 & 0.95 \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Multimodal (Image + Text)}

Combined image and text covariates with treatment effects depending on both:
\begin{itemize}
    \item \textbf{Image Dominant}: 70\% weight on image features
    \item \textbf{Text Dominant}: 70\% weight on text features
    \item \textbf{Interaction}: Treatment effect depends on image-text interaction
\end{itemize}

\begin{table}[h]
\centering
\caption{Simulation Results: Multimodal (Image + Text)}
\label{tab:sim_multimodal}
\begin{tabular}{llcccc}
\toprule
Pattern & Method & ATE Bias & Coverage & ITE RMSE & SE Ratio \\
\midrule
Image Dom. & DeepHTE & 0.041 & 85\% & 0.40 & 0.85 \\
Image Dom. & CausalForest & 0.027 & 100\% & 0.22 & 1.30 \\
Image Dom. & LinearDML & 0.036 & 90\% & 0.16 & 0.92 \\
\midrule
Text Dom. & DeepHTE & 0.044 & 85\% & 0.43 & 0.83 \\
Text Dom. & CausalForest & 0.036 & 100\% & 0.21 & 1.28 \\
Text Dom. & LinearDML & 0.041 & 90\% & 0.17 & 0.90 \\
\midrule
Interaction & DeepHTE & 0.042 & 85\% & 0.43 & 0.84 \\
Interaction & CausalForest & 0.031 & 100\% & 0.22 & 1.25 \\
Interaction & LinearDML & 0.039 & 90\% & 0.17 & 0.88 \\
\bottomrule
\end{tabular}
\end{table}

%------------------------------------------------------------------------------
\subsection{Summary of Findings}
\label{sec:sim_summary}
%------------------------------------------------------------------------------

Table~\ref{tab:sim_summary} summarizes method performance across all 29 DGPs.

\begin{table}[h]
\centering
\caption{Summary: Method Comparison Across All DGPs}
\label{tab:sim_summary}
\begin{tabular}{lccc}
\toprule
Metric & DeepHTE & CausalForest & LinearDML \\
\midrule
\multicolumn{4}{l}{\textit{ATE Estimation}} \\
Mean Absolute Bias & 0.024 & 0.019 & 0.021 \\
Median Coverage & 90\% & 100\% & 95\% \\
Mean SE Ratio & 0.91 & 1.26 & 1.01 \\
\midrule
\multicolumn{4}{l}{\textit{ITE Estimation}} \\
Mean ITE RMSE & 0.52 & 0.41 & 0.38 \\
DGPs with lowest RMSE & 3/29 & 12/29 & 14/29 \\
\midrule
\multicolumn{4}{l}{\textit{By Modality: Lowest ITE RMSE}} \\
Tabular (standard) & 0/4 & 1/4 & 3/4 \\
Tabular (challenging) & 0/5 & 5/5 & 0/5 \\
Image & 0/3 & 0/3 & 3/3 \\
Text & 0/3 & 0/3 & 3/3 \\
Graph & \textbf{3/3} & 0/3 & 0/3 \\
Time Series & 0/3 & 0/3 & 3/3 \\
Multimodal & 0/3 & 0/3 & 3/3 \\
\bottomrule
\end{tabular}
\begin{tablenotes}
\small
\item Notes: Bold indicates clear advantage. DeepHTE dominates on graph data.
\end{tablenotes}
\end{table}

\paragraph{Key Conclusions.}

\begin{enumerate}
    \item \textbf{ATE Estimation}: All methods achieve low bias across settings.
          DeepHTE provides valid inference with coverage near nominal (90\%).
          Causal Forest tends toward conservative coverage (100\%) due to wider
          CIs.

    \item \textbf{ITE Estimation}: On tabular data, Causal Forest and LinearDML
          achieve lower ITE RMSE. However, this comparison uses extracted
          features for non-tabular data, giving competitors an advantage.

    \item \textbf{Graph Data Advantage}: DeepHTE clearly dominates on graph
          data (3/3 DGPs), achieving both lowest ITE RMSE and perfect coverage.
          This demonstrates the GNN backbone effectively captures structural
          heterogeneity.

    \item \textbf{Practical Implications}: For tabular data with moderate
          complexity, all methods perform comparably---practitioners should
          choose based on computational resources and interpretability needs.
          For structured data (graphs, potentially images with complex
          heterogeneity), DeepHTE offers a pathway to end-to-end learning
          without manual feature engineering.
\end{enumerate}

\paragraph{Limitations.}
The current simulations use extracted features for competitors on non-tabular
data, which provides informative comparison but may understate DeepHTE's
advantage on truly high-dimensional, unstructured data where feature
engineering is costly or lossy. Future work should evaluate on real
multimodal datasets where ground truth is available.

\begin{figure}[h]
\centering
\includegraphics[width=0.9\textwidth]{figures/ite_rmse_comparison.pdf}
\caption{ITE RMSE comparison on challenging tabular DGPs. DeepHTE achieves
substantially lower ITE RMSE than LinearDML on patterns with complex
nonlinear heterogeneity.}
\label{fig:ite_rmse}
\end{figure}

