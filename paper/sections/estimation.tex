%------------------------------------------------------------------------------
\section{Estimation}
\label{sec:estimation}
%------------------------------------------------------------------------------

This section details the estimation procedure, including cross-fitting,
training, and influence function computation.

\subsection{Cross-Fitting Procedure}

We use $K$-fold cross-fitting to ensure out-of-sample predictions. Each fold's
model is trained on $(K-1)/K$ of the data, then predictions and influence
scores are computed on the held-out $1/K$. With $K=50$ folds, each model sees
98\% of the data, yielding stable estimates while maintaining independence
between training and evaluation.

\begin{algorithm}[H]
\caption{Influence Function Estimation with Cross-Fitting}
\label{alg:crossfit}
\begin{algorithmic}[1]
\STATE \textbf{Input:} Data $(Y_i, T_i, X_i)_{i=1}^n$, number of folds $K$
\STATE Partition $\{1, \ldots, n\}$ into $K$ folds $\mathcal{I}_1, \ldots, \mathcal{I}_K$
\FOR{$k = 1$ to $K$}
    \STATE Let $\mathcal{I}_{-k} = \{1, \ldots, n\} \setminus \mathcal{I}_k$ (training set)
    \STATE Train \texttt{StructuralNet} on $\{(Y_i, T_i, X_i) : i \in \mathcal{I}_{-k}\}$
    \STATE Train \texttt{NuisanceNet} on $\{(T_i, X_i) : i \in \mathcal{I}_{-k}\}$
    \STATE Compute Hessian: $\Lambda_k = \frac{1}{|\mathcal{I}_{-k}|} \sum_{j \in \mathcal{I}_{-k}} W_j \tilde{T}_j^{\otimes 2}$
    \FOR{$i \in \mathcal{I}_k$}
        \STATE Compute $\hat{\theta}_i = [\hat{\alpha}(X_i), \hat{\beta}(X_i)]$
        \STATE Compute influence score: $\psi_i = H(\hat{\theta}_i) - \nabla\ell_i^\top \Lambda_k^{-1} \nabla H$
    \ENDFOR
\ENDFOR
\STATE \textbf{Output:} $\hat{\mu} = \frac{1}{n}\sum_i \psi_i$, $\text{SE} = \text{std}(\psi) / \sqrt{n}$
\end{algorithmic}
\end{algorithm}

Key notation:
\begin{itemize}
    \item $W_i$: Hessian weight from family (e.g., $W_i = 1$ for linear, $W_i = p_i(1-p_i)$ for logit)
    \item $\tilde{T}_i = [1, T_i - \bar{T}]$: Centered treatment vector
    \item $H(\theta) = \beta$: Target functional (the treatment effect parameter)
    \item $\nabla\ell_i$: Gradient of loss with respect to $\theta$ at observation $i$
    \item $\nabla H = [0, 1]$: Gradient of target with respect to $\theta$
\end{itemize}

The recommended setting is $K = 50$ folds, which ensures 98\% of data is used
for training each model while maintaining independence between training and
evaluation.

\subsection{Training Objective}

For exponential family outcomes, the neural network parameters are estimated
by minimizing the negative log-likelihood:
\[
\hat{\theta} = \arg\min_\theta \sum_{i \in \mathcal{I}_{-k}}
  -\ell(Y_i; \alpha_\theta(X_i), \beta_\theta(X_i), T_i)
\]
where $\ell$ is the family-specific log-likelihood.

Training uses the Adam optimizer with the following regularization:
\begin{itemize}
    \item \textbf{Dropout}: Rate $p = 0.1$ on hidden layers
    \item \textbf{Weight decay}: L2 regularization $\lambda = 10^{-4}$
    \item \textbf{Early stopping}: Patience of 10 epochs based on validation loss
\end{itemize}

\subsection{Influence Function}

The influence function for estimating $\mu^* = E[\beta(X)]$ takes the form:
\begin{equation}
\label{eq:influence}
\psi_i = H(\hat{\theta}_i) - \nabla\ell_i^\top \Lambda^{-1} \nabla H
\end{equation}

For the linear family with target $H(\theta) = \beta$:
\begin{itemize}
    \item $\nabla\ell_i = (Y_i - \hat{\alpha}_i - \hat{\beta}_i T_i) \cdot [1, \tilde{T}_i]$
    \item $\Lambda = \frac{1}{n}\sum_j \tilde{T}_j^{\otimes 2}$
    \item $\nabla H = [0, 1]$
\end{itemize}

The Hessian correction term $\nabla\ell_i^\top \Lambda^{-1} \nabla H$ accounts
for the bias introduced by regularization. Without this correction, standard
errors are severely underestimated.

\subsection{Average Treatment Effect Estimation}

The ATE estimator and its standard error are:
\begin{align}
\hat{\mu} &= \frac{1}{n}\sum_{i=1}^n \psi_i \label{eq:ate} \\
\widehat{\text{SE}}(\hat{\mu}) &= \frac{1}{\sqrt{n}} \sqrt{\frac{1}{n-1}
  \sum_{i=1}^n (\psi_i - \hat{\mu})^2} \label{eq:se}
\end{align}

The 95\% confidence interval is:
\[
\text{CI}_{95\%} = \left[\hat{\mu} - 1.96 \cdot \widehat{\text{SE}}, \;
\hat{\mu} + 1.96 \cdot \widehat{\text{SE}}\right]
\]

\subsection{Implementation Details}

The default configuration in \texttt{deepstats}:
\begin{itemize}
    \item Cross-fitting with $K = 50$ folds
    \item MLP backbone: hidden layers $[64, 32]$
    \item Adam optimizer: learning rate $0.01$
    \item Maximum 100 epochs with early stopping (patience 10)
    \item Batch size 64
    \item Validation split 10\%
\end{itemize}

A separate \texttt{NuisanceNet} estimates $E[T|X]$ for centering the treatment
in the Hessian computation: $\tilde{T}_i = T_i - E[T|X_i]$.
