%------------------------------------------------------------------------------
\section{Introduction}
\label{sec:intro}
%------------------------------------------------------------------------------

Neural networks offer flexible function approximation for complex relationships
in observational data. However, standard neural network training with
regularization (weight decay, dropout, early stopping) introduces bias that
invalidates traditional inference procedures. This paper implements and
validates the influence function approach of \citet{farrell2021deep} for
constructing valid confidence intervals with neural network estimators.

\paragraph{The Problem.}
Consider estimating heterogeneous treatment effects from the structural model:
\[
Y_i = \alpha(X_i) + \beta(X_i) \cdot T_i + \varepsilon_i
\]
where $\alpha(X)$ captures baseline effects, $\beta(X)$ captures the
conditional average treatment effect (CATE), and $T \in \{0, 1\}$ is a binary
treatment. Neural networks can estimate both functions flexibly, but
regularization shrinks predictions toward zero, reducing estimated variance.

Naive standard errors computed as $\text{SE} = \text{std}(\hat{\beta}) / \sqrt{n}$
severely underestimate uncertainty. In our Monte Carlo simulations, naive 95\%
confidence intervals achieve only 3--10\% coverage---a catastrophic failure
of inference.

\paragraph{The Solution.}
The influence function framework of \citet{farrell2021deep} provides valid
inference by correcting for regularization bias. The key insight is that
cross-fitting combined with influence function corrections yields
$\sqrt{n}$-consistent and asymptotically normal estimators for the average
treatment effect $\mu^* = E[\beta(X)]$.

The influence function approach:
\begin{enumerate}
    \item Uses $K$-fold cross-fitting to avoid overfitting bias
    \item Computes Hessian-based corrections for regularization bias
    \item Produces standard errors that achieve the semiparametric efficiency bound
\end{enumerate}

\paragraph{Contributions.}
This paper makes three contributions:

\begin{enumerate}
    \item \textbf{Implementation.} We implement the FLM influence function
    framework for nine exponential family models: linear (Gaussian), gamma,
    Gumbel, Poisson, logit (Bernoulli), Tobit (censored), negative binomial,
    Weibull, and heteroskedastic linear. Each family specifies the appropriate
    loss function, residuals, and Hessian weights for influence computation.

    \item \textbf{Validation.} Monte Carlo simulations on linear and logit
    models demonstrate that the influence function method achieves 90--93\%
    coverage of nominal 95\% confidence intervals, with well-calibrated
    standard errors (SE ratio near 1.0). Naive estimation without corrections
    achieves only 3--10\% coverage.

    \item \textbf{Software.} The \texttt{deepstats} Python package provides
    a simple API for practitioners: \texttt{get\_dgp()}, \texttt{get\_family()},
    and \texttt{influence()} functions handle data generation, model
    specification, and inference respectively. A command-line interface
    enables reproducible Monte Carlo studies.
\end{enumerate}

\paragraph{Organization.}
Section~\ref{sec:model} presents the structural model and exponential family
extensions. Section~\ref{sec:estimation} describes the cross-fitting procedure.
Section~\ref{sec:theory} establishes asymptotic normality.
Section~\ref{sec:simulations} reports Monte Carlo validation results.
Section~\ref{sec:software} describes the software implementation.
Section~\ref{sec:conclusion} concludes.
