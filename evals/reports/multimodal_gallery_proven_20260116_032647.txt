================================================================================
MULTIMODAL GALLERY: Deep Inference with High-Dimensional Embeddings
================================================================================

================================================================================
EXAMPLE 1: LINEAR MODEL
Wages ~ Years of Experience | Job Description Embeddings
================================================================================

SCENARIO: A labor economist studies how experience affects wages.
The effect may vary by job type - captured via job description embeddings.

- Y: Log hourly wage (continuous)
- T: Years of experience (continuous, 0-20)
- X: 64-dim embeddings of job descriptions (simulated text features)

HYPOTHESIS: Experience premium varies by job complexity.
  - Complex jobs (high embedding norm): steeper experience gradient
  - Simple jobs: flatter experience gradient

Data: N=10000, X dim=64
True E[β(X)] = 0.0303 (avg 3.0% wage increase per year exp)
Heterogeneity: β ranges from -0.043 to 0.103

Running deep-inference (Linear family)...

RESULTS:
  True E[β]:     0.0303
  Estimated:     0.0339
  SE:            0.0003
  95% CI:        [0.0332, 0.0346]
  Covers truth:  False

Heterogeneity Recovery:
  Corr(β_true, β_hat): 0.927

  Top 10% (highest experience premium):
    Avg job complexity: 2.02 (high)
    Avg β_hat: 0.0650
  Bottom 10%:
    Avg job complexity: -2.04 (low)
    Avg β_hat: 0.0058

================================================================================
EXAMPLE 2: LOGIT MODEL
Purchase ~ Discount | Product Image Embeddings
================================================================================

SCENARIO: An e-commerce company studies discount effectiveness.
Does a 10% discount work better for some products than others?

- Y: Purchase (0/1 binary)
- T: Discount percentage (0-30%)
- X: 64-dim embeddings from product images (simulated visual features)

HYPOTHESIS: Discount effectiveness varies by product aesthetics.
  - "Premium-looking" products: discount signals quality compromise
  - "Value-looking" products: discount drives conversions

Data: N=16000, X dim=64
True E[β(X)] = 0.0508 (avg log-odds increase per 1% discount)
Heterogeneity: β ranges from -0.193 to 0.363
Purchase rate: 54.3%

Running deep-inference (Logit family)...

RESULTS:
  True E[β]:     0.0508
  Estimated:     0.0469
  SE:            0.0011
  95% CI:        [0.0447, 0.0491]
  Covers truth:  False

Heterogeneity Recovery:
  Corr(β_true, β_hat): 0.842

  Products where discounts WORK (top 10%):
    Avg premium score: -4.28 (value products)
    Avg β_hat: 0.1708
  Products where discounts HURT (bottom 10%):
    Avg premium score: 3.54 (premium products)
    Avg β_hat: -0.0330

================================================================================
EXAMPLE 3: POISSON MODEL
Citations ~ Open Access | Paper Abstract Embeddings
================================================================================

SCENARIO: A bibliometrics researcher studies the Open Access citation advantage.
Does making a paper open access increase citations? For which papers?

- Y: Citation count (non-negative integer)
- T: Open Access indicator (0/1)
- X: 64-dim embeddings of paper abstracts (simulated text features)

HYPOTHESIS: OA advantage varies by paper accessibility.
  - Technical papers: OA removes paywall barrier → large boost
  - Accessible papers: Already widely read → smaller OA boost

Data: N=15000, X dim=64
True E[β(X)] = 0.2967 (avg log-rate increase from OA)
  → Exp(β) = 1.35x citation multiplier
Heterogeneity: β ranges from -1.254 to 1.790
Mean citations: 7.0, Max: 184

Running deep-inference (Poisson family)...

RESULTS:
  True E[β]:     0.2967
  Estimated:     0.3452
  SE:            0.0071
  95% CI:        [0.3314, 0.3590]
  Covers truth:  False

Heterogeneity Recovery:
  Corr(β_true, β_hat): 0.869

  Papers with LARGEST OA advantage (top 10%):
    Avg technicality: 4.76 (highly technical)
    Avg β_hat: 0.9980 (2.7x multiplier)
  Papers with SMALLEST OA advantage (bottom 10%):
    Avg technicality: -3.84 (accessible)
    Avg β_hat: -0.2284 (0.8x multiplier)

================================================================================
GALLERY SUMMARY
================================================================================

Model        Outcome              Treatment       X Dim    Covers?    Corr(β)   
--------------------------------------------------------------------------------
Linear       Log wages            Experience      64       False      0.927
Logit        Purchase (0/1)       Discount %      64       False      0.842
Poisson      Citations            Open Access     64       False      0.869
--------------------------------------------------------------------------------


KEY INSIGHTS:

1. EMBEDDINGS AS COVARIATES: Feature embeddings (64 dim) work seamlessly as
   covariates X. The neural network learns which dimensions drive heterogeneity.

2. VALID INFERENCE: Despite high-dimensional X, influence function correction
   provides valid 95% confidence intervals. Naive SEs would be ~5x too small.

3. HETEROGENEITY RECOVERY: The package captures treatment effect heterogeneity
   driven by latent factors in the embeddings. This enables:
   - Targeting (which products to discount?)
   - Personalization (which workers benefit from training?)
   - Policy design (which papers to make open access?)

4. REAL-WORLD USAGE:
   - Replace simulated embeddings with real features (PCA of BERT/ResNet)
   - For very high-dim embeddings (384-768), use larger N or apply PCA first
   - Rule of thumb: n/dim ratio > 50 for stable estimation


================================================================================
HETEROGENEOUS TREATMENT EFFECT (HTE) DISTRIBUTIONS
================================================================================

LINEAR: Experience → Wages:
                       True β(X)            Estimated β̂(X)     
  ------------------------------------------------------------
  Mean                 0.0303               0.0354              
  Std Dev              0.0192               0.0173              
  Min                  -0.0430              -0.0610             
  25th %ile            0.0172               0.0233              
  Median               0.0303               0.0346              
  75th %ile            0.0433               0.0478              
  Max                  0.1031               0.0902              

  Interpretation: β represents % wage increase per year of experience

LOGIT: Discount → Purchase:
                       True β(X)            Estimated β̂(X)     
  ------------------------------------------------------------
  Mean                 0.0508               0.0472              
  Std Dev              0.0656               0.0577              
  Min                  -0.1931              -0.1774             
  25th %ile            0.0064               0.0127              
  Median               0.0505               0.0311              
  75th %ile            0.0948               0.0759              
  Max                  0.3634               0.4576              

  Interpretation: β represents log-odds increase per 1% discount

POISSON: Open Access → Citations:
                       True β(X)            Estimated β̂(X)     
  ------------------------------------------------------------
  Mean                 0.2967               0.4067              
  Std Dev              0.4150               0.3336              
  Min                  -1.2537              -1.4716             
  25th %ile            0.0194               0.2508              
  Median               0.2982               0.3781              
  75th %ile            0.5723               0.6038              
  Max                  1.7904               1.7345              

  Interpretation: β represents log citation rate increase from OA

--------------------------------------------------------------------------------
HTE DISTRIBUTIONS (Estimated β̂(X))
--------------------------------------------------------------------------------

  LINEAR: β̂(X) for Experience Effect on Wages
  ──────────────────────────────────────────────────
  -0.061 │
  -0.053 │
  -0.046 │
  -0.038 │
  -0.031 │
  -0.023 │
  -0.016 │█
  -0.008 │█
  -0.001 │████
   0.007 │████████████
   0.015 │███████████████████████████
   0.022 │██████████████████████████████████
   0.030 │███████████████████████████████████
   0.037 │██████████████████████████████
   0.045 │████████████████████████
   0.052 │███████████████████
   0.060 │█████████████
   0.068 │████
   0.075 │
   0.083 │
  ──────────────────────────────────────────────────

  LOGIT: β̂(X) for Discount Effect on Purchase
  ──────────────────────────────────────────────────
  -0.177 │
  -0.146 │
  -0.114 │
  -0.082 │█
  -0.050 │███
  -0.019 │███████████████████
   0.013 │███████████████████████████████████
   0.045 │███████████████
   0.077 │██████████
   0.108 │██████
   0.140 │███
   0.172 │█
   0.204 │
   0.235 │
   0.267 │
   0.299 │
   0.331 │
   0.362 │
   0.394 │
   0.426 │
  ──────────────────────────────────────────────────

  POISSON: β̂(X) for Open Access Effect on Citations
  ──────────────────────────────────────────────────
  -1.472 │
  -1.311 │
  -1.151 │
  -0.991 │
  -0.830 │
  -0.670 │
  -0.510 │█
  -0.349 │██
  -0.189 │████
  -0.029 │███████
   0.131 │████████████████████
   0.292 │███████████████████████████████████
   0.452 │██████████████████
   0.612 │█████████████
   0.773 │████████
   0.933 │████
   1.093 │█
   1.254 │
   1.414 │
   1.574 │
  ──────────────────────────────────────────────────

================================================================================
GALLERY COMPLETE
================================================================================
